paragraph
A  ábrán látható probléma bármely eddig látott tervkészítővel megoldható. A  ábra (ha a számokkal nem törődünk) a részben rendezett tervkészítő megoldását mutatja. Hogy az inkább egy ütemezési, mintsem egy tervkészítési feladat legyen, minden cselekvéshez meg kell határoznunk, hogy mikor kezdődjön és mikor végződjön. Ez azt jelenti, hogy figyelnünk kell minden cselekvés hosszára, csakúgy, mint a sorrendjére. Az Időtartam(d) jelölés egy cselekvés kapcsán (ahol d csak egy számmal helyettesíthető be) azt jelenti, hogy egy cselekvés elvégzéséhez d percre van szükség.
A „probléma” csak az, hogy az 1-es játékos egyenletében is ott van a p22, amit a 2-es játékos befolyásol, illetve a 2-es játékos egyenletében meg ott van p12, amit pedig az 1-es játékos befolyásol. Tehát sajnos ezt a feladatot nem tudjuk klasszikus maximumkeresési módszerekkel megoldani. Nem tudjuk azt mondani, hogy az ax+b alakú egyenletben a és b konstans, és így ennek fényében határozzuk meg azt az x-et, aminél ax+b értéke maximális. Ezt nem tehetjük, hiszen x (mondjuk p12) megjelenik a másik játékos egyenletében, amelyik pedig azt próbálja maximalizálni, és ezzel módosítja nálunk az a és b értékét. A fenti egyenletek tehát kvázi lineárisak.
Ezen becslések fontos jellemzője – ami egyáltalán nem magától értetődő az ábrákon –, hogy a becsléseket egyetlen Gauss-eloszlás jellemzi. A 25.  ábrán látható hibaellipszisek csupán ennek az eloszlásnak a vetületei a robot- és referenciakoordináták alterébe. Ez a többváltozós Gauss a posteriori eloszlás tartja fenn a korrelációt az összes becslés között. Fontos ez az észrevétel annak megértéséhez, hogy mi történik a  (d) ábrán. Itt a robot észrevesz egy korábban már feltérképezett referenciapontot. Ennek következtében saját pozíciójának bizonytalansága drasztikusan csökken, mint ahogy az öszszes többi referenciapont bizonytalansága is. Ez annak a következménye, hogy a robot és a pontok helyzetének becslése erősen korreláltak a Gauss a posteriori eloszlásban. Bármilyen új információ egy változóról (ebben az esetben a robot helyzetéről) automatikusan csökkenti az összes többi változó bizonytalanságát.
Wanner (1974) úgy találta, hogy a kísérlet alanyai véletlenszerűen – körülbelül 50%-os gyakorisággal – jól választottak, de inkább az elolvasott szöveg tartalmára emlékeztek, több mint 90%-os biztonsággal. Ez azt sugallja, hogy az emberek a szavakat úgy dolgozzák fel, hogy egy nem verbális reprezentációra alakítják át, amit emlékezetnek (memory) nevezünk.
a következőképpen olvasható: „Egy mondatot összeállíthatunk egy alanyi esetben álló NP-ből és az azt követő, üres alkategória-listájú VP-ből.” A  ábra bemutat egy elemzési fát, amelyik ezt a nyelvtant használja.  ábra - A „You give me the gold (Ideadod nekem az aranyat)” elemzési fája, amely bemutatja az ige és az igei kifejezés alkategóriáit A „You give me the gold (Ideadod nekem az aranyat)” elemzési fája, amely bemutatja az ige és az igei kifejezés alkategóriáit
Látható, hogy C′ valóban C alaptermje. Általánosságban ahhoz, hogy C[1]′-nek és C[2]′-nek legyen rezolvense, úgy kell létrehozni őket, hogy először a C[1]-beli és a C[2]-beli komplemens literálok legáltalánosabb egyesítőjét alkalmazzuk C[1]-re és C[2]-re. A kiterjesztési lemma felhasználásával könnyű hasonló állításokat levezetni a rezolúciós szabály alkalmazásának bármely sorozatára:
A valósvilág-beli objektív racionális viselkedéshez, vagy akár csak ennek az objektív racionális viselkedésnek az elfogadható megközelítéséhez megoldandó problémák méretéhez képest túl kicsi az emberi elme komplex problémák megfogalmazására és megoldására szolgáló kapacitása.
A környezet részlegesen megfigyelhető (nem rendelkezhetünk az internet teljes adatbázisával egyszerre), determinisztikus (bár az esetleges hálózati problémák miatt tekinthető sztochasztikusnak...), epizódszerű (a legtöbb esetben a megrendelések függetlenek, de kivételt jelenthetnek esetleges törzsvásárlói kedvezmények!), diszkrét (az internet végességéből adódóan), egyágenses (bár felkészülhetünk arra az esetre, ha több hasonló ágens próbálja megszerezni ugyanazt az egy ritka kötetet, ekkor többágenses, kompetitív).
Ezeknek a szabálycsoportoknak a kimenete egy 4. tömbbe vezet, amelyek kimenete egy összefoglaló kockázati tényezőt számító szabályrendszerbe kerül. A szabályokat úgy alakítottuk ki, hogy ez az összefoglaló kockázati tényező a [0,1] intervallumból való, és az 1 közeli számok nagyobb, a 0-hoz közeli számok kisebb kockázatot jelentenek.
A személynek mind a műtét során tanúsított külső viselkedése, mind belső tapasztalatai érdekelnek minket. A kísérlet alanyának külső viselkedésének definíció szerint változatlannak kell maradnia az operáció nélkül megfigyelhető viselkedéshez képest.^[281] A kísérlet alanyának azonban egyben képesnek kell lennie arra, hogy – noha a tudatosság hiányát vagy jelenlétét egy harmadik személy nehezen állapíthatja meg – beszámolót adjon a tudatos tapasztalatát ért változásokról. A történtekre vonatkozó különböző intuícióink nyilván ellentmondanak egymásnak. Moravec szerint, aki robotkutató és funkcionalista, változatlan maradna a tudata. A filozófus és biológiai naturalista Searle azonban ugyanilyen erősen vallja, hogy a tudata eltűnne:
- Teljesítmény. Az első és második séma közötti különbség akkor tűnik fel, ha a legközelebbi csúcs, ahonnan folytatni szeretnénk az utat, egy akadály túloldalán van, ami azt eredményezheti, hogy az eléréséhez nagyot kell kerülni. Ez nem fordulhat elő az eredeti útiterv előző pontjával. Előfordulhat azonban olyan eset is, hogy az optimálisként megtalált út egy akadályt az egyik oldalról kerülne meg, a hiba miatt a másik oldalon indultunk el, de visszamenni az akadály elejéhez költségesebb lenne, mint azon az oldalon megkerülni, amerre már elindult az ágens. Ilyen esetben mind a második, mind a harmadik módszer jobbnak bizonyulhat.
Láthatjuk tehát, hogy az 1-es játékosnak u és v a stratégiája, míg a 2-esnek x, y, és z. Láthatjuk továbbá a játékosok kifizetését is az összes lehetséges stratégia-kombináció esetén. Határozzuk meg, hogy van-e bármelyik játékosnak dominált stratégiája!
 Megjegyzés Tekintse a  ábrán látható porszívóvilágot. a. A fejezetben ismertetett algoritmusok közül amelyik lenne alkalmas e probléma megoldására? Szükséges-e, hogy az algoritmus ellenőrizze az ismételt állapotokat? b. A 3 × 3 világban alkalmazza a megválasztott algoritmust az optimális cselekvési szekvencia kiszámítására egy olyan kezdeti állapottal, ahol a felső három négyzet mindegyikében található piszok, és az ágens a középső négyzetben tartózkodik. c. Építsen kereső ágenst, és értékelje a teljesítményét egy olyan 3 × 3 porszívóvilág halmaz számára, ahol minden négyzetben 0,2 a kosz valószínűsége. A teljesítmény mérésénél mind az útköltséget, mind a keresési költséget vegye figyelembe, egy értelmes átszámítási tényezőt használva. d. Hasonlítsa össze az eddigi legjobb kereső ágens viselkedését egy olyan egyszerű, randomizált reflexszerű ágensével, amely koszt szív, ha azt megtalálja, különben véletlen módon mozog. e. Értékelje, hogy mi történne, ha a világot n × n-re nagyítanánk. Hogyan változna a kereső és a reflexszerű ágens teljesítménye n növelésével?
Amit még talán érdemes megemlítenünk: amíg Nash bizonyítása, bár jóval általánosabb Neumann bizonyításánál, sajnos nem konstruktív. Neumann bizonyítását használva azonban akár meg is konstruálhatjuk egy 2-szereplős, zéró-összegű játék Nash-egyensúlyát. Azaz a Minimax tétel bizonyításából egy algoritmus is adódik e játékok egyensúlyának kiszámítására, azaz e játékok megoldására, míg Nash tétele egzisztenciális. Az említett Minimax algoritmussal most nem foglalkozunk bővebben, viszont a Minimax tételt bebizonyítjuk.
okozati kapcsolattal. Hogy feloldjuk ezt az ütközést, egy rendezési megkötést illesztünk be, ami az OtthagyÉjszakára cselekvést az Eltávolít(Pótkerék, Csomagtartó) elé rendeli. Az így előálló terv a  ábrán látható. (Miért oldja ez fel az ütközést és miért nincs erre más lehetőség?) 4. Most az egyetlen fennmaradó nyitott előfeltétel az Ott(Pótkerék, Csomagtartó), ami az Eltávolít(Pótkerék, Csomagtartó) cselekvés előfeltétele. Az egyetlen cselekvés, ami ezt elérheti, a meglévő Indít cselekvés, de az Indít és az Eltávolít(Pótkerék, Csomagtartó) közötti okozati kapcsolat ütközik az OtthagyÉjszakára cselekvés ¬Ott(Pótkerék, Csomagtartó) következményével. Most nincs mód arra, hogy feloldjuk az ütközést az OtthagyÉjszakára cselekvéssel: nem sorolhatjuk az Indít elé (mert semmi nem kerülhet az Indít elé), és nem rendelhető az Eltávolít(Pótkerék, Csomagtartó) után (mert már van egy rendezés, ami elé helyezi). Ezért muszáj visszalépnünk, törölnünk az Eltávolít(Pótkerék, Csomagtartó) cselekvést és a két utolsó okozati kapcsolatot, és visszalépnünk a  ábrán bemutatott állapothoz.
Az 1989-ben elkezdett kutatások rövidtávú célja a győzelem az akkori világbajnok ellen, hosszútávú célja pedig a játék teljes kielemzése az összes lehetséges kombinációjával együtt. Nyilvánvalóan ez egy óriási adatbázis- kezelést igénylő feladat, a CHINOOK végleges sikeréhez természetesen hozzájárult a számítógépes technika gyors fejlődése és a párhuzamosan egyszerre futtatható műveletek helyes meghatározása [3].
Az útkereső algoritmusok másik fő csoportja a szkeletonizálás (skeletonization) elvén alapul. Ezek a módszerek a robot szabad terének egydimenziós leképezésével dolgoznak, amely alapján a tervezési probléma már könnyebben megoldható. Ezt az alacsonyabb dimenziójú reprezentációt hívják a konfigurációs tér vázának (skeleton).
Látható, hogy az előállt (Balra, Balra) egyensúly szub-optimális, hiszen a (Jobbra, Balra) eredményeképp a (10,4) kimenetel állna elő, ami mindkét játékos számára pontosan kétszer hasznosabb, mint a mostani, (5,2) kifizetésű kimenetel. De hiába, a nem-kooperatív viselkedés gyakorta szub-optimális kimeneteleket eredményezhet.
A Fuzzy Logic Toolbox a nevében hordozza a rendeltetését: egy olyan toolbox, ami a Scilabhoz rendelhető és fuzzy szabályozásra használható. Jelenleg a sciFLT 0.2 terjedt el leginkább. Hátránya, hogy csak a Scilab 3.0-ás verzióval működik, ahol még nem az Xcos volt a grafikai toolbox, hanem az annál egyszerűbb felépítésű Scicos, de amennyiben a felhasználó ismerős az Xcos vagy a Simulink környezetben, úgy el tud boldogulni a Scicosszal is.
A Kérdez-zel végrehajtjuk ezt a lekérdezést, és vissza fogunk kapni egy lekötési listát, mint például: {a/Megragad}. Az ágensprogram ezután visszaadhatja a Megragad-ot, mint elvégzendő cselekvést, de először a Kijelent felhasználásával közölnie kell a saját tudásbázisával, hogy egy Megragad-ot hajt végre.
Tekintsük a következő logikai rejtvényt: Öt különböző színű házban öt különböző nemzetiségű személy él, és mindegyikük más márkájú cigarettát, más italt és más háziállatot szeret. Az alábbi tények alapján a megválaszolandó kérdés a következő: „Hol lakik a zebra, és melyik házban isznak vizet?”
Nézzünk meg néhány példát arra, hogy az Egyesít-nek hogyan kell viselkednie. Tételezzük fel, hogy van egy Ismer(János, x) lekérdezésünk: kit ismer János? Erre a kérdésre néhány választ úgy találhatunk, hogy megkeressük az összes mondatot a tudásbázisban, amely egyesíthető az Ismer(János, x)-szel. Itt vannak az egyesítési eredmények négy különböző, a tudásbázisban előforduló mondatra.
Alkalmazási területeit tekintve, a jegykiválasztás régóta alkalmazott módszer az adatbányászat és szövegbányászat [2] terén. Mindemellett az elmúlt évtizedben a jelentős fejlődést elért biotechnológiát kiszolgáló bioinformatika egyik alapvető technikájává vált. A microarray analízis, a genomszekvencia vizsgálat és az egynukleotidos polimorfizmusok elemzése terén vezető szerepre tett szert.[12]
Vesszük a lista első elemét, az (SA, NSW) élet, és ellenőrizzük, hogy konzisztens-e. Megállapíthatjuk, hogy konzisztens, mivel SA egyetlen RED értékével konzisztens NSW egyetlen GREEN értéke. Az élet tehát töröljük a listáról, és jöhet a következő, (Q, NSW) él.
A feladat reprezentálható a szokásos módon, ahogy rejtvényújságokban megtalálható: egy részleges táblázatként, amelynek oszlopai illetve sorai az egyes tulajdonságok (kedvelt ital, háziállat, kedvelt cigaretta, stb.), és a táblázat mezőibe - illetve + jelek kerülhetnek. Ez precízebben megfogalmazva azt jelenti, hogy a változók a tulajdonságok, a kényszerek pedig ilyen tulajdonság-párokra vonatkozó állításokat fogalmaznak meg.
Több kar egyidejű működtetése esetén is k hagyományos félkarú rabló közül választhatunk, azonban egy időben egyszerre m (m<k) kart működtethetünk. Mint az előző esetben, itt is csak plusz feltételek mellett lehetséges a feladat optimális megoldása előrefele következtetéssel. Ilyen feltételek lehetnek: veszteség (regret), veszteség és büntetés folyamatváltás esetén. Teneketzis et al. egy olyan feltételt fogalmaz meg, mely szerint, ha az egyes folyamatokra a Gittins indexek elég „távol” esnek egymástól, akkor az alap MAB probléma megoldása itt is optimális megoldáshoz vezet [6].
A bioinformatika olyan eljárásokat és modelleket kínál, melyek segítségével betekinthetünk a betegségek molekuláris mechanizmusaiba, és megérthetjük a betegségek lefutásában, illetve a terápiás hatásokban megfigyelhető egyéni eltérések okait. Az orvosi informatika eszköztárában olyan módszerek szerepelnek, melyek biztosítják a betegekkel kapcsolatos információk hatékony szervezését, és segítik az orvosi döntések meghozatalát . A két szakterületről származó technikák integrálásával olyan informatikai szolgáltatásokhoz jutunk, melyek hozzájárulnak a kockázatok felméréséhez, s ezek alapján az optimális tennivalók meghatározásához.
A tárgytartomány ismeretének fontossága nyilvánvaló volt a természetes nyelvfelismerés területen is. Bár Winograd természetes nyelvfelismerő rendszere, a Shrdlu igen élénk érdeklődést keltett, a szintaktikai elemzéstől való függősége néhány, a kezdeti gépi fordításban már tapasztalt problémához vezetett. A program képes volt a kétértelműségen felülkerekedni, és a névmási szerkezeteket megérteni. Ez azonban azért volt lehetséges, mert a programot kifejezetten egy adott tárgytartományhoz – a kockavilághoz – fejlesztették ki. Néhány kutató, köztük Eugene Charniak, Winograd végzős társa az MIT-ről, felvetette, hogy a természetes nyelv robusztus felismerése a világról szóló általános ismereteket és ezen ismeretek általános felhasználási módszereit igényli.
Okulva az ilyen leckén, Feigenbaum és mások a Stanfordon belekezdtek a heurisztikus programozási projektbe (Heuristic Programming Project, HPP) azzal a céllal, hogy megvizsgálják, a szakértőrendszereknek (expert systems) ez az új módszertana milyen mértékben alkalmazható az emberi szakértelem más területein. A következő komoly erőfeszítés az orvosi diagnózis területén született meg. Feigenbaum, Buchanan és dr. Edward Shortliffe vérrel kapcsolatos fertőzések diagnosztizálására fejlesztették ki a Mycin-rendszert. 450 szabályával a Mycin elérte az egyes szakértők hatékonyságát és a kezdő orvosoknál lényegesen jobb teljesítményt nyújtott. A Mycin két fő vonatkozásban különbözött a Dendral-tól. Először is, a Dendral szabályaival ellentétben, a Mycin-szabályok származtatásához nem létezett semmilyen általános elméleti modell. A szabályokat a szakértők kiterjedt kikérdezése révén kellett beszerezni, akik viszont a szabályokat könyvekből, más szakértőktől és közvetlen tapasztalatokból merítették. A másik különbség abból eredt, hogy a szabályoknak tükrözniük kellett az orvosi ismeret bizonytalanságát. A Mycin-ben bizonyossági tényezőknek (certainty factors) (lásd  fejezet) nevezett bizonytalanságkezelő mechanizmust alkalmaztak, amelyről akkortájt úgy tűnt, jól tükrözi a tényállás diagnózisra gyakorolt hatásának orvosi megítélését.
 Megjegyzés Alkalmazza egy pásztázó távolságmérővel ellátott szimulált robot esetében a Monte Carlo-helymeghatározási algoritmust. A rácstérkép és a távolságadatok elérhetők az aima.cs.berkeley.edu kódtárából. A megoldás akkor teljes, ha bemutatja a robot sikeres globális helymeghatározását.
A fenti ábra alapján a „legnépszerűbb” szűrés a tüdőszűrés. Ennek említési aránya minden korosztályban az átlag (90%) körül alakult, amely feltehetően többnyire a szűrés kötelező jellegével magyarázható.
A következők alapján kiszámíthatjuk annak valószínűségét, hogy egy „súlyosan hibás” h[r] ∈ H[rossz] hipotézis konzisztens lesz az első N példával. Tudjuk, hogy error(h[r]) > ε. Tehát annak valószínűsége, hogy egy adott példára helyes választ ad, legfeljebb 1 – ε. Ezek után az N példára vonatkozó összefüggés:
Sajnálatos módon azonban nem hagyhatjuk mindig figyelmen kívül a bizonytalanságot. Egyes problémák esetén egyszerűen túlságosan nagy! Hogyan is használhatnánk determinisztikus pályatervezést egy mobil robot irányításához, ha fogalma sincs a robotnak arról, hogy hol van? Általában, ha a robot valódi állapota nem egyezik meg a maximális valószínűségi szabály által számítottal, akkor az irányító algoritmus nem lesz optimális. Az eltérés nagyságától függően ez egy sor nem kívánatos eseményt idézhet elő, például ütközést más tárgyakkal.
Alkalmazhatunk hasonló elemzést az előző részben bemutatott wumpus világbeli következtetési példára is. Tekintsük a  (b) ábrán látható szituációt: az ágens nem észlelt semmit az [1, 1]-ben és szellőt észlelt a [2, 1]-ben. Ezek az érzetek, kombinálva az ágensnek a wumpus világ szabályaira vonatkozó tudásával ( szakasz - A tudásbázisú ágens részben található TKCSÉ-leírás), alkotja a TB-t. Az ágenst (más dolgok mellett) az érdekli, hogy vajon a szomszédos [1, 2], [2, 2], [3, 1] négyzetek tartalmaznak-e csapdát. Bármelyik a három négyzet közül tartalmazhat vagy éppen nem tartalmaz csapdát, így a példa esetében 2^3 = 8 lehetséges modell létezik. Ezt mutatja a  ábra.^[62]
A tétel bizonyításából látszik, hogy gyakorlatilag „leszimuláljuk” az eredeti G mechanizmust egy megfelelően megkonstruált G* igazmondó mechanizmussal. A tétel bizonyítása tehát ilyen értelemben konstruktív. Ha adott számunkra egy olyan G mechanizmus, amely domináns egyensúlyi döntési elv mellett képes implementálni, azaz D-implementálni egy bizonyos f KDF-et, akkor biztosan van olyan G* igazmondó mechanizmus is, amely D-implementálja f-et.
A MAS Console a Jason saját ablaka, de a szimulátor is megjelenít itt üzeneteket. Az ágensek a print belső utasítással tudnak szöveges üzeneteket kiírni, ami a hibakeresésük során igen hasznos lehet.  ábra - A szimulátor főablaka A szimulátor főablaka
Új mondatoknak a tudásbázishoz való hozzáadására, illetve a tudás lekérdezésére valamilyen eljárásra van szükségünk. Ezeknek a feladatoknak a tipikus elnevezése a Kijelent, illetve a Kérdez. Mindkét feladat tartalmazhat következtetést (inference), azaz új mondatok levezetését régiekből. A logikai ágenseknél (logical agents), amelyekkel ebben a fejezetben foglalkozunk, a következtetéssel szemben az alapvető követelmény, hogy amikor valaki Kérdez egy kérdést a tudásbázisról, akkor a válasznak következnie kell azokból a mondatokból, amit korábban a tudásbázishoz hozzáadtunk (pontosabban Kijelent-ettünk). Később a fejezetben még pontosabban leírjuk az igen fontos „következik” fogalom jelentését. Most egyelőre ez jelentsen csak annyit, hogy a következtetési folyamat során nem csak úgy egyszerűen keletkeznek dolgok.
Sajnos a valódi pókerjátéknak (pl. a 10-szereplős Texas Hold’em Poker-nek) még nem tudták meghatározni a Nash-egyensúlyát, olyannyira komplex, de várhatóan 2-nél több szereplő esetén a Nash-egyensúlyi megoldásnak, főleg ha komplexebb a játék, és így több is van (több Nash-egyensúly), úgy egy-egy Nash-egyensúlyi stratégiának közel sem lenne akkora kényszerítő ereje, mint a fentebbi 1-lapos póker esetén. Két szereplő esetén ugyanis, ha a játéknak csak egyetlen Nash-egyensúlya van, úgy amennyiben az egyik játékos a Nash-egyensúlyi stratégiája szerint cselekszik, úgy a másiknak várhatóan (vagy átlagosan) nem éri meg eltérnie a saját Nash-egyensúlyi stratégiájától, hiszen úgy semmiképpen sem profitálhat (max. veszthet). Zéró-összegű játéknál pedig, mint például a fentebbi pókerjáték esetében is, még ártani sem tud az egyik játékos a másiknak azzal, ha eltér Nash-egyensúlyi stratégiájától, hiszen amennyivel kevesebbet nyer az egyik, annyival többet nyer a másik. 2-szereplős zéró-összegű játék esetén tehát, ahol csupán egyetlen Nash-egyensúly létezik (akár tiszta, akár kevert), semmiképp sem éri meg eltérni Nash-egyensúlyi stratégiánktól, ha tudjuk, hogy a másik játékos is aszerint cselekszik.
Utólag sokan próbálták elemezni a játékot és különböző megállapításokra jutottak. Amiben látszólag egyetértettek az az, hogy Kasparov hibázott. Hogy miért, azt nem tudhatjuk. Az azonban bizonyos, hogy Kasparov nem ismerte a gép játékát, míg a gép rengeteg adattal rendelkezett ellenfeléről. Egyesek szerint Kasparov nem a megszokott játékát játszotta, hanem inkább olyan trükkökkel próbálkozott, amivel talán nehézségeket okozhat egy gép számára.
A klasszikus halmazelmélet egyértelműen kezeli egy elem és egy halmaz kapcsolatát: az elem vagy tagja az adott halmaznak vagy nem. A mindennapi élet során felmerülő viszonyokat általában nem lehet a fenti logika szerint kezelni. A "Jó tanuló vagy?" kérdésre, kisszámú szélsőséges esetektől eltekintve nem lehet igennel vagy nemmel válaszolni. A nehézséget a klasszikus halmazelmélet korlátai okozzák. Vannak nálam jobb tanulók, jobb átlaggal és rosszabb tanulók, rosszabb átlaggal. Akkor én beletartozom a "Jó tanulók" halmazába?
  ábra - (a) Egy y = (θ[1] + θ[j] + θ[2]) egyenlettel leírható lineáris Gauss-modell additív, rögzített varianciájú Gauss-zajjal. (b) E modell alapján generált 50 adatpontból álló halmaz. (a) Egy y = (θ1 + θj + θ2) egyenlettel leírható lineáris Gauss-modell additív, rögzített varianciájú Gauss-zajjal. (b) E modell alapján generált 50 adatpontból álló halmaz.
A szabály n + 1 premisszát tartalmaz: n számú p′[[i]] atomi mondatot és egy implikációt. A konklúzió a q konzekvenciára történő helyettesítés alkalmazásának az eredménye. A mi példánkra ezt így alkalmazhatjuk:
Amíg a tökéletes racionalitás még szigorúan csak a döntések racionalitására fókuszál, addig a korlátozott racionalitás már a rendszer működésére, a döntések meghozásáért felelős mechanizmusokra (is) vonatkozik. Észrevehető a racionalitás definíciójának abszolúttól relatív irányba történő elmozdulása. Amíg tehát a tökéletes racionalitás gyakorlatilag figyelmen kívül hagyja a rendszer képességeit, addig a számítható racionalitás már közvetve ezekről tesz megállapítást, míg a korlátozott racionalitás már a rendszer egészének viszonylatában definiálja a racionalitást.
Az előretekintés eredményeképp az SA változó értékkészlete üresre redukálódott, ami problémát jelent. Azt jelenti, hogy az eddigi {NSW=RED, WA=RED, NT=GREEN, Q=BLUE} behelyettesítés mellett már nincs az SA változó számára értelmes érték.
Nézzük meg alaposan ezt az állításhalmazt. Mivel a mi modellünkben János király az egyetlen király, a második mondat kijelenti, hogy ő egy személy, ahogy azt reméltük is. De mi a helyzet a többi négy mondattal, amelyek azért szerepelnek, hogy állításokat fogalmazzanak meg lábakról és koronákról? Része ez annak a jelentésnek, hogy „Minden király személy”? Valójában, a többi négy állítás is igaz a modellben, de egyáltalán nem állítanak semmit a lábak, koronák vagy akár Richárd személyes tulajdonságairól. Ez azért van, mert ezen objektumok egyike sem király. Ha megnézzük a (⇒) összekötőjel igazságtábláját ( ábra), láthatjuk, hogy az implikáció igaz, amikor a premisszája hamis – tekintet nélkül a konklúzió igazságtartalmára. Így tehát az univerzális kvantorral ellátott mondat kijelentése egyenértékű azzal, hogy egyedi implikációk teljes listáját definiáljuk. Végül is egy szabály konklúzióját adjuk meg azokra az objektumokra, amelyek premisszája igaz, de nem mondunk egyáltalán semmit azokról az egyedekről, akikre nézve a premissza hamis. Így az implikáció igazságtáblájának sorai tökéletesnek bizonyulnak univerzális kvantorokat tartalmazó általános szabályok megírásához.
Például: * UMLS (Unified Medical Language System), melynek célja a szakirodalmi adatbázisok és az e-kórlap összekapcsolása, valamint intelligens alkalmazások fejlesztése. * SNOMED (Systematised Nomenclature in Medicine), a legrészletesebb orvosi kódrendszer, ami ellentétben az elterjedt klasszifikációkkal (BNO és változatai), nem epidemiológiai statisztikai célokat szolgál (nem egy populáció egészségi állapotának jellemzésére készült), hanem egyes személyek egészségi állapotának lehető legrészletesebb kódolt (formális) leírását igyekszik lehetővé tenni. * MeSH (Medical Subject Headings) segítségével mintegy 5 000, túlnyomórészt angol nyelvű indexelt folyóiratban kereshetünk (1950-ig visszamenően). Az adatbázist ma már több szolgáltatótól és felhasználói felületről elérhetjük. * ICD (International Classification of Diseases), vagyis a Betegségek Nemzetközi Osztályozása (magyarul: BNO) az Egészségügyi Világszervezet (WHO) betegségosztályozási rendszere, az egyik legrégebben keletkezett, mai napig általános használatban lévő és folyamatosan fejlődő orvosi kódrendszer.
Az a gondolat, hogy az induktív tanulás kivitelezhető inverz dedukcióval W. S. Jevonsig vezethető vissza (Jevons, 1874), aki azt írta, hogy: „Mind a formális logika, mind a valószínűség-elmélet tanulmányozása arra az álláspontra juttatott engem, hogy egy olyan dolog, mint egy külön indukciós módszer, a dedukcióval szembeállítva nincs, és az indukció egyszerűen a dedukció egy fordított alkalmazása.” A számítástudományi elemzések Gordon Plotkin figyelemre méltó PhD-disszertációjával kezdődtek Edinburghban (Plotkin, 1971). Bár Plotkin számos, a mai ILP területén használt tételt és módszert dolgozott ki, az indukció egyes részproblémáira vonatkozó bizonyos eldönthetetlenségi eredmények elvették a kedvét. A MIS (Shapiro, 1981) újból bevezette a logikai programok tanulási problémáját, azonban ezt főleg az automatikus hibakeresés elméletéhez való hozzájárulásának tekintették. A szabályindukció kutatása, vagyis az olyan rendszerek, mint az ID3 (Quinlan, 1986) és CN2 (Clark és Niblett, 1989) a Foil-hoz vezettek (Quinlan, 1990), amely első ízben tette lehetővé relációs szabályok gyakorlati indukcióját. A területet Muggleton és Buntine pezsdítették fel (Muggleton és Buntine, 1988). Cigol programjuk az inverz rezolúció nem egészen teljes változatát tartalmazta, és új predikátumok generálására is alkalmas volt.^[193] A predikátum felfedezésével Wirth és O’Rorke is foglalkoztak (Wirth és O’Rorke, 1991). A következő nagyobb rendszer a Golem volt (Muggleton és Feng, 1990), amely a Plotkin-féle relatíve legkevésbé általános általánosítás ötletén alapuló lefedő algoritmust használja. A Foil fentről lefelé módon, míg a Cigol és a Golem lentről felfelé módon dolgoztak. E korszak más rendszerei az Itou (Rouveirol és Puget, 1989) és a Clint (De Raedt, 1992) rendszerek voltak. Újabban a Progol (Muggleton, 1995) az inverz vonzatreláció egy hibrid (fentről lefelé és lentről felfelé) megközelítését valósította meg, és a rendszert számos gyakorlati problémára alkalmazták, különösképpen a biológiában és a természetes nyelvfeldolgozásban. Muggleton a Progol egy kiterjesztését írja le (Muggleton, 2000), sztochasztikus logikai programok alakjában reprezentált bizonytalanság kezelésére.
1. lépés: ebben a lépésben  szakasz -  Visszalépéses keresés előretekintéssel az NSW=RED értékadást tette. Ugyanez a választás történik akkor, ha LCV heurisztika szerint választunk értéket, mivel itt az NSW változó {RED, GREEN, BLUE} értékkészletének mindegyik értéke összesen ugyanannyi értéket „zárna ki” a szomszédai (Q, SA, és V) értékkészletéből. Ha tehát a RED értéket választjuk az NSW változó számára, akkor mindhárom szomszéd esetében a későbbiekben inkonzisztencia adódna, ha ezek után bármelyiküknek is a RED értéket adnánk, azaz az NSW=RED értékadással kiegészítve az eddigi üres behelyettesítést, összesen 1+1+1=3 értéket zárnánk ki a szomszédok értékkészletéből. De ugyanez igaz a GREEN-re és a BLUE-ra is. Tehát az NSW változó értékkészletének mindhárom eleme összesen 3-3 értéket zárna ki az NSW változó szomszédainak értékkészletéből. Az LCV heurisztika szerint tehát az NSW változó mindegyik értéke egyenrangú, így ezek közül az első, a RED érték kerül kiválasztásra. Ezért van tehát az LCV-vel kiegészített, előretekintéssel ellátott visszalépéses keresésnél is első lépésben NSW=RED értékadás. Röviden mindezt úgy írhatnánk, és így is fogjuk írni a későbbiekben, hogy: NSW{RED(3), GREEN(3), BLUE(3)}. Ebből tehát az látszik, hogy az LCV heurisztika első lépésben milyen heurisztikus értéket rendel a kiválasztott NSW változó értékkészletének egyes elemeihez - mindegyikhez 3-at (mivel az NSW szomszédainak értékkészletéből 3-3 értéket zárnának ki), és mivel itt nincs egyértelmű minimum, ezért az első, RED érték kerül kiválasztásra.
Amellett hogy egy teljes és nem redundáns reprezentációja a tárgytartománynak, egy Bayes-háló gyakran sokkal tömörebb, mint az együttes valószínűség-eloszlás függvény. Ez a tulajdonsága teszi használhatóvá a sokváltozós tárgyterületek kezelésében. A Bayes-háló tömörsége a lokálisan strukturált (locally structured) (vagy ritka – sparse) rendszerek egy igen általános tulajdonságának példája. Egy lokálisan strukturált rendszerben egy komponens csak korlátos számú más komponenssel van kapcsolatban közvetlenül, függetlenül a komponensek teljes számától. Lokális struktúrákhoz általában inkább a lineáris, mint az exponenciális komplexitásnövekedés kapcsolható. A Bayes-háló esetében jogos azt feltételezni, hogy a legtöbb tárgytartomány esetén egy valószínűségi változót csak k számú más változó befolyásol, ahol k konstans. Ha bináris változókat tételezünk fel az egyszerűség kedvéért, akkor az egy csomóponthoz tartozó feltételes valószínűségi tábla megadásához legfeljebb 2^k érték szükséges, így a teljes háló n2^k értékkel megadható. Ezzel szemben az együttes valószínűség-eloszlás függvény 2^n értéket tartalmaz. Egy konkrét példával élve, ha 30 csomópontunk van (n = 30), és mindegyiknek legfeljebb 5 szülője van (k = 5), akkor a Bayes-háló 960 számot igényel, míg az együttes valószínűség-eloszlás függvény több mint egy milliárdot.
A távgyógyászat fejlesztésével csökkenteni lehet a kórházak egyes működési költségeit, hiszen a beteget nem kell befektetni hosszú megfigyelésre. Lehetővé válik a beteg állapotának folyamatos követése, az orvosi vizitekre utazás nélkül, amely sok esetben hatalmas terhet ró a betegre.
Írjon le egy mondatot, amely azt állítja, hogy a + egy kommutatív funkció. Következik ez a mondat Peano axiómáiból? Ha igen, magyarázza meg, hogy miért, ha nem, adjon meg egy modellt, amelyben az axiómák igazak, az Ön mondata pedig hamis.
Oktatóprogramok illetve Mesterséges Intelligenciák készítése során érdemes figyelembe venni a fenti eredményeket. Az ember és gép kapcsolatát sokkal kényelmesebbé lehet tenni, ha a gép képes megérteni a vicceket, és esetleg képes azokra egy másik viccel reagálni.
Az így összeállított bemeneti változó értékekre elvégeztetjük a rendszerrel a kimeneti változó becslését. Azért fogunk közelítőleg jó eredményt kapni, mert léteznek olyan szabályok, amik erre az állapotra is vonatkoznak. Többek között a következő bemeneti állapottal rendelkező szabály is ilyen:
Például feltételezzük azt, hogy a tudásbázis tartalmaz egy új β állítást, amely azt mondja ki, hogy pontosan 8 csapda van a világban. Ez a tudás segítheti az ágenst további konklúziók levezetésében, de nem teheti érvénytelenné egyik korábban kikövetkeztetett α konklúziót sem – így azt a konklúziót sem, hogy nincsen csapda az [1, 2]-ben. A monotonitás azt jelenti, hogy a következtetési szabályok bármikor alkalmazhatók, ha a megfelelő premisszák megtalálhatók a tudásbázisban – a szabály konklúziójának következnie kell, függetlenül attól, hogy mi más is van még a tudásbázisban.
A Bellman-egyenletek képezik az alapját az MDF-ek megoldására szolgáló értékiteráció algoritmusnak. Ha n lehetséges állapot van, akkor n Bellman-egyenlet létezik, mindegyik állapotra egy. Az n egyenlet n ismeretlent tartalmaz – az állapotok hasznosságát. Így a hasznosságok megállapításához ezen egyenletek együttesét szeretnénk megoldani. Azonban van egy probléma: az egyenletek nemlineárisak, mivel a „max” operátor nemlineáris operátor. Míg a lineáris egyenletrendszerek hatékonyan megoldhatók lineáris algebrai eszközökkel, a nemlineáris egyenletrendszerek már problematikusabbak. Ekkor iteratív módszerrel próbálkozhatunk. Kezdéskor tetszőleges kezdeti értékeket választunk a hasznosságoknak, kiszámítjuk az egyenletek jobb oldalát, majd behelyettesítjük a bal oldalra – így frissítve az egyes állapotok hasznosságát a szomszédjainak a hasznosságával. Ezt addig ismételjük, ameddig el nem érünk egy egyensúlyi helyzetet. Jelölje U[i](s) az s állapot hasznosságértékét az i-edik iterációban. A Bellman-frissítésnek (Bellman update) nevezett iterációs lépés a következőképpen néz ki:
ami megfelel a Loves(John, Mary)-nek.  ábra - Egy nyelvtan, amely képes egy elemzési fa és szemantikai értelmezés levezetésére a „John loves Mary” (és három másik) mondat számára Egy nyelvtan, amely képes egy elemzési fa és szemantikai értelmezés levezetésére a „John loves Mary” (és három másik) mondat számára  ábra - A „John loves Mary” füzér elemzési fája szemantikai értelmezésekkel A „John loves Mary” füzér elemzési fája szemantikai értelmezésekkel
A sarokpont keresés legtriviálisabb megoldása egy sarok képpel való korreláció vizsgálata, ez azonban számításigényes megoldás. Épp ezért számos speciális sarokpont detektor alakítottak ki, melyek közül néhányat ismertetek a teljesség igénye nélkül. Ezek az algoritmusok keverhetőek, egyszerre több is alkalmazható, így változatos tulajdonságú struktúrák detektálhatók. A valóságban a genetikus programozást – a genetikus algoritmus (GA) általánosítását programokra – alkalmazzák az optimális algoritmuskombináció megtalálására. A GA-hoz felhasznált fitnesz függvény előnyben részesíti a megismételhető algoritmusokat – azokat, melyek hasonló képeken megtalálják az egymásnak megfelelő pontokat – és azokat, melyek kimenetén a jellemző pontok a térben egyenletesen fordulnak elő.
Ilyen módon az eredeti útkeresési feladatot a Voronoi-gráfon lévő útvonal megtalálására redukáltuk. A Voronoi-gráf (néhány különleges esetet leszámítva) egydimenziós, és véges sok olyan pontja van, ahol három vagy több görbe metszi egymást. Így azon a legrövidebb út megtalálása egy diszkrét gráfkeresési probléma, amilyet a 3. és  fejezetekben már tárgyaltunk. Ha a Voronoi-gráf mentén haladunk, akkor nem a legrövidebb utat kapjuk, de az egyik legbiztonságosabbat. A hátránya ennek a módszernek, hogy nehéz többdimenziós konfigurációs terekre alkalmazni, és sokszor túl nagy kitérőket tesz, főleg ha a szabad tér nagyon tág. Ezenfelül a Voronoi-diagram számítása bonyolult lehet a konfigurációs térben, különösen ahol az akadályok alakja nagyon összetett.
T(Benne(Shankar, NewYork), Ma)  ábra - Összetett események ábrázolása. (a) T(Mindkettő(p, q), i), másképpen T(p o q, i), (b) T(Egyik(p, q), i), (c) T(VagyVagy(p, q), i). Összetett események ábrázolása. (a) T(Mindkettő(p, q), i), másképpen T(p o q, i), (b) T(Egyik(p, q), i), (c) T(VagyVagy(p, q), i).
d) Könnyen el tudunk képzelni nagy negatív költségű operátorokat még az útkeresési probléma esetén is. Például néhány útszakasz olyan gyönyörű tájakon halad, hogy az idő és üzemanyag tekintetében messze túlszárnyalja a normál költségeket. Pontos megfogalmazásban magyarázza el, hogy az emberek miért nem kocsikáznak a végtelenségig a szép tájak körül, és hogyan lehetne az útkeresési problémához az állapotteret és az operátorokat definiálni, hogy a mesterséges ágens is el tudja kerülni a ciklusokat.
A probléma tárgyalását az elméletileg lehetséges legjobb lépés definíciójával és ennek egy keresőalgoritmusával kezdjük. Ezek után olyan technikákat nézünk meg, amelyek korlátozott idő alatt is alkalmasak egy jó lépés megválasztására. A nyesés vagy metszés (pruning) lehetővé teszi számunkra, hogy figyelmen kívül hagyjuk a keresési fa azon részeit, amelyek nincsenek befolyással a végső választásra. A heurisztikus kiértékelő függvények (evaluation functions) lehetővé teszik, hogy kimerítő keresés nélkül meg tudjuk becsülni egy adott állapot valódi hasznosságát. A  alfejezet olyan kétszemélyes játékokat tárgyal, amelyekben a véletlen is megjelenik, mint például az ostábla.^[55] Foglalkozunk a briddzsel is, amely tartalmazza a hiányos információ (imperfect information) elemeit, hiszen egy-egy játékos számára az összes kártya nem ismert. Megnézzük végül, hogy a jelenlegi legfejlettebb játékprogramok hogyan győzik le az erős emberi ellenfeleket, és milyenek a jövőbeli trendek.
a válasz tehát: érvényes! (A, B, C változók minden kombinációjára, azaz minden lehetséges interpretációban igaz. Ne felejtsük, hogy A, B, C változók különböző interpretációi a változók különböző logikai értékeiben nyilvánulnak meg, de ha a végeredmény azoktól független, akkor az állítás interpretáció-független).
Eddig csak egylépéses játékokat vizsgáltunk. A legegyszerűbb többlépéses játék az ismétlődő játék (repeated game), amelyben a játékosok újra és újra ugyanazzal a választással szembesülnek, de az egyes alkalmakkor már ismert az összes játékos korábbi választásainak a története. Az ismétlődő játéknál egy stratégiaprofil egy cselekvésválasztást ad meg mindegyik játékos számára, minden időpontban és a korábbi választások minden lehetséges történetére. Ahogyan az MDF-eknél, a jutalmak az időben additívak.
Példaként arra, hogy mire képesek az n-gram modellek, vegyük a szegmentáció (segmentation) feladatát, ami szóhatárok megtalálása szóköz nélküli szövegben. Ez a feladat elengedhetetlen a japán és kínai nyelv esetén; ezek olyan nyelvek, amelyek nem raknak szóközt a szavak közé, feltételezzük azonban, hogy a legtöbb olvasónak az angol nyelv^[237] jobban megfelel. Az
Lehetséges egy Horn-program Clark-lezárását képezni, majd a következtetések levonása végett egy tételbizonyítónak átadni. Hatékonyabb azonban általában egy olyan speciális rendeltetésű következtető gépet használni, mint amilyen a Prolog, amelynél a zárt világ és az egyedi elnevezések feltételezések a következtetési mechanizmusba be vannak építve.
- Az új helymeghatározási eljárást minden olyan ponton be kell építeni az ágensbe, ahol annak ezelőtt is meg kellett keresnie a jelenlegi pozícióját a "térképen". Ez az egyszerű esetben mindössze az iteráció kezdetén szükséges, a bonyolultabb esetben (ahol a robot hibázhat) viszont minden lépés után ellenőrizni kell, hogy a megfelelő helyen van-e, ezért a megoldás összes cselekvése után végre kell hajtani. Szélsőséges esetben ez nagyon költséges lehet.
A tökéletesen racionális ágens tehát olyan, hogy az általa megvalósított ágens-függvény legalább annyira hasznos az adott probléma-környezetben, mint bármelyik másik lehetséges ágens-függvény. A kérdés már csak az, hogy vajon nem-triviális ágens-környezetek esetén is megvalósítható-e ez a tökéletesen racionális ágens-függvény? Adott ágens architektúra esetén könnyen elképzelhető, hogy f[opt] olyan, hogy az architektúrán futtatható programok között nincs olyan, amely az f[opt] ágens-függvénynek megfelelő működést valósítaná meg. Ekkor azt mondjuk, hogy az ágens-függvény nem megvalósítható.
A következő kérdés, hogy hogyan kódoljuk a cselekvés leírásokat az ítéletlogikában. A legegyszerűbb megközelítés, hogy minden cselekvés megjelenésére egy ítéletlogikai szimbólumot vezetünk be. Például a Repül(P[1], SFO, JFK)^0 igaz, ha a P[1] repülőgép az SFO-ról a JFK-ra repül a 0. időpillanatban. Csakúgy, mint a  fejezetben, felírjuk az ítéletlogikai változatait a következő állapotaxiómáknak, melyeket a  fejezetben a szituációkalkulushoz fejlesztettünk ki. Adott például az
Az első gondolat az, hogy éppen úgy, ahogyan egy egzisztenciális kvantorral ellátott mondatot felcserélhetünk a mondat egy példányával, egy univerzális kvantorral ellátott mondatot is felcserélhetünk a mondat összes lehetséges példányosításainak halmazával. Például tételezzük fel, hogy a tudásbázisunk mindössze a következő mondatokat tartalmazza:
Az ágens szó a latin ago, agere („mozgásban lenni”, „mozgásba hozni”, „elintézni”) igéből származik. Egy ágens bármi lehet, amit úgy tekinthetünk, hogy az érzékelői segítségével érzékeli a környezetét, és beavatkozói segítségével megváltoztatja azt”
Természetesen tisztázni kell, hogy mit is jelölhet A és B. Ha az ágens cselekvései determinisztikusak, akkor A és B tipikusan a cselekvések konkrét, teljesen specifikált eredményállapotai. Az általánosabb, nemdeterminisztikus esetben A és B szerencsejátékok (lotteries). Egy szerencsejáték alapvetően egy valószínűségi eloszlás az aktuális következmények halmaza felett (melyek a szerencsejáték „díjai”). Az L szerencsejátékot a C[1], …, C[n] következményekkel, amelyek p[1], …, p[n] valószínűségekkel következhetnek be, a következőképpen jelöljük:
A legközelebbi-szomszéd modellek legalább Fix és Hodges (Fix és Hodges, 1951) munkájáig nyúlnak vissza, és azóta a statisztika és alakfelismerés standard eszközei. Az MI-n belül Stanfill és Waltz népszerűsítették ezeket a modelleket (Stanfill és Waltz, 1986), ők a távolságmetrika adatokhoz történő adaptálási módszereivel foglalkoztak. Hastie és Tibshirani kifejlesztettek egy módszert, amellyel a tér egyes pontjaihoz kötötték az ezen pont körüli adateloszlástól függő metrikát (Hastie és Tibshirani, 1996). A legközelebbi-szomszédok hatékony indexelési sémával történő megtalálásával az algoritmusokat kutató közösség foglalkozott (pl. Indyk, 2000). A kernelsűrűség-becslést, amelyet Parzen ablak (Parzen window) sűrűségbecslésnek is neveznek, kezdetben Rosenblatt és Parzen tanulmányozta (Rosenblatt, 1956; Parzen, 1962). Azóta óriási az irodalma a különböző becslők tulajdonságai vizsgálatának. Devroye alapos bevezetést nyújt ehhez a témához (Devroye, 1987).
Mivel időnként minden kapcsolási rajzban meg kell törnünk a vezetéket, szükségünk van „kanyar” elemre is, azaz olyan elemre, amihez egy vízszintes és egy függőleges vezeték csatlakoztatható, és a kimenetén ugyanaz az érték jelenik meg, mint a bemenetén. Egy lehetséges megvalósítás látható a  ábrán.  ábra - Kanyar elem Kanyar elem
Ahhoz, hogy kimondhassuk egy-egy ilyen f KDF mechanizmus általi implementációját (adott környezetben), előbb még definiálnunk kell a játékosok viselkedésére vonatkozó feltételezéseinket. Ezt jelöli az S játékelméleti döntési elv (pl. Domináns egyensúly, Nash-egyensúly, vagy éppen Bayes-i Nash-egyensúly).
A cselekvésdekompozíciós módszerek általános leírásait egy tervkönyvtárban (plan libary) tároljuk, ahonnan kinyerhetők és a készülő terv igényeinek megfelelően felhasználhatók. Minden módszer egy Dekomponál(a, d) formájú kifejezés, amelynek jelentése, hogy egy a cselekvés dekomponálható a d tervbe, mely egy – a  alfejezetben leírtaknak megfelelő – részben rendezett tervként van megadva.
Hagyományosan az aktív tanulás során az egyes minták esetén ismertek, vagy kis költséggel hozzáférhetőek a változóértékek (x). A változóérték lekérdezés esetében ezek az információk hiányosak, egyes mintákhoz nem feltétlenül adott minden változóhoz érték. Ebben az esetben úgy módosul a feladat, hogy egy lekérdezéssel egy változó egyetlen mintához tartozó értékét kérdezzük le, így nem csak a megfelelő mintát kell megválasztani, hanem a megfelelő változót is. Egy ilyen problémára ad megoldást Olivetti et al. [5], azzal az eltéréssel, hogy az általuk felvázolt esetben minden mintához adott volt a címke is. Olivetti et al. ráadásul nem egy „legjobb” modellt akar tanulni a meglévő adathalmazon, hanem a címkézésre releváns változókat kiválasztani a lehető legkevesebb információ alapján.
Az aknakereső egy népszerű egyszemélyes logikai játék, amely szinte minden mai személyi számítógépen megtalálható. A játék első változatai a 80-as évek elején jelentek meg, ma ismert formájában 1990-ben, a Microsoft cég Windows Entertainment Pack nevű játékgyűjteményének részeként látott napvilágot. A játék (a pasziánsszal egyetemben) a Windows 3.1 kiadása óta része az operációs rendszernek, így felhasználók milliói játsszák rendszeresen. A legtöbb számítógépes játékkal ellentétben az aknakereső során a játékosnak sokkal inkább az eszére, semmint ügyességére kell hagyatkoznia. A játék elé egyszerű ahhoz, hogy könnyedén meg lehessen érteni a szabályokat, ugyanakkor elég komplex ahhoz, hogy ne váljon gyorsan unalmassá. A későbbiekben látni fogjuk, hogy a játék még egy számítógép számára is kellően bonyolult, jelenleg nem ismert hatékony algoritmus, amely képes lenne hibátlanul megoldani minden aknakereső-táblát.
Vizsgálja a láz  alfejezetben bemutatott zajos-VAGY modelljét. Magyarázza meg, hogyan alkalmazható a maximum-likelihood tanulás arra, hogy egy teljes adathalmazra illesszük egy ilyen modell paramétereit. (Segítség: a parciális deriváltakra használja a láncszabályt.)
így megadjuk az univerzum részét képező x=150:0.1:190; pontsorozat elemeire a Magas tagsági függvény rájuk kiszámított értékeit, és megrajzolhatjuk a két pontsorozat alapján plot paranccsal a függvényt.
Ebben az esetben vissza kell lépnünk (24. lépésben) és az előző oszlopban tartózkodó királynőnek új helyet kell keresnünk. Szerencsére találunk számára alkalmas helyet. Az áthelyezést szemlélteti a következő ábra:
A következtetés harmadik sora önmagában is hasznos tény és kiterjeszthető a Boole-típusú logikai esetből az általános diszkrét esetre. Legyen 〈d[1], …, d[n]〉 a D diszkrét változó értelmezési tartománya. Könnyű megmutatni ( feladat), hogy
Az eset annak felel meg, hogy akkor nyerünk egy érmedobás sorozatban, ha fej mellett lesz benne írás is. A vesztes sorozat a fej, fej, fej, ..., fej, ... és így végtelenségig. Tudjuk, hogy egy ilyen sorozat valószínűsége rohamosan 1/2^N szerint csökken, de valószínűségről akkor beszélgethetünk, ha a hasonló problémamegoldást többször (igen sokszor) megismételhetjük. Ha csak egyetlenegy próbáról van szó (mert pl. nem a kincses kamra nyílik, hanem élelem nélkül egy kamrába be vagyunk zárva, minden kisérlettel apad az erőnk, és az életünk múlik a gyors kiszabaduláson), akkor bezzeg egy ritka esemény is meg tud fordulni, mi pedig porul fogunk járni.
Az AdaBoost tanuló algoritmus egyetlen hipotézissé kombinálja össze az iterációk során meghatározott „gyenge” hipotézisek becslését. A „gyenge” hipotézisek összekombinálását az  ábra szemlélteti. Vegyük észre, hogy egyszerű koordináta tengelyek szerinti vágások összekombinálása esetében az algoritmus teljesen el tudta különíteni a példában szereplő egyedeket, tehát az AdaBoost algoritmus alacsony komplexitású „gyenge” osztályozók (Decision Stump) hipotéziseit összekombinálva megbízható eredményt adhat. Az ábrán vastag vonal jelöli a vágási felszínt, és a színárnyalatok jelölik a „gyenge” hipotézisek metszetének „jóságát” (becslési összegét). Látható, hogy a meghatározott vágási felszín mellett jól elkülönültek a valódi virágok a műanyagból készültektől.  ábra -  ábra. Az AdaBoost algoritmus lépései során meghatározott hipotézisek.  ábra. Az AdaBoost algoritmus lépései során meghatározott hipotézisek.  ábra -  ábra. A gyenge tanuló algoritmusok összekombinálása.  ábra. A gyenge tanuló algoritmusok összekombinálása.
A  ábra a  alfejezetben megfogalmazott négy kiértékelési kritérium tükrében összehasonlítja a keresési stratégiákat.  ábra - A keresési stratégiák értékelése. b az elágazási tényező, d a legsekélyebb megoldás mélysége, m a keresési fa maximális mélysége, ℓ a mélységkorlát. A felső indexszel jelzett kikötések a következők: ^a teljes, ha b véges; ^b teljes, ha a lépésköltség ≥ ε, pozitív ε-ra; ^c optimális, ha a lépésköltségek mind azonosak; ^d ha mindkét irányban szélességi keresést használunk. A keresési stratégiák értékelése. b az elágazási tényező, d a legsekélyebb megoldás mélysége, m a keresési fa maximális mélysége, ℓ a mélységkorlát. A felső indexszel jelzett kikötések a következők: a teljes, ha b véges; b teljes, ha a lépésköltség ≥ ε, pozitív ε-ra; c optimális, ha a lépésköltségek mind azonosak; d ha mindkét irányban szélességi keresést használunk.
Az átalakítás igen egyszerű: két csoportba vesszük a negatív és pozitív literálokat, előbbieket 'és' kapcsolattal összefogva, negációk nélkül az implikáció bal-, utóbbiakat vagy kapcsolattal az implikáció jobb oldalára írjuk. Az így kapott implikáció jobb oldalán álló klózt Q-val helyettesítve és az előbbi állítást alkalmazva látható, hogy a két mondat ekvivalens lesz.
egy vagy több feltétel törlésével három relaxált problémát hozhatunk létre: a. Egy lapka az A mezőről a B mezőre mozgatható, ha A és B szomszédosak. b. Egy lapka az A mezőről a B mezőre mozgatható, ha a B mező üres. c. Egy lapka az A mezőről a B mezőre mozgatható.
2007 júliusában hivatalosan is kijelentették, hogy tökéletes játék esetén a dámajáték eredménye döntetlen lesz [2]. Erre a következtetésre 1989 óta tartó kutatások eredményeként jutott Jonathan Schaeffer- a University of Alberta egyetem számítástechnikai tanszékének igazgatója- és csapata. A program, amellyel ezt a bizonyítást véghez vitték a CHINOOK nevet viseli. Az alábbiakban a program létrejöttéhez vezető hosszú utat szeretném bemutatni.
A szenzor nélküli problémák eddigi elemzésében determinisztikus cselekvéseket tételeztünk fel. Az elemzés lényegében érvényes marad, ha a környezet nemdeterminisztikus, azaz ha a cselekvéseknek néhány lehetséges kimenetele lehet. Ennek az az oka, hogy szenzorok nélkül az ágens nem képes megmondani, hogy milyen kimenetel történt valójában. A különböző lehetséges kimenetelek így pótlólagos fizikai állapotok a követő hiedelmi állapotban. Tételezzük fel például, hogy a környezetben Murphy-törvény uralkodik: a Szív cselekvés néha koszt is hagy a szőnyegen, de csak akkor, ha ott előzetesen a kosznak nyoma sem volt.^[32] Ha a Szív cselekvést tehát a 4. fizikai állapotban alkalmazzuk (lásd  ábra), a két lehetséges kimenetel a 2. és a 4. állapot. A kezdeti {1, 2, 3, 4, 5, 6, 7, 8} hiedelemállapotra alkalmazva a Szív olyan hiedelemállapothoz vezet, ami a cselekvés eredményeinek uniója a nyolc fizikai állapotra. Kiszámítva, azt kapjuk, hogy az új hiedelmi állapot az {1, 2, 3, 4, 5, 6, 7, 8}. A Szív cselekvés a Murphy-törvény uralta világban egy szenzor nélküli ágens hiedelmi állapotát tehát nem változtatja meg! A probléma valójában megoldhatatlan (lásd  feladat). Ennek intuitív magyarázata az, hogy az ágens nem képes megmondani, hogy az aktuális mező koszos-e, ebből kifolyólag nem képes megmondani, hogy a Szív cselekvés takarítani fog-e, vagy még több koszt hagy maga után.
Egy ősi kriptában egy oszlopot találunk, négy, egy magasságban, szimmetrikusan elhelyezett nyílással, amibe épp a kezünk fér bele. Az ősi írás az oszlopon elmondja, hogy az oszlop a kincses kamrát nyitó mechanizmus része. Minden nyílás mélyén egy kapcsoló el van rejtve, mely „fel”, vagy „le” állásban lehet. Ha mind a négy kapcsoló azonos állásban lesz, a kincses kamra ki fog nyílni. Az oszlop elegendően kicsi, hogy át lehessen ölelni, két kézzel tehát tetszőleges kapcsolópárhoz férhetünk hozzá egyszerre.
Mindazonáltal a környezet explicit ismerete nélkül is lehetséges közvetlenül szabályozót tervezni a feladathoz. (Már láttuk ezt a PD szabályozó esetében, amely képes volt a komplex robotkart a megadott pályán tartani a robot dinamikájának pontos ismerete nélkül. Ugyanakkor szükség volt a kinematikai modell alapján megalkotott referenciapályára.) A hatlábú robot esetében megfelelő absztrakciós szinten meglepően könnyű meghatározni az irányítási törvényt. A megfelelő algoritmus ciklusosan mozgathatná a lábakat. Egy ideig a levegőben mozognak, utána pedig a földön támaszkodnak. A hat lábat úgy kell koordinálni, hogy egyszerre három (ellentétes oldalakon) mindig a földön legyen, biztosítva a robot fizikai stabilitását. Egy ilyen irányítási algoritmust könnyű programozni, és remekül működik sík felületen. Nehéz terepen az egyes akadályok meggátolhatják a lábakat az előremozdulásban. Ezen a problémán egy egészen egyszerű irányítási szabállyal segíthetünk: ha a láb mozgását valami blokkolja, a robot egyszerűen húzza vissza, emelje magasabbra, és próbálja újra! Az eredményül kapott vezérlést mint véges automatát mutatja a  (b) ábra. Egy reflexszerű ágenshez állapotokat társít, ahol a belső állapot a gép aktuális állapotának indexe (s[1]-től s[4]-ig) jelöli.
A „tudja, mi” koncepció valamivel bonyolultabb. Kísértést érezhetünk arra, hogy azt állítsuk, egy ágens tudja, mi Béla telefonszáma, ha létezik egy x, amiről az ágens azt tudja, hogy x = TelefonSzáma(Béla). De ez így nem jó, mert például az ágens tudhatja, hogy Aliz és Béla telefonszáma ugyanaz (azaz TelefonSzáma(Aliz) = TelefonSzáma (Béla)), de ez sokat nem segít, ha Aliz telefonszámát sem tudja. A „tudja, mi” jobb definíciója azt mondja ki, hogy az ágensnek valami olyan x-et kell ismernie, amely egy számjegyekből álló füzér és amely Béla telefonszáma:
Az eddigiekben csupán absztrakt módon definiáltuk az IR-rendszerek működését, azonban nem magyaráztuk el, hogy hogyan lehet olyan hatékonnyá tenni őket, hogy egy webes keresőgép visszaadhassa a legfelső találatokat egy több milliárd oldalas gyűjteményből egytized másodperc alatt. Minden IR-rendszer számára két kulcsfontosságú adatstruktúra szükséges: a szókincs, amely felsorolja a dokumentumok szavait, és az invertált index, amely megadja, hogy az egyes szavak hol szerepelnek a dokumentumgyűjteményben.
A P(E|F) fordítási modellt (translation model) nehezebb meghatározni. Egyrészt nem áll rendelkezésünkre (angol, francia) mondatpárokból álló kész gyűjtemény, amely alapján taníthatnánk. Másrészt a modell komplexitása nagyobb, mivel mondatok keresztszorzatára, és nem pedig különálló mondatokra alapul. Egy túlzottan leegyszerűsített fordítási modellel fogunk kezdeni, és felépítünk valamit, ami az „IBM Model 3”-at (Brown és társai, 1993) közelíti, ami továbbra is a végletekig leegyszerűsítettnek tűnik, azonban az esetek körülbelül felében elfogadható fordításokat generált.
A 8-királynő probléma tulajdonképpen egy speciális esete az ennél általánosabb n-királynő problémának. Ezen probléma azt a kérdést veti fel, hogy hányféleképpen lehet lerakni „n” darab királynőt (vezért) egy n x n-es sakktáblán.
A versenybe benevezett csapatok beadása a Beadó rendszeren keresztül történik a verseny megfelelő fordulóját, mint feladatot kiválasztva. A beadás során egyetlen ZIP fájl feltöltését várjuk, melynek tartalmával kapcsolatban a gépi feldolgozás miatt az alábbi szigorú megkötésekkel élünk.
Az emberek nem így játszanak. A sakkban gyakran van egy konkrét célunk – például le akarjuk ütni az ellenfél vezérét –, és ezt a célt használhatjuk arra, hogy a cél elérése érdekében szelektíven megfelelő terveket generáljunk. Ez a fajta célvezérelt következtetés (goal-directed reasoning) vagy tervkészítés (planning) néha teljesen eltünteti a kombinatorikus keresést (lásd IV. rész). David Wilkins Paradise programja (Wilkins, 1980) az egyetlen olyan program, amely sikeresen alkalmazta a célvezérelt következtetést a sakkra: néhány 18 lépésből álló kombinációt igénylő sakkfeladványt is képes volt megoldani. Ez idáig azonban még nem látjuk tisztán, hogyan kellene a két algoritmustípust egyetlen robusztus és hatékony rendszerbe ötvözni, bár a Bridge Baron^TM jelenthet egy, a helyes irányban tett lépést. Egy ilyen teljesen integrált rendszer jelentős eredménynek számítana nemcsak a játékkutatás területén, hanem általánosságban véve az MI-kutatás területén is, mert egy általános intelligens ágens számára megfelelő alapot is tudna nyújtani.
Egy módosított wumpus világról fogunk példát venni, ahol az ágens orientációjával nem törődünk, és ahol az ágens egy helyről egy szomszédos helyre Megy. Tegyük fel, hogy ágens az [1, 1]-nél és arany az [1, 2]-nél van. A cél az aranyat az [1,1] helyen birtokoljuk. A folyó esemény predikátumok a Nála(o, x, s) és Tart(o, s). A kezdeti tudásbázis az alábbi leírást tartalmazhatná:
A sokdimenziós terek még egy további problémát jelentenek, nevezetesen azt, hogy egy ilyen térben a legközelebbi szomszédok rendszerint nagyon messze vannak. Vegyünk egy N elemű, d dimenziós egységkockában elhelyezkedő adathalmazt, és tegyük fel, hogy a szomszédságok b oldalú hiperkockák, amelyek térfogata b^d. (Ugyanez a megfontolás működne hipergömbök feltételezése esetén is, de a hipergömb térfogatképlete bonyolultabb.) Ahhoz, hogy k pontot tartalmazzon, az átlagos szomszédság a teljes térfogat – amit egységnyinek tekintünk – k/N-ed részét kell elfoglalnia. Ennek megfelelően b^d = k/N, vagyis b = (k/N)^1/d. Eddig jó. Legyen most d = 100, k = 10 és N = 1 000 000 értékű. Akkor b ≈ 0,89 – azaz a szomszédság majdnem az egész bemeneti teret elfoglalja! Ez arra utal, hogy sokdimenziós esetben a legközelebbi-szomszéd alapú módszerek nem megbízhatók. Alacsony dimenziószám esetén nincs probléma, d = 2 esetén b = 0,003.
Az ébredőszobai tartózkodás során is szükséges megfigyelést és adatrögzítést végezni ott, ahol ez az egység rendelkezésre áll. Működtetése a betegek biztonságát szolgálja, mivel kihelyezéskor teljesen éber állapotban engedhető vissza őket küldő osztályukra. A perioperatív medicina művelésének állomása még a műtét utáni akut fájdalomcsillapítás. Rendszerének felépítéséhez elegendő orvosra és szakasszisztensre van szükség, de a várható előnyök előbb- utóbb kikényszeríthetik felépítését. Az informatikai támogatást és a megbízható adatkezelést mobil eszközökkel igyekeznek biztosítani a POC (beteg közeli ellátás) filozófiáját követve. Az ehhez elégséges eszköz a PDA vagy újabban MDA (egészségügyi célfejlesztést használó Medical Digital Assistant) vagyis a zsebszámítógép. Az eszköz a komplett aneszteziológiai rendszerhez dokkoló állomással, vagy akár drótnélküli kapcsolat használatával illeszkedhet, ahonnan beszerzi a beteg adatait, és a betegágy mellett rendelheti el, vagy módosíthatja a terápiát a teljes körű információval rendelkező orvos.
Amíg a morfológiai leképezés egyes szervek, szövetek alaki jellemzőit mutatja be, addig a funkcionális képalkotás ezek működését jeleníti meg valamilyen képi formában. Attól függően, hogy milyen működést – mozgást, anyagcserét – szeretnénk tanulmányozni, más és más képalkotó technika áll rendelkezésünkre. A legegyszerűbb megjeleníthető működés a mozgás, ami átvilágítással vagy bármilyen egyéb, a szervek helyzetét kellő gyakorisággal bemutató eljárással elemezhető. Általában a történésekkel egyidejű, ún. real time leképezésre törekszünk. A 4D megjelenítés ténylegesen mozgáskövető, és térbeli leképezés és az időbeli felbontás összekapcsolásának köszönhetően a képernyőn a vizsgált szerv saját mozgásával egy időben változó, térézetet keltő ábrázolás.
Ez a trend a neurális hálókra is igaz. Az 1980-as években a kutatás többsége arra irányult, hogy kitapasztalják, a hálókkal meddig mehetnek el, és hogy megtanulják, a hálók a „hagyományos” technikáktól miben különböznek. A jobb módszertan és az elméleti háttér révén eljutottak ahhoz, hogy most a hálókat össze lehet hasonlítani a megfelelő statisztikai, alakfelismerési és gépi tanulási technikákkal, és az adott alkalmazáshoz meg lehet választani a leginkább sikerrel kecsegtetőt. Az ilyen fejlődés eredményeképpen az adatbányászat (data mining) technológia virágzó új iparrá nőtte ki magát.
A számítógépes nyelvészetet a tágabb értelemben vett nyelvészeten belül az alkalmazott nyelvészet (egy igen jelentős) alterületeként helyezhetjük el, hiszen tulajdonképpen a fenti nyelvészeti eszközök és módszerek számítástechnikai segítséggel történő alkalmazásáról van szó. A hagyományos alkalmazott nyelvészet körébe tartozott a szótárkészítés, valamint a természetes nyelvek tanítása racionális alapokon (nyelvtan, stilisztika és helyesírás). [1]
A mintaszám becslésben használt ε  hibavalószínűség tulajdonképpen a Neurális hálózatok könyvben használt általánosítási hibát jelenti. Az ott használt kifejezésekkel: ha a neuronhálónk (hipotézisünk), az N tanítómintát hibátlanul megtanulta, és a tanítóminta pontok száma meghaladta a közölt alsó korlátot, akkor 1- δ  a valószínűséggel állíthatjuk, hogy az általánosítási hibája legfeljebb ε  lesz. Ehhez persze biztosítani kell, hogy a lehetséges neuronhálók – mint hipotézisek – száma véges legyen, különben N végtelen nagyra adódik.
Hogy befejezzük a rezolúció tárgyalását, most megmutatjuk, hogy az IK-Rezolúció algoritmus teljes. Ahhoz, hogy ezt megtehessük, hasznos bevezetni az S klózok halmazának rezolúciós lezártját (resolution closure), RC(S)-t, ami az összes olyan klóz halmaza, amely levezethető a rezolúció ismételt alkalmazásaival az S halmazbeli elemekből és ezek leszármazottaiból. Az IK-Rezolúció a rezolúciós lezárás halmaz elemeit sorolja fel a klózok változóban. Könnyű belátni, hogy RC(S)-nek végesnek kell lennie, mert véges sok különböző klózt lehetséges konstruálni az P[1], …, P[k] szimbólumokból, amelyek elemei S-nek. (Vegyük észre, hogy ez nem volna igaz a faktorálási lépés nélkül, amely megszünteti a literálok többszöröződését.) Így az IK-Rezolúció mindig terminálódik.
A rezolúciót a teljesség tulajdonsága igen fontos következtetési módszerré teszi. Számos gyakorlati esetben azonban, a rezolúció teljes erejére nincs szükség. A valósvilág-beli tudásbázisok gyakran a klózoknak csak egy, Horn-klózoknak (Horn clause) nevezett korlátosabb fajtáját tartalmazzák. A Horn-klóz literálok olyan diszjunkciója, amelyek közül legfeljebb egy pozitív. Például a (¬P[1,1] ∨ ¬Szellő ∨ S[1,1]) klóz, ahol P[1,1] jelenti, hogy az ágens pozíciója az [1, 1], egy Horn-klóz, míg az (S[1,1] ∨ C[1,2] ∨ C[2,1]) nem az.
A manipulátorok kinematikai leírását az alábbi algoritmusban foglalhatjuk össze Schilling leírásához hasonlóan (Schilling). 1. azonosítsuk az egyes izületeket 0‑tól kezdődően (n‑1)‑ig; 2. az L[0] ortonormált, jobbsodrású koordináta-rendszert úgy rendeljük a manipulátorkar első izületéhez, hogy a z[0] egységvektor az izület mozgástengelyével egybeessék. Állítsuk az i indexet 1-re; 3. állítsuk a z[i] egységvektorokat az izületi mozgástengelyek irányába; 4. helyezzük az egyes L[i] lokális koordináta-rendszerek origóját vagy a z[i] és z[i][-1] mozgástengelyek metszéspontjába, vagy – kitérő egyenesek esetében – a közös normális és a z[i] tengely metszéspontjába; 5. a z[i] és z[i-1] egységvektorok ismeretében keresztszorzással kiszámoljuk az x[i] egységvektorokat. Ha z[i] és z[i][-1] párhuzamos, akkor alkalmazzuk a fentiekben meghatározott szabályokat; 6. válasszuk y[i]‑t úgy, hogy L[i] jobbsodrású koordináta-rendszert alkosson; 7. legyen i = i + 1. Ha i < n, lépjünk vissza 3‑ra; egyébként folytassuk az algoritmust 8‑cal; 8. helyezzük az L[n] lokális koordináta-rendszer origóját a szerszámközéppontba (TCP). Mutasson z[n] a szerszám megközelítés-vektorának (a) irányába, y[n] az orientációs vektor és x[n] a normálvektor irányába. Legyen i = 1; 9. határozzuk meg ϑ[i] izületi szöget; 10. határozzuk meg b[i] izületi távolságot; 11. határozzuk meg a[i] általánosított kartaghosszt; 12. határozzuk meg α[i] kartag csavarási szöget; 13. legyen i = i + 1. Ha i ≤ n, folytassuk 9‑cel, egyébként fejezzük be az algoritmust.
Nézzünk néhány „józan ész” példát a háttértudás felhasználásával történő tanulásra. Sok, a következtetésen alapuló, látszólag racionális viselkedésről ki fog derülni, hogy ha a megfigyeléseket is figyelembe kell venni, nyilvánvaló lesz, hogy az ágens nem a tiszta indukció egyszerű elvét követi. * Néha az általános konklúziót már egyetlenegy megfigyelés alapján megfogalmazhatjuk. Gary Larson egy karikatúrájában Zog, a szemüveges ősember egy fanyárs végére tűzött gyíkot süt a tűz felett. Kevésbé intelligens barlangtársainak bámulatba ejtett csoportja figyeli: ők az ennivalót a tűz felett csupasz kezükben tartják. Ez a megvilágosító tapasztalat elegendő, hogy a csoport tagjai a fájdalommentes főzés elvét elsajátítsák. * Vagy vegyük például a Brazíliába érkező utazó esetét, amikor is az első brazil bennszülöttel találkozik. Hallván, hogy az portugálul beszél, utazónk egyből megállapítja, hogy Brazíliában mindenki beszél portugálul, felfedezvén azonban, hogy a bennszülöttet Fernandónak hívják, ezt a felfedezését az összes brazil férfira egyáltalán nem terjeszti ki. Hasonló esetek a tudományra is jellemzők. Amikor például egy kezdő fizikushallgató egy adott hőmérsékletű rézmintának a sűrűségét és az elektromos vezetőképességét méri, a mérési eredményeket magabiztosan általánosítja a réz minden lehetséges darabkájára. Ha azonban a minta tömegét méri, meg sem fordul a fejében, hogy más rézdarabkáknak is ez lenne a tömege. Ez az általánosító feltevés viszont egészen értelmes lenne, ha például az amerikai 1 centes érmékről lenne szó. * Végül vegyük egy gyógykezelésben járatlan, diagnosztikai szempontból azonban képzett orvostanhallgató esetét, aki egy páciens és egy szakértő belgyógyász konzultációját figyeli. Egy sor kérdés és felelet után a szakértő egy bizonyos antibiotikumos gyógykezelést ír elő a páciens részére. A hallgatónk azt az általános szabályt fogalmazza meg, hogy ez a konkrét antibiotikum hatásos erre a konkrét típusú fertőzésre. Fontos Ezek mind olyan esetek, amikor a háttértudás felhasználása sokkal gyorsabb tanulást tesz lehetővé ahhoz képest, mint amit a tisztán induktív programtól el lehetne várni.
Az egész folyamat a bizonyíték megoldó segítségével történik, ő adja meg a csomópontok aktuális értékét a menedzsernek. Ha egy csomópont a likely loss csoportba tartozik, annak kiértékelése adott pillanatban nem lényeges, ezáltal megkíméljük a programot a felesleges munkától. Amint a bizonyítási fa kiértékelődött egy bizonyos határértékre, a határértéket megnöveljük és az egész bizonyítási eljárást előről kezdjük. Ha egy már kiértékelt csomópont értéke továbbra is a határ felett/ alatt van, azokat nem kell újra kiértékelni, maradnak a likely win/ loss kategóriában. Ha egy csomópont az első körben már valós (nem likely) értéket kapott, azt sem kell újra vizsgálni. Tehát az új határérték felállítása után csak azokat a csomópontokat kell kiértékelni, amelyek nincsenek bizonyítva, nem esnek konkrétan egyik kategóriába sem a jelen állapot szerint. Ezt az eljárást addig folytatjuk, amíg el nem érünk egy maximális határértéket, amikor már egy csomópont sem esik bele a likely kategóriába, vagyis az összes csomópontot kiértékeltük, a bizonyítási fa teljes. A bizonyítási menedzser egyszerre párhuzamosan több csomópontot is vizsgál, hogy a háttérben a bizonyíték megoldók párhuzamosan tudják kiértékelni a nekik küldött csomópontokat. A bizonyítási fa menedzser egyszerre több száz pont kiértékelését kéri a párhuzamosan futó megoldóktól. A csapat 2007-ben átlagosan 50 számítógépet használt párhuzamosan a megoldók párhuzamos futtatására [2]. Emellett a menedzser az összes eddigi eredményt is eltárolja, ezáltal például ismételt lépéssorozatok is feltárhatóak, amelyek a döntetlenhez vezetnének [9].
A 3. szakasz - Jól definiált problémák és megoldások részben azt mondtuk, hogy nem tárgyaljuk a negatív útköltséggel rendelkező problémákat. Ennek a feladatnak a kapcsán részletesebben megvizsgáljuk a kérdést. a. Tegyük fel, hogy a cselekvéseknek tetszőlegesen nagy negatív költségük lehet. Magyarázza meg, hogy ez a lehetőség miért kényszerít minden optimális algoritmust a teljes állapottér felkutatására. b. Segíteni fog-e, ha ragaszkodunk ahhoz, hogy a lépésköltség egy negatív c konstansnál nagyobb vagy azzal egyenlő legyen? Mind a fákra, mind a gráfokra gondoljon. c. Tételezzük fel, hogy cselekvések egy halmaza ciklust alkot úgy, hogy a halmaz cselekvéseit valamilyen sorrendben végrehajtva az állapot nem változik. Milyen következményekkel jár egy ilyen környezetben tevékenykedő ágens optimális viselkedésére, ha ezen cselekvések mindegyike negatív költségű? d. Könnyen el tudunk képzelni nagy negatív költségű operátorokat még az útkeresési probléma esetén is. Például néhány útszakasz olyan gyönyörű tájakon halad, hogy az idő és üzemanyag tekintetében messze túlszárnyalja a normál költségeket. Pontos megfogalmazásban magyarázza el, hogy az emberek miért nem kocsikáznak a végtelenségig a szép tájak körül, és hogyan lehetne az útkeresési problémához az állapotteret és az operátorokat definiálni, hogy a mesterséges ágens is el tudja kerülni a ciklusokat. e. Tud olyan valódi problémakört mondani, amelyben a lépések költsége ciklust okoz?
Az I. részben bevezettük a racionális ágensrendszert mint a mesterséges intelligencia egységes szemléleti keretét. Bemutattuk, hogy a tervezési probléma az ágens rendelkezésére álló észleléseken és cselekvéseken, az ágens viselkedésével kielégítendő célon és a környezet természetén múlik. Az ágensrendszertervek egész sorát lehet elképzelni, a reflexszerű ágenstől kezdve egészen a teljesen céltudatos, tudásalapú ágensig. Továbbá ezen rendszertervek komponensei többféle konkrét alakot ölthetnek – például logikait, valószínűségit vagy „neurálist”. Az ilyen jellegű komponensek működésével a megfelelő fejezetek foglalkoztak. Fontos Az ágensrendszer és komponensei tekintetében mind tudományos megértésünk, mind technológiai képességünk hatalmasat fejlődött. Ebben a fejezetben hátrébb lépünk a részletektől, és azt kérdezzük: „Elvezet-e ez a fejlődés egy változatos környezetekben működésre képes, általános célú intelligens ágenshez?” A  alfejezetben számba vesszük az intelligens ágens komponenseit, hogy meghatározhassuk, mi az, amit ismerünk, és mi az, ami még hiányzik. A  alfejezetben az általános ágensarchitektúra kapcsán tesszük ugyanezt. A  alfejezetben azt vizsgáljuk, hogy a „racionális ágens tervezése”-e az elsődlegesen helyes cél. (A válasz: „Nem igazán, de egyelőre megteszi.”) Végül a  alfejezetben azzal foglalkozunk, milyen következményekkel járhat vállalkozásunk sikere.
A darwini evolúció igencsak kis hatékonyságú mechanizmusnak tűnhet, hiszen vakon létrehozott kb. 10^45 szervezetet, anélkül hogy a keresési heurisztikáin egy csöppet is javított volna. Darwin előtt 50 évvel a különben neves francia természettudós Jean Lamarck egy olyan evolúció elméletet javasolt (Lamarck, 1809), ahol egy szervezet az élete során, adaptációja révén megszerzett tulajdonságokat is képes az utódoknak átadni. Az ilyen mechanizmus hatékony lenne, de úgy tűnik, a természetben nem fordul elő. Sokkal később James Baldwin látszólag hasonló elmélettel állt elő (Baldwin, 1896), hogy a szervezet élete alatt megtanult viselkedés növelhetné az evolúció sebességét. Lamarck elméletével ellentétben a Baldwin-elmélet a darwini evolúcióval teljesen konzisztens, hiszen alapja egy szelekciós nyomás, amely olyan egyedekre hat, amelyek lokális optimumokat találtak a genetikus felépítésük által engedélyezett viselkedések között. A korszerű számítógépes szimulációk alátámasztják a „Baldwin-effektus” valós voltát, feltéve, hogy a „közönséges” evolúció olyan szervezeteket képes kialakítani, amelyeknél a belső hatékonysági mérték a saját tényleges fitness-értékükkel valahogy korrelál.
A repülőtér problémakörből adott példákkal magyarázza meg, hogy a szimbólumszétválasztás hogyan csökkenti az előfeltétel és a cselekvéskizárási axiómák méretét. Származtasson egy általános képletet, mely az axiómahalmazok méretét adja meg az időlépések száma, a cselekvéssémák száma, ezek aritása, valamint a tárgyak száma függvényében.
A következőkben néhány gyakran alkalmazott élkereső algoritmust mutatok be. Ezek mindegyikének implementációja rendelkezésre áll Matlabban az edge (IPT) függvény megfelelő paraméterezésével. Az első négy ismertetendő algoritmus csupán a kép gradiensét közelíti, tehát az így kapott eredményben nem feltétlenül lesznek összefüggő élek. Ezzel szemben a Canny detektor a gradiens képet tovább alakítja, és egy összefüggő, általában pontosabb élképet ad.
Egy döntési cella kívánt D és tényleges S kimeneti jele az egyszerűsített modell szerint négyféle kombináció szerinti lehetséges (00, 01, 10, 11). Az első és az utolsó esetben a súlyozás megváltoztatására nincs szükség, hisz a kívánt és a tényleges jelek megegyeznek. A másik két esetben a két jel közötti különbségnek megfelelően egyiknél csökkenteni, másiknál növelni kell a súlyt, hogy S közelítse, helyesebben el is érje D –t.
Hogyha a fényceruzát egy képrészlet fölé visszük és az észlelt mutatott pont az említett tűréshatáron belülre esik, akkor a tényleges pozíciót nem vesszük figyelembe, helyette az álpozíciót használjuk, ami egybeesik a mutatott alakzat egyik pontjával. Amennyiben nincs ilyen alakzatunk aminek a környezetében járunk, az álpozíció megegyezik a tényleges pozícióval, amit egy fényes ponttal ábrázoluk a képernyőn. Tehát mindig amikor egy ábra felé közelítjük a fényceruza „hegyét”, az álpozíció az alakzat közelében az alakzat vonalára ugrik, ha metszésponthoz érünk, annak környezetében arra.
Az eddig említett kényszerek mind abszolút kényszerek voltak: egy kényszer megszegése kizár egy megoldásjelöltet. A valós alkalmazások során azonban sok kényszerkielégítési probléma tartalmaz preferenciakényszereket, melyek jelzik, hogy mely megoldások preferáltak. Egy egyetemi órarend meghatározásakor például X professzor talán inkább reggel tanítana, míg Y professzor a délutáni órákat részesítené előnyben. Egy olyan órarend, amely szerint X professzornak délután kettőkor kellene tanítania, még megoldás lenne (persze csak ha X professzor éppenséggel nem a tanszékvezető), de ez nem lenne optimális megoldás. A preferenciakényszerek gyakran az egyedi változó-hozzárendelések költségeként ábrázolhatók: például ha X professzor egy délutáni időpontot kap, az két pontot ront az általános célfüggvényen, míg a reggeli óraidőpont csak egy pontba kerül. Így megfogalmazva a preferenciákkal kiegészített kényszerkielégítési problémák (útvonalalapú vagy lokális) optimalizációs keresési módszerekkel megoldhatók. Az ilyen kényszerkielégítési problémákkal a fejezetben többet nem foglalkozunk, de az irodalomjegyzékben megadunk néhány kiindulási pontként használható hivatkozást.  ábra - (a) Egy betűrejtvény. Mindegyik betű különböző számjegyet jelöl, a rejtvény célja olyan számjegyeket helyettesíteni a betűk helyére, amelyekkel a kiadódó összeg aritmetikailag helyes lesz (azzal a külön megkötéssel, hogy nem kezdő nullák nem megengedettek). (b) A betűrejtvény kényszerhipergráfja a MindKül kényszer és az oszloponkénti összeadási kényszerek feltüntetésével. A kényszereket négyzet alakú dobozok jelölik. Minden doboz azzal a változóval van összekötve, amelyre a doboz által jelzett kényszer vonatkozik. (a) Egy betűrejtvény. Mindegyik betű különböző számjegyet jelöl, a rejtvény célja olyan számjegyeket helyettesíteni a betűk helyére, amelyekkel a kiadódó összeg aritmetikailag helyes lesz (azzal a külön megkötéssel, hogy nem kezdő nullák nem megengedettek). (b) A betűrejtvény kényszerhipergráfja a MindKül kényszer és az oszloponkénti összeadási kényszerek feltüntetésével. A kényszereket négyzet alakú dobozok jelölik. Minden doboz azzal a változóval van összekötve, amelyre a doboz által jelzett kényszer vonatkozik.
A független kontrollok gyártói rendszerint lehetőséget biztosítanak arra, hogy a laboratóriumok nemzetközi szinten, anonim módon összevethessék egymással az azonos gyártási tételű kontrollokból a napi belső minőség - ellenőrzés során mért eredményeiket. Az összegyűjtött adatokat készülékek és mérési módszerek szerint csoportosítják, szűkített módszertani csoportokat (peer group) hoznak létre, és azokat statisztikai módszerekkel, adott időszakonként (általában havonta) értékelik. Ezeket az értékeléseket a résztvevő laboratóriumok minden értékelési periódus végén rendszeresen megkapják. Az adatok alkalmasak a mérőberendezés megbízhatóságának követésére és a kontrollanyagnak a folyamatos ellenőrzésére is. A résztvevők információt nyernek továbbá az egyes készülékek és mérési módszerek saját hibájának mértékéről (CV), mely alapján meghatározható, mely Westgard – szabályokat kell alkalmazni a leghatékonyabb napi minőség – ellenőrzés folytatásához, sőt orientálhat a legmegfelelőebb mérési módszer kiválasztásában is. Az ISO 15189 szabvány is javasolja részvételt a nemzetközi laboratóriumi összehasonlító rendszerekben.
A bemutatott algoritmus lépésszámát vizsgálva láthatjuk, hogy amíg az előző szakaszban, pusztán előretekintést használva 20 lépésre volt szükség ahhoz, hogy az első megoldást megtaláljuk (ezen belül 12 értékadásra, és 8 értékkészlet szűkítésre), addig most csak 11 lépésre (ezen belül 7 értékadásra, és 4 értékkészlet szűkítésre). Ez számottevően jobb futási statisztika - az algoritmus ugyanazon a tesztproblémán közel kétszeresére gyorsult az MRV heurisztikának köszönhetően.
A konzisztens behelyettesítés után az AC3/MAC következtetésre tér rá a vezérlés. Ennek során, mivel T értékkészlete az előző változóktól eltérően (még) nem egy-elemű, ezért először is egy-eleműre redukálja, azaz T=RED értékadásnak megfelelően T értékkészletét {RED}-re szűkíti, majd pedig létrehozza az AC3 számára szükséges kiindulási él-listát a T-re mutató összes élből. Mivel azonban T nem áll kapcsolatban egyetlen másik változóval sem, azaz nem szerepel egyetlen korlátban sem, ezért ez az él-lista üres lesz, ami azt jelenti, hogy az AC3 következtetés sikerrel véget ér, és a visszalépéses keresés ezek után újfent rekurzíve meghívhatja önmagát.
Már megmutattuk, hogy minden Kalman-szűrő reprezentálható egy DBH-ban folytonos változókkal és lineáris Gauss feltételes eloszlásokkal (lásd  ábra). Az előző fejezet végén tárgyaltak miatt azzal is tisztában kell lenni, hogy nem minden DBH reprezentálható egyetlen Kalman-szűrő modellel. Egy Kalman-szűrőnél az aktuális állapoteloszlás mindig egy egyedülálló többváltozós Gauss-eloszlás – azaz egyetlen „dudor” egy konkrét helyen. A DBH-k ezzel szemben tetszőleges eloszlást képesek modellezni. Számos valós alkalmazásnál ez a rugalmasság elengedhetetlen. Gondoljuk meg például a kulcsaim jelenlegi helyét. Lehetnek a zsebeimben, az éjjeli szekrényen, a konyhapulton vagy a bejárati ajtóban himbálódzva. Egyetlen Gauss-dudornak, ami ezt az összes helyet magában foglalja, jelentős valószínűséget kell rendelni ahhoz, hogy a kulcsok az előszobában a levegőben vannak. A valós világ olyan aspektusai, mint céltudatos ágensek, akadályok és zsebek „nemlinearitásokat” vezetnek be, amelyek diszkrét és folytonos változók kombinációját igénylik, hogy egy elfogadható modellhez jussunk.
A  alfejezetben részletesen leírt TIT algoritmusokat főleg az induktív logikai programozás, ILP (inductive logical programming) területén tanulmányozták. Az előzetes tudásnak egy ILP-rendszerben két kulcsszerepe van a tanulás komplexitásának csökkentésében: 1. Tekintettel arra, hogy a megfogalmazott hipotézisek mindegyikének konzisztensnek kell lennie a megfigyelésekkel és az előzetes tudással is, ténylegesen egy redukált hipotézistérrel van dolgunk, hiszen ez a hipotézistér csakis azokat az elméleteket tartalmazza, amelyek az eddig ismert dolgokkal konzisztensek. 2. Bármilyen adott megfigyeléshalmaz esetén a megfigyelések magyarázatát megadó hipotézis lényegesen redukált méretű lehet, hiszen a megfigyeléseket magyarázó új szabályok meghatározásához az előzetes tudás is rendelkezésünkre áll. Viszont minél kisebb a hipotézis, annál könnyebb azt megtalálni.
Ez olyan, mintha a filozófusok azt tartanák magukról, hogy szakértő magyarázói a színpadi bűvészek trükkjeinek, aztán, amikor megkérdezzük tőlük, hogy milyen trükköt használnak a bűvészek a nő kettéfűrészelésénél, akkor azt a magyarázatot adják, hogy mindez nagyon is világos: a bűvész nem fűrészeli ketté a nőt, ezt csak színleli. Ha azt kérdezzük azonban: „Mégis hogyan színleli?”, a filozófusok ezt felelik: „Nem ránk tartozik.”
Adottak a címkék a takaró éleken (például a nyilak az óramutató járásával azonosan mutatnak), ekkor egyáltalán nem lesz szükség a visszalépő algoritmusra, ha a sorrend ABCD; minden választást meghatároznak a meglévő címkék. (24.  ábra)  ábra -  ábra Az L alakú objektum címkézése  ábra Az L alakú objektum címkézése
Kevésbé szerencsés a helyzet a)-val, ez ugyanis nem következik a tudásbázisból. Tekintsük a következő modellt: M = igaz, H = igaz, E = igaz, Sz = igaz, Ma = igaz. Ez a modell kielégíti mind a négy mondatot. Viszont a M = hamis, H = igaz, E = igaz, Sz = igaz, Ma = igaz szintén kielégíti mind a négy mondatot, tehát létezik olyan modell, amiben igazak a tudásbázis mondatai, de nem igaz az állítás, de olyan is, amiben igazak a mondatok és igaz az állítás is.
A Bayes-tanulás (Bayesian learning) során egyszerűen kiszámítjuk minden egyes hipotézis valószínűségét az adatokra támaszkodva, majd ennek alapján adunk predikciót. Azaz nem egyetlen „legjobb” hipotézist használunk a predikcióhoz, hanem az öszszes hipotézist használjuk, valószínűségükkel súlyozva őket. Reprezentálja D az összes adatot, legyen d a megfigyelt értékek vektora, ekkor az egyes hipotézisek valószínűségét a Bayes-szabállyal adhatjuk meg:
A szimulált lehűtés legbelső ciklusa (lásd  ábra) nagyon hasonlít a hegymászáshoz. A legjobb lépés megtétele helyett azonban egy véletlen lépést tesz. Ha a lépés javítja a helyzetet, akkor az mindig végrehajtásra kerül. Ellenkező esetben az algoritmus a lépést csak valamilyen 1-nél kisebb valószínűséggel teszi meg. A valószínűség exponenciálisan csökken a lépés „rosszaságával” – azzal a ΔE mennyiséggel, amivel a kiértékelő függvény értéke romlott. A valószínűség a T „hőmérséklet” csökkenésével is csökken. A „rossz” lépések az indulásnál T magasabb értékeinél valószínűbbek, T csökkenésével egyre valószínűtlenebbé válnak. Be lehet bizonyítani, hogy ha a hűtési karakterisztika T értékeit kellően lassan csökkenti, az algoritmus 1-hez tartó valószínűséggel a globális minimumban köt ki.  ábra - A szimulált lehűtés keresési algoritmus. A hegymászó keresés egy olyan változata, ahol lefelé tartó lépések is megengedettek. A lefelé tartó lépéseket a lehűtési terv elején inkább, az idő múlásával egyre kevésbé fogadjuk el. A lehűtési terv, mint az eljárás bemenete, a T értékeit mint időfüggvényt adja meg. A szimulált lehűtés keresési algoritmus. A hegymászó keresés egy olyan változata, ahol lefelé tartó lépések is megengedettek. A lefelé tartó lépéseket a lehűtési terv elején inkább, az idő múlásával egyre kevésbé fogadjuk el. A lehűtési terv, mint az eljárás bemenete, a T értékeit mint időfüggvényt adja meg.
A kritikus útvonal (critical path) az az út, amelynek a teljes ideje a leghosszabb; az útvonal „kritikus”, mert meghatározza a teljes terv hosszát. Más utak rövidítése nem rövidíti a terv egészét, de a kritikus útvonalon lévő bármely cselekvés kezdetének elhalasztása a teljes tervet lassítja. Az ábrán a kritikus útvonal vonalát vastag vonallal jelöljük. Hogy a teljes tervet minimális idő alatt hajtsuk végre, a kritikus útvonal cselekvéseit úgy kell végrehajtani, hogy köztük ne legyen késleltetés. A kritikus útvonalon kívül eső cselekvéseknek van valamennyi mozgásterük — egy időablak, amelyben végrehajthatók. Ezt az időablakot a kezdeti időpont lehető legkorábbi (ES) és a lehető legkésőbbi (LS) értékével definiáljuk. Az LS – ES mérőszámot, a cselekvés mozgásterének (slack) nevezzük. A  ábrán láthatjuk, hogy a teljes terv 85 percet vesz igénybe, a kritikus útvonal minden cselekvésének a mozgástere 0 (ez mindig igaz), és a C[1] összeszerelésének minden cselekvése egy 15 perces ablakban indítható. A cselekvések ES és LS idői a probléma ütemtervét (schedule) adják.  ábra - A  ábrán szereplő ütemezési probléma megoldása. Az ábra tetején a részben rendezett terv megoldása látható. Minden cselekvés hozza a téglalap alján látható, a legkorábbi és a legkésőbbi kezdési időkkel [ES, LS] a bal felső sarokban. A két szám között különbség a cselekvés mozgástere, a 0 mozgásterű cselekvések a kritikus útvonalon vannak, melyeket vastag vonallal jelölünk. Az ábra alján ugyanezen megoldás időrendjét láthatjuk. A szürke téglalapok azt az intervallumot jelölik, ami alatt a cselekvés végrehajtható, feltételezve, hogy a sorrendezési megkötéseket betartjuk. A szürke téglalapok nem felhasznált, nem kitöltött területei a mozgásteret jelölik. A  ábrán szereplő ütemezési probléma megoldása. Az ábra tetején a részben rendezett terv megoldása látható. Minden cselekvés hozza a téglalap alján látható, a legkorábbi és a legkésőbbi kezdési időkkel [ES, LS] a bal felső sarokban. A két szám között különbség a cselekvés mozgástere, a 0 mozgásterű cselekvések a kritikus útvonalon vannak, melyeket vastag vonallal jelölünk. Az ábra alján ugyanezen megoldás időrendjét láthatjuk. A szürke téglalapok azt az intervallumot jelölik, ami alatt a cselekvés végrehajtható, feltételezve, hogy a sorrendezési megkötéseket betartjuk. A szürke téglalapok nem felhasznált, nem kitöltött területei a mozgásteret jelölik.
Számos bonyolult problématerületen a megerősítéses tanulás az egyetlen lehetséges út arra, hogy egy programnak magas szintű működést tanítsunk. Például a játékok terén nagyon nehéz lenne a szakember dolga: a lehetséges nagyszámú állás pontos és következetes értékelését kellene elvégeznie annak érdekében, hogy a példák alapján közvetlenül megtanítson egy kiértékelő függvényt. Ehelyett a programnak megadható az az információ, hogy nyert vagy vesztett, és ezt felhasználhatja egy olyan kiértékelő függvény tanulására, amely elfogadható pontossággal képes megbecsülni bármely adott állásban a nyerés valószínűségét. Hasonlóképpen rendkívül nehéz megtanítani egy ágenst helikopter vezetésére, de ha megfelelő negatív jutalmat kap minden balesetnél, bukdácsoló mozgásnál vagy a beállított útvonaltól való eltérésnél, akkor az ágens önállóan megtanulhat repülni.
Ebben a játékban 10 játékos vesz részt, ámde akár tetszőleges N>1 szereplő esetén is van értelme. Mint látjuk, a játéknak számos Nash-egyensúlya van, ahova a rendszer automatikusan eljut, bárhonnan is indítjuk. Ha például mindenki kooperálna, azaz mindenki csak 1 tehenet engedne ki a közös legelőre, akkor mindenkinél ott lenne a motiváció/késztetés arra, hogy kiengedjen még egyet, és majdnem 2-szer annyit keressen (őket hívják itt dezertőrnek). Viszont, ha már 8 dezertőr van, és 2 kooperatív játékos, akkor már senkinek sem áll értekében ettől a helyzettől eltérnie. Azaz annyi Nash-egyensúly van, ahány féleképpen 10-ből el tudunk hagyni 2-őt, azaz 10-alatt-a-2, ami egészen pontosan 45. Azaz 45 darab különböző, de végső soron egyenértékű, szimmetrikus megoládsa van ennek a játéknak. ...vajon melyiket válasszuk? Melyik szerint játsszunk?
Az 1980-as években megindult elméleti munka tetőzése után, napjainkban ismét a temporális tervkészítés kerül előtérbe, amikor az új algoritmusok és a megnövekedett feldolgozási teljesítmény lehetővé tette a gyakorlati felhasználást. A két tervkészítő a Sapa (Do és Kambhampati, 2001) és a T4 (Haslum és Geffner, 2001) előrefelé történő állapottér keresést alkalmaztak, amit az időtartammal és erőforrásokkal rendelkező cselekvések kezeléséhez egy kifinomult heurisztikával egészítettek ki. Egy másik lehetőség a nagyon költséges cselekvésleíró nyelvek használata, melyeket kézzel írt és alkalmazási terület specifikus heurisztikákkal irányíthatunk, amint az az ASPEN (Fukunaga és társai, 1997), a HSTS (Jonsson és társai, 2000) és az IxTeT (Ghallab és Laruelle, 1994) esetén is történik.
 Megjegyzés Implementálja az állapotátmenet-függvény két változatát a 8-as játék számára. Az egyik az összes követőt egyszerre generálja a 8-as játék adatstruktúrájának másolásával és editálásával. A másik hívásakor, csak egy követőt generál, a szülő állapot közvetlen módosításával (és szükség esetén a módosítások visszaállításával). Írja meg az iteratívan mélyülő keresés olyan változatait, amelyek a függvény fenti változatait használják, és hasonlítsa össze a megoldások hatékonyságát.
[871-1.png] a. Futassa a perceptron tanulási szabályt erre az adathalmazra, és adja meg a végső súlyokat. b. Futassa a döntési fa tanulási algoritmust, és adja meg az eredményül kapott döntési fát. c. Értékelje az eredményeket.
Kiséreljük feltérképezni a nem mindig érzékelhető, de valódi környezeti állapotokat. Itt nemcsak az oszlopot kell modelleznünk (hány kapcsoló van „fel” állapotban), de az ágenst is (azaz inkább a szenzorikus apparátusát, amivel csak meghatározott konfigurációban meg tudja vizsgálni a kapcsolók állapotát). Az „A” állapotokban tehát egyetlenegy kapcsoló van felkapcsolva, és az állapotok számozása megkülönbözteti annak fekvését az ágensre nézve akkor, amikor kezeinek benyulásával meg szeretne róla győződni. Az alábbi ábrán feltüntettük az összes állapotot (felkapcsolt kapcsoló: fekete, lekapcsolt: fehér). A „0” és „1” állapotok a kamrát nyító oszlopállapotok.
A laboratórium saját vizsgálati kérőlapokat szerkeszt. A kérőlapon feltüntetik a választható vizsgálatokat, melyekért a laboratórium felelősséget vállal. A lehetőségek ismeretében egyes országokban (pl.: Ausztrália) az orvos saját kézírásával valamennyi választott vizsgálatot megnevezve küldi el a kérőlapot, mivel a választás a klinikus felelőssége.
Példaként tekintsük a következőt: Helyezzünk el egy királynőt a sakktábla második oszlopának harmadik sorába. Ekkor a „sor” tömb 3-adik eleme, az „oszlop-sor” tömb -1-edik eleme és az „oszlop+sor” tömb 5-ödik eleme „hamis” értékű lesz. Az algoritmus a korábban megismert backtrack algoritmushoz hasonlóan működik, azzal a különbséggel, hogy itt most akkor állítunk be királynőt az adott sor és oszlop találkozásánál lévő mezőre, ha a mezőhöz tartozó mindhárom tömbelem logikai „igaz” értékű [12].
Ha egyszer létrehoztunk egy objektumokat is tartalmazó logikát, természetes, hogy objektumok egész gyűjteményeire vonatkozó tulajdonságokat is ki akarunk fejezni anélkül, hogy az objektumokat nevük felhasználásával fel kellene sorolni. Ezt a kvantorok (quantifiers) alkalmazása teszi lehetővé. Az elsőrendű logika két standard kvantort tartalmaz, amelyeket univerzális és egzisztenciális kvantoroknak nevezünk.
Ismételje meg a  feladatot egy olyan esetre, amikor az elhelyezkedés érzékelőt egy „ütközés” érzékelő váltja fel, ami érzékeli az ágens olyan elmozdulási kísérleteit, amikor nekiütközik egy akadálynak vagy átlépi a környezet határát! Tegye fel, hogy az ütközésérzékelő működése leáll! Hogyan kellene az ágensnek viselkednie?
1958 volt az az év is, amikor Marvin Minsky az MIT-re ment át. Kezdeti együttműködése McCarthyval nem tartott sokáig. McCarthy a reprezentációra és a formális logikai következtetésre tette a hangsúlyt, Minskyt inkább az érdekelte, hogy a programok működőképesek legyenek, majd végül logikaellenes álláspontra helyezkedett. 1963-ban McCarthy a Stanfordon megalakította az ottani MI-labort. Kutatási programja – amely arra irányult, hogy a logikát felhasználja a legvégső Advice Taker építésében – lökést kapott, amikor J. A. Robinson felfedezte a rezolúciót, az elsőrendű logika teljes bizonyítási eljárását (lásd  fejezet). A Stanfordon a kutatás hangsúlyozottan a logikai következtetés általános módszereire irányult. A logika alkalmazásaihoz tartoztak Cordellnek a Green kérdését megválaszoló és tervkészítő rendszerei (Green, 1969b), továbbá Shakey robotikus projektje az új Stanfordi Kutatóintézetben (Stanford Research Institute, SRI).^[15] Ez a projekt – amit részletesebben a  fejezetben tárgyalunk – volt az első, amely a logikai következtetést és a fizikai aktivitást teljes egészében integrálta.  ábra - Egy konkrét probléma, amelyet Evans Analogy programja megoldott Egy konkrét probléma, amelyet Evans Analogy programja megoldott
Kahneman és Tversky továbbment, és kifejlesztett egy leíró elméletet, ami megmagyarázza, hogy az emberek miért kerülik a kockázatot nagy valószínűségű eseményeknél, viszont miért hajlandók nagyobb kockázatot vállalni valószínűtlen jutalmakkal. A kapcsolat az MI és ezen tapasztalat között az, hogy az ágensünk döntései csak annyira jók, amennyire jók a preferenciák, amin alapulnak. Ha az emberi informátoraink ellentmondásos preferenciákhoz ragaszkodnak, akkor az ágensünk semmit sem tud tenni, hogy konzisztens legyen velük.
Ez azt mondja, hogy bármely s szituációban, a p terv egy alkalmazható összetett terv, ha az ágensek közösen tudják, hogy a p eléri a célt. További axiómákra van szükségünk, hogy biztosítsuk egy adott összetett terv végrehajtására vonatkozó közös szándék (joint intention) együttes ismeretét. Az ágensek csak ebben az esetben kezdhetnek cselekedni.
A legközelebbi-szomszéd tanulási algoritmus használható közvetlen ellenőrzött tanulásra is. Ha adott egy tesztpélda x bemeneti értékekkel, akkor az y = h(x) az x k-legközelebbi-szomszédjának y értékeiből nyerhető. Diszkrét esetben többségi szavazással kaphatunk egyetlen jósolt értéket. Folytonos esetben átlagolhatjuk a k darab értéket, vagy lokális lineáris regressziót végezhetünk a k pontra illesztve, ez utóbbi esetben a jóslást a kialakuló hipersík x-beli értéke adja.
A fájl első felében a szimulátor gyakorlati működését meghatározó beállítások kaptak helyet, melyeket alább részletezünk. A fájl második felében a játékkal kapcsolatos beállítások szerepelnek (például cselekvések költsége, pálya mérete, stb.), melyeket a verseny keretén belül nincs értelme módosítani, ezért jelentésükre (a fájlban elhelyezett kommenteknél bővebben) nem térünk ki.
 Megjegyzés Mutassa meg precízen, hogy hogyan módosítjuk az És-Vagy-Gráf-Keresés algoritmust, hogy ciklikus tervet generáljon, amennyiben ciklusmentes terv nem létezik. Ehhez három problémával kell megküzdenie: a terv lépéseinek címkézése, hogy egy ciklikus terv vissza tudjon mutatni a terv egy korábbi pontjára, a Vagy-Keresés módosításával, hogy egy ciklikus terv megtalálása után ciklusmentes tervet keressen, és a terv leírásának olyan bővítésével, mely jelzi, hogy a terv ciklikus. Mutassa meg, hogyan működik az algoritmus (a) a tripla-Murphy porszívóvilágban, (b) a váltakozó dupla-Murphy porszívóvilágban. Használhat számítógépes implementációt az eredmények ellenőrzéséhez. A (b) eset terve felírható egy hagyományos ciklusszintaxis használatával?
Az ℰ[1] nyelv fejlettebb az ℰ[0]-hoz képest, de még ez is túlgenerál. Az egyik probléma abban a módban rejlik, ahogy az igei kifejezéseket összefűzi. El akarjuk fogadni a „give me the gold” és a „go to 1,2” jellegű igés kifejezéseket. Ezek mindegyike létezik az ℰ[1]-ben, de sajnos éppígy létezik a „go me the gold” és a „give to 1,2” is. Az ℰ[2] nyelv megszünteti ezeket a VP-ket úgy, hogy egyértelműen megadja, melyik igét melyik kifejezés követheti. Ezt az igék alkategória (subcategorization) listájának hívjuk. Az elképzelés az, hogy a Verb kategóriát alkategóriákra bontjuk – egy a tárggyal nem rendelkezők igék számára, egy az egyetlen tárggyal rendelkezőkre és így tovább.
Gondoljuk át a fogolydilemma ismétlődő változatát. Együtt fog-e működni Aliz és Bendegúz, visszautasítva a tanúskodást, azt tudva, hogy újra fognak találkozni? A válasz a találkozás részleteitől függ. Tegyük fel, hogy Aliz és Bendegúz tudja, hogy pontosan 100 menetben játszanak fogolydilemmásat. Ekkor mindketten tudják, hogy a 100. menet nem lesz ismételt játék – azaz, a kimenetelének nincs hatása jövőbeli menetekre –, és ezért ebben a menetben mindketten a domináns stratégiát, a tanúskodás-t választják. De a 100. menet meghatározásával a 99. menetnek nem lehet hatása következő menetekre, így ennek szintén lesz egy domináns stratégiai egyensúlya a (tanúskodik, tanúskodik)-nál. Indukcióval mindkét játékos a tanúskodást fogja választani minden menetben, fejenként összesen 500 év börtönt kapva. Eltérő megoldásokat nyerhetünk az interakció szabályainak a megváltoztatásával. Tegyük fel, hogy minden menet után 99% esélye van, hogy a játékosok újra találkoznak.
Definition 1.6 Egy p  eloszlást stabilnak ^[1] vagy hűségesnek hívunk, ha létezik egy DAG ami pontosan reprezentálja a p  eloszlás függetlenségi relációit (azaz úgynevezett tökéletes térkép: (X    ⊥⊥    Y|Z) G ⇔ (X    ⊥⊥    Y|Z) p  , ∀  X,Y,Z⊆V  ). Egy p  eloszlás stabil a G  DAG-ra nézve, ha G  pontosan reprezentálja a függetlenségeit.
Ebből már sejthető, hogy miként kapcsolhatjuk össze játékok extenzív alakját a játékok normál alakjával. A játékok extenzív alakjához egyértelműen tartozik egy és csakis egy normál alak, viszont egy normál alakhoz akár végtelen számú extenzív alak is tartozhat. Tehát az extenzív-normál konverzió egyértelmű, míg a normál-extenzív irány már nem az.
A könyvhöz tartozó példaprogramtár (aima.cs.berkeley.edu) tartalmaz számos környezetimplementációt, egy általános célú környezetszimulátorral együtt, amely egy vagy több ágenst helyez el egy szimulált környezetben, megfigyeli a viselkedésüket az időben, és kiértékeli őket egy adott teljesítménymérték szerint. Ilyen kísérleteket gyakran nem egy, hanem több, egy környezetosztályból (environment class) származó környezetre hajtanak végre. Például egy taxisofőr szimulált forgalomban történő értékelésére több szimulációt futtatnánk különböző forgalommal, világítással, és időjárási viszonyokkal. Ha az ágenst egyedi esetre terveztük, kihasználhatjuk az adott környezet speciális tulajdonságait, de nem találhatjuk meg a vezetés általában jó módját. Ezért a programtár tartalmaz egy környezetgenerátort (environment generator) minden környezetosztályra, amelyik kiválaszt bizonyos környezeteket (bizonyos valószínűséggel) az ágens futtatására. Például a porszívó környezetgenerátor véletlenszerűen állítja be a piszok mintázatát és az ágens helyét. Ezek után az ágens átlagos teljesítményére vagyunk kíváncsiak az adott környezetosztályra nézve. Egy racionális ágens az adott környezetosztályra maximalizálja ezt az átlagos teljesítményt. A  és  közötti feladatok végigvisznek egy környezetosztály kialakításán és abban többféle ágens kiértékelésén.
A legegyszerűbb példát a determinisztikus csomópontok (deterministic nodes) szolgáltatják. Egy determinisztikus csomópont értékét a szüleinek az értéke teljesen meghatározza, mindenfajta bizonytalanságtól mentesen. A reláció lehet egy logikai kapcsolat – például ha a szülőcsomópontok azt jelentik, hogy Kanadai, Egyesült Államokbeli és Mexikói, a gyermekcsomópont pedig azt, hogy Észak-Amerikai, akkor a közöttük lévő kapcsolat egyszerűen a szülők diszjunkciója. A reláció lehet numerikus is – például ha a szülőcsomópontok egy gépkocsi különböző árai különböző forgalmazóknál, a gyermekcsomópont pedig az az ár, amit egy legolcsóbbat kereső vevő végezetül fizetne, akkor a gyermekcsomópont értéke a szülők értékeinek a minimuma. Másik példa, ha a szülőcsomópontok az egy tóba bejövő vízmennyiségek (folyók, vízlevezetők, csapadék) és az onnan eltávozó vízmennyiségek (folyók, párolgás, elszivárgás), a gyermek pedig a tó szintjének a megváltozása, akkor a gyermek értéke a kifolyó és befolyó szülők értékeinek a különbsége.
A játék szabálya – mint az előzetesen is látszott – végtelenül egyszerű. Van n db halmunk (eredetileg hárommal játszották), mindegyikben valamennyi (természetesen véges) elemszámú kavics. Ezekből a halmokból kell kavicsokat a játékosoknak felváltva eltávolítani, majd az veszít, aki az utolsó kavics elvételére kényszerül. A játék menete, hogy a soron következő játékos kiválaszt egy halmot, majd egy vagy több kavicsot eltávolít a kupacból, nyilván legfeljebb az összeset, majd ezután a másik játékoson a sor. Mivel véges számú kupac, és minden kupacban véges számú kavics van, ezért az összes kavics száma is véges. Ezen feltételek mellett minden lépésben csökken a játéktéren elhelyezkedő kavicsok száma, így a játék legfeljebb összes kavicsszám lépésben véget ér. A szabályból adódik az is, hogy döntetlen állás nincs.
* BT: Visszalépéses keresés * BT-DEG: Visszalépéses keresés fokszám heurisztikával * BT-FC: Visszalépéses keresés előretekintéssel * BT-FC-MRV: Visszalépéses keresés előretekintéssel, és legkevesebb megmaradó érték heurisztikával * BT-FC-LCV: Visszalépéses keresés előretekintéssel, és legkevésbé korlátozó érték heurisztikával * BT-AC3: Visszalépéses keresés élkonzisztencia ellenőrzéssel * BT-AC3-MRV-DEG-LCV: Visszalépéses keresés élkonzisztencia ellenőrzéssel, és legkevesebb megmaradó érték, fokszám, illetve legkevésbé korlátozó érték heurisztikával * MC: Lokális keresés (Min-Conflicts algoritmus)
E változástól eltekintve a nyelvtannal kapcsolatban minden más változatlan marad, ami ösztönző hír; azt sugallja, hogy helyes úton járunk, ha ilyen könnyedén bonyolíthatjuk például az igeidővel (bár csak érintettük az időre és igeidőre vonatkozó teljes nyelvtan felszínét). E bemelegítésként elért sikerrel készen állunk egy sokkal nehezebb reprezentációs probléma kezelésére.
A  alfejezet felvázolta a játékok és a feltételes tervkészítési problémák közötti analógiát. A  ábrán szereplő feltételes tervkészítő algoritmus olyan tervet készít, amely a környezetről a legrosszabb esetet feltételezve is működik, így olyan versenyhelyzetekben is alkalmazható, ahol az ágenst csak a siker vagy a bukás érdekli. Amikor az ágens és ellenfelei a terv költségeivel is foglalkoznak, akkor a minimax megoldás a helyénvaló. Eddig nagyon kevés munkát fordítottak a minimax módszer olyan módszerekkel való kombinálására, mint a részben rendezett tervkészítés vagy a HFH-tervkészítés, ami túlmutat a  fejezetben használt állapottér-keresési modellen. A versengés kérdésére a játékelmélettel foglalkozó  alfejezetben még visszatérünk.
Ez a kérdés az 13. szakasz - Állítások részben tárgyaltaknak megfelelően, az elemi események tulajdonságaival foglalkozik. a. Bizonyítsa be, hogy az összes lehetséges elemi esemény egyesítése logikailag megegyezik az igaz állítással. (Segítség: bizonyítékként alkalmazzon indukciót a véletlen változók számára vonatkozóan.) b. Bizonyítsa be, hogy bármely állítás logikailag ekvivalens azon elemi események diszjunkciójával, amelyek maguk után vonják annak igazságát.
Az alapelem minden mintavételi algoritmusban a minták generálása egy ismert valószínűség-eloszlásból. Például egy szabályos érme felfogható egy Érme valószínűségi változónak 〈fej,írás〉 értékekkel és P(Érme) = 〈0,5, 0,5〉 a priori eloszlással. A mintavétel ebből az eloszlásból pontosan megfelel egy érme dobálásának: 0,5 valószínűséggel fej-et ad, és 0,5 valószínűséggel írás-t. Ha rendelkezésre áll a [0, 1] tartományba eső véletlen számoknak egy forrása, akkor bármely egyváltozós eloszlásból egyszerű dolog mintavételezni (lásd  feladat).
Nem következik, mivel nem kötöttük ki, hogy a tárgyterület csak a-ból és b-ből áll. Amennyiben tartalmaz például egy c elemet is, úgy erről a c elemről és P(c) állítás igazságértékéről semmit nem tudunk.
Az előretekintés előbb, a 12-es lépésben az SA változó értékkészletéből vette ki a GREEN értéket a Q=GREEN értékadás miatt. Tehát ezt kell invertálnunk. Magyarán az SA értékkészletet a mostani {} üresről visszamódosítjuk {GREEN}-re.
Ez a modell rendelkezik azzal az előnnyel, hogy könnyű elmagyarázni és megvalósítani. Azonban rendelkezik néhány hátránnyal is. Először, a dokumentum relevanciája egyetlen bit, így nincs semmilyen iránymutatás arra, hogy hogyan rendezzük a megjelenítés során a releváns dokumentumokat. Másodszor, a logikai kifejezések szokatlanok lehetnek a nem programozó vagy nem logikával foglalkozó felhasználók számára. Harmadszor, még egy gyakorlott felhasználó számára is nehéz lehet a megfelelő lekérdezést megfogalmazni. Tételezzük fel, hogy az [információkeresés ÉS modellek ÉS optimálás] kérdést tesszük fel, és üres eredményhalmazt kapunk vissza. Megpróbálhatjuk az [információkeresés VAGY modellek VAGY optimálás] lekérdezést, azonban ha túl sok eredményt ad vissza, akkor nehéz megmondani, mit kell utána kipróbálni.
Folytonos fekete vonal jelzi az akadályt, piros vonal jelzi az algoritmus első lépését, ami után elakad! (Mindkét szomszédos állapot távolabb van a céltól, mint az aktuális, így a hegymászó algoritmus lokális maximumba került.)
Az említett randomizált reflexszerű ágens egészen jól teljesít kisméretű porszívóvilágokban, különösen, ha a kosz valószínűsége nagy. Ha azonban a piszkos/tiszta szobák aránya csökken (akár takarítás közben, akár eleve kevés volt a kosz), úgy romlik a hatékonysága is. Már a vizsgált 3x3-as modellben is sokáig keresgélheti a sarokban maradt koszt.
Itt tehát bevezettünk egy újabb jelölést: V jelöli azt a haszon-függvényt, amely a teljesítménymértékre alapozva megmondja, hogy mennyire jó egy adott ágens-függvénynek megfelelő ágens egy adott környezetben. Az ágens (illetve tervezőjének) célja e függvényt maximalizálni (vagy minimalizálni). A cél tehát az, hogy az adott környezetben az ágens olyan állapot-történetet eredményezzen, amely maximálja a teljesítménymértéket.
A hiba-visszaterjesztési algoritmus a következő módon foglalható össze: * Számítsuk ki a kimeneti neuronokra a Δ értékeket a megfigyelt hiba alapján. * A kimeneti réteggel kezdve ismételjük a következő lépéseket minden rétegre, amíg a legelső rejtett réteget el nem érjük: + Terjesszük vissza a Δ értékeket a megelőző rétegre. + Frissítsük a két réteg közötti súlyokat.
Egy feltételezésalapú igazság-karbantartó rendszert (assumption-based truth maintenance system, ATMS) arra terveztek, hogy ezt a fajta hipotetikus világok közötti kontextusátkapcsolást kifejezetten hatékonnyá tegye. Egy JTMS-ben az igazolások karbantartása lehetővé teszi, hogy az egyik állapotból gyorsan egy másikba lépjünk át néhány visszavonással és kijelentéssel, de minden időpillanatban csak egyetlen állapotot reprezentálunk. Egy ATMS az összes állapotot egyszerre reprezentálja, amivel valaha is foglalkoztunk. Míg egy JTMS egyszerűen bent- vagy kintlevőnek jelöli a mondatokat, egy ATMS nyomon követi, minden egyes mondatra, hogy a mondatot mely feltételezések tennék igazzá. Más szavakkal minden egyes mondatnak van egy címkéje, amely a feltételezés halmazokat tartalmazó halmazból áll. A mondat csak azon esetben áll fenn, amikor egy feltételezés halmaz összes feltételezése fennáll.
Mivel újfent 8 különböző értékkombinációnk van, ezért az előbbi, 5-ös állítás esetéhez hasonlóan (lásd. 2-2. táblázat) most is egy 8-elemű értékkészlettel rendelkező további, immáron 54. (segéd)változót célszerű bevezetnünk. Legyen a változó neve ZNB, értékkészlete pedig: {1, 2, 3, 4, 5, 6, 7, 8}.
El lehet merengeni a „nyerés esélye” kifejezésen. Végül is a sakk nem szerencsejáték, az aktuális állapotot biztosan tudjuk, és nincs a dologban kockavetés. Ha azonban egy nem végállapotban levágtuk a keresést, akkor az algoritmus szükségszerűen bizonytalan lesz annak valódi kimenetelében. Ezt a bizonytalanságot nem információs, hanem inkább számítási korlátok okozzák. Ha a kiértékelő függvény egy adott állapotban csak egy bizonyos mennyiségű számítást végezhet el, a legjobb, amit tehet, hogy kitalálja a végkimenetelt.
A viccekkel kapcsolatban az első jelentős kutatást Freud végezte, legfőbb eredményeit a 1905-ös könyvében foglalta össze. Freud a viccek osztályozása közben a következő viccet hozza példának: Egy férfi belép egy pékségbe és rendel egy tortát, de ezt hamarosan visszaviszi, és kér egy pohár italt. Ezután megissza az italt és fizetés nélkül távozik. A pék utána szól: –Nem fizette ki az italát –De adtam egy tortát cserébe –Azt sem fizette ki –Nem is ettem meg.
Amennyiben a szimulátor konfigurációja engedélyezi a játék végén a grafikonok megjelenítését, akkor az utolsó kör után megjelenő, az alábbihoz hasonló ablakban látható az ágensek illetve a csapatok összenergiájának alakulása.  ábra - A grafikonok ablak A grafikonok ablak
Hátrány viszont, hogy a módszer lineáris felbontása egyelőre korlátozott és a mérések többször tíz percig is eltartanak, ami nehezíti az együttműködésre képtelen beteg vizsgálatát. A módszer érzékenysége korlátozott ki kiterjedésű meszesedések és friss vérömlenyek kimutatásában. A nagy térfogatot érintő, hosszú mérési folyamat alatt a mellkasban és hasban gyakran keletkeznek mozgási műtermékek. Az elmozdulás miatt a kisgyermekek, nyugtalan vagy zavart felnőttek, súlyos sérültek nem vagy alig vizsgálhatók. Túlságosan testes egyedek be sem férnek a zárt mágnes gyűrűjébe. A szűk méretalagútban némelyek olyan súlyos bezártsági iszonytól szenvednek, hogy nyugtatóra szorulnak, sőt a vizsgálat meg is hiúsulhat.( ábra).
Nézzük most sorban azokat az elemeket, amelyek szükségesek ahhoz, hogy formálisan is meg tudjuk fogalmazni, hogy mit értünk implementáció alatt. Lássuk, hogy formálisan mit értünk környezet, mechanizmus, adott környezet fölött adott mechanizmus által indukált játék, illetve közösségi döntési függvény alatt!
Láthatjuk, hogy a finom mozgások tervezése nemtriviális; valójában sokkal nehezebb, mint a pontos mozgások tervezése. Vagy minden egyes mozgáshoz rögzített számú diszkrét értéket választunk, vagy a környezet geometriáját használjuk fel azon irányok megállapításához, amelyek minőségileg különböző viselkedést eredményeznek. A finommozgás-tervező a konfigurációs tér leírását, a sebességbizonytalansági kúp szögét és azt a specifikációt tekinti inputnak, amely megadja a lehetséges leállásifeltétel-érzékeléseket (ebben az esetben a felülettel való érintkezést). Az eredménynek egy olyan többlépéses feltételes tervnek vagy stratégiának kell lennie, ami garantálja a sikert, ha egyáltalán létezik ilyen terv.
Végeredményben tehát olyan mentális objektumok és az azokat manipuláló mentális folyamatok modelljére van szükségünk, amelyek másvalaki fejében (vagy másvalami tudásbázisában) találhatók meg. A modellnek valósághűnek kell lennie, azonban nem kell a részletekre kitérnie. Nem kell, hogy képesek legyünk megjósolni, hogy egy konkrét ágens a következtetéseit hány milliszekundum alatt állítja elő, vagy hogy egy konkrét vizuális ingerrel szembesítve, mely neuronok tüzelnek az állat agyában. Boldogok leszünk, ha kikövetkezhetjük, hogy román rendőr eligazít minket Bukarest irányába, ha tudja az utat, és azt hiszi, eltévedtünk.
Az osztályozás felügyelt tanítási probléma, és mint ilyen, a 18. fejezetben ismertetett bármelyik módszerrel megtámadható. Az egyik népszerű megközelítés a döntési fák alkalmazása. Amennyiben rendelkezünk a megfelelő kategóriákkal címkézett dokumentumokból álló tanító halmazzal, építhetünk egyetlen döntési fát, amelynek levelei a dokumentumot a megfelelő kategóriához rendelik. Ez akkor működik jól, ha mindössze néhány kategória van; nagyobb kategóriahalmazok esetén minden egyes kategóriára külön döntési fát építünk, amelynek a levelei megadják, hogy a dokumentum az adott kategóriába tartozik-e vagy sem. Általában az egyes csomópontokban tesztelt tulajdonságok egyedi szavak. Például a „Sport” kategóriában az egyik csomópont tesztelheti a „kosárlabda” szó meglétét. Javított teljesítményű döntési fák, naiv Bayes-modellek, valamint szupport vektor gépek mindegyikét használták szövegosztályozásra, sok esetben a hitelesség 90–98%-os volt bináris osztályozás esetén.
Általánosságban elmondható, hogy az átméretezésre való érzéketlenséget az ún. skála térbe való áttéréssel lehet elérni. Ennek során a kép kiegészül plusz egy skála dimenzióval és a kép megfelelő skálabeli értékei egy skála paramétertől függő szórásparaméterű Gauss simítással kaphatók. Ezzel az átméretezéskor előforduló részletvesztés modellezhető. Az invariáns tulajdonság ára, hogy az eddig általában kétdimenziós paramétertér –, a két síkbeli koordináta – egy dimenzióval kibővül, tehát a keresési tér nagyban megnő. Az affin invariancia elérése hasonlóképp történik. A különbség, hogy ebben az esetben a Gauss simításhoz használt kernelt egy – a síkban egy 2x2-es mátrixszal leírható – lineáris transzformációnak kell alávetni, mely egy anizotrop Gauss szűrést eredményez.
A biológiában evolúció alatt folyamatos változások olyan sorozatát értjük, melynek során bizonyos populációk öröklődő jellegei nemzedékről nemzedékre változnak. Más megközelítésben az evolúció alatt a populációknak a változó környezeti feltételekhez való alkalmazkodását értjük, mely alkalmazkodást génállományuk, ezen keresztül tulajdonságaik megváltozásával érik el.
Míg az identifikáció határátmenetben megközelítés a végső konvergenciára koncentrál, addig a Kolmogorov-komplexitás (Kolmogorov complexity) vagy más néven algoritmikus komplexitás (algorithmic complexity), amelyet egymástól függetlenül Solomonoff (1964) és Kolmogorov (1965) fejlesztett ki, formális definíciót próbál adni az Ockham borotvája elvben használt egyszerűségnek. Hogy valahogy kimeneküljenek abból a csapdából, hogy az egyszerűség függ az információ reprezentációjának módjától, azt javasolták, hogy az egyszerűséget annak a legrövidebb programnak a hosszával mérjék, amely egy általános Turing-gépen helyesen adja vissza a megfigyelt adatokat. Bár sok különböző általános Turing-gépet lehet létrehozni, és így több különböző „legrövidebb” program lehetséges, ezek hossza legfeljebb csak egy konstansban tér el, amely független az adatok mennyiségétől. Ezt a gyönyörű eredményt, amely betekintést nyújt abba, hogy bármely előzetes reprezentációs eltérést végül maguk az adatok győznek le, csupán az teszi tönkre, hogy a legrövidebb program hosszának kiszámítása eldönthetetlen problémára vezet. Közelítő mértékeket lehet alkalmazni, például a legrövidebb leíró hosszt (minimum description length) vagy LLH-t (Rissanen, 1984), amivel kimagasló gyakorlati eredményeket értek el. Li és Vitányi írása a Kolmogorov komplexitás legjobb összefoglalása (Li és Vitányi, 1993).
Bizonyos esetekben ki kell szűrni a képekből a fontos információt. Ez lehet a képfeldolgozásnál jelentkező zaj (BP 98) vagy fölösleges adatok, amik a megoldás keresését nehezítik (BP 73).  ábra - BP 98: háromszög - négyszög BP 98: háromszög - négyszög  ábra - BP 73: ellipszis és téglalap merőleges - párhuzamos BP 73: ellipszis és téglalap merőleges - párhuzamos
A háló topológiája – a csomópontok és élek halmaza – megadja a tárgyterületen fennálló feltételes függetlenségi kapcsolatokat, hogy mily módon, azt hamarosan pontosan kifejtjük. Egy helyesen létrehozott hálóban az X csomópontot az Y csomóponttal összekötő nyíl intuitív jelentése rendszerint az, hogy az X-nek közvetlen befolyása van az Y-ra. A tárgyterület szakértője számára általában könnyű eldönteni, hogy milyen közvetlen befolyások teljesülnek egy adott területen – valójában sokkal könnyebb, mint a megfelelő valószínűségeket megadni. Ha pedig a Bayes-háló topológiája kész, már csak az egyes változókhoz tartozó feltételes valószínűség-eloszlásokat kell meghatározni a szülőkkel mint feltételekkel. Látni fogjuk, hogy a topológia és a feltételes eloszlások együttese elegendő, hogy megadja (implicit módon) az összes változó feletti együttes valószínűség-eloszlás függvényt.
i) Vegyünk egy egészen ügyetlen heurisztikát, amely általában a Manhattan távolsággal dolgozik, de valamiért nem kedveli azokat az állapotokat, ahol a hatos mező a középen van. Ilyen esetekben hússzal nagyobb a függvény értéke. Tekintsük most a következő ábrát! A kezdőállapotból az optimális megoldás 10 lépéses (az alsó 2x3-as részt kell két lépésben körbeforgatni az óramutató járásának megfelelő irányban), de a heurisztika ezt a cselekvéssorozatot el fogja kerülni, mivel rögtön középre kéne tennie a hatos mezőt! Ehelyett egy szuboptimális, 20 lépéses megoldást nyújt (a megoldás maga hasonló a fentihez, csak most az óramutató járásával ellentétes irányban forgatjuk körbe az alsó részt).
Az eljárásmód-javítás lépése nyilvánvalóan egyértelmű, de hogyan valósítsuk meg az eljárásmód-értékelést. Kiderül, hogy ennek elvégzése sokkal egyszerűbb, mint a szabványos Bellman-egyenletek megoldása (amit az értékiteráció végez el), mivel az eljárásmód egy cselekvést minden állapotban rögzít. Az i-edik iterációban a π[i] eljárásmód a π[i](s) cselekvést írja elő. Ez azt jelenti, hogy a   Bellman-egyenlet egy egyszerűsített változatával állunk szemben, ami az s hasznosságát (π[i] mellett) a következőképpen kapcsolja a szomszédai hasznosságához:
A 8-as kirakójáték a bonyolultabb 15-ös kirakójáték kisebb rokona, amit a híres amerikai játéktervező, Sam Loyd talált ki az 1870-es években (Loyd, 1959). A játék akkoriban gyorsan óriási, Rubik bűvös kockájához mérhető népszerűségre tett szert Amerikában. A matematikusok is felfigyeltek a problémára (Johnson és Story, 1879; Tait, 1880). Az American Journal of Mathematics szerkesztői megállapították: „A 15-ös kirakójáték az elmúlt hetekben teljesen lebilincselte az amerikai embereket – 10 emberből 9 nemre, korra és származásra való tekintet nélkül a játék bűvöletébe került. Ez azonban még nem hatott volna a szerkesztőkre olyan mértékben, hogy rávegye őket, erről a témáról cikkeket jelentessenek meg az American Journal of Mathematics folyóiratban, de figyelembe véve azt a tényt, hogy…” (itt a 15-ös kirakójáték által felvetett érdekes matematikai kérdések rövid taglalása következett). A 8-as játék esetén P. D. A. Schofield számítógép segítségével elvégezte a probléma kimerítő elemzését (Schofield, 1967). Ratner és Warmuth megmutatta, hogy a legrövidebb megoldás megkeresése a 15-ös játék n × n-es általánosításánál az NP-teljes problémák osztályába tartozik (Ratner és Warmuth, 1986).
4. (direktség) Ha már abban bizonyosak is vagyunk, hogy X,Y  változók között létezik oksági kapcsolat, adott esetben fontos lehet azt is tudni, hogy az egy aktuális változóhalmazhoz képest ``direkt''-e, azaz más változók által közvetített vagy nem (máshogy fogalmazva a többiek ismerete ezt a kapcsolatot blokkolja vagy nem).
Faszerkezet: a deszkriptor helyét mutatja más deszkriptorok viszonylatában egy témakörön belül. Pl: az egyes speciális agyi idegek esetében azt látjuk, hogy egy tágabb deszkriptornak, az AGYI IDEGEK-nek vannak alárendelve. Minden deszkriptornak van legalább egy ilyen besorolása, melynek segítségével általánosabbá, szélesebbé tehetjük a keresést, vagy éppenséggel specifikusabb területeket választhatunk.
Ha ezt a kifejezést jobbról balra kiértékeljük, valami érdekeset veszünk észre:  definíció szerint 1-gyel egyenlő. Így ezt eleve kihagyhattuk; a változó M a kérdésre irreleváns. Máshogy fogalmazva, a P(JánosTelefonál∣Betörés = igaz) lekérdezés eredményét nem változtatja meg a MáriaTelefonál eltávolítása a hálóból. Általában, bármely levélcsomópontot eltávolíthatunk, ami nem célváltozó vagy bizonyítékváltozó. Eltávolítás után lehetnek újabb levélcsomópontok, amelyek szintén irrelevánsak lehetnek. Fontos Ezt az eljárást folytatva végül szükségszerűen arra jutunk, hogy minden változó, ami nem őse a célváltozónak vagy egy bizonyítékváltozónak, irreleváns a lekérdezésre. A változó elimináló algoritmus ezért az összes ilyen változót eltávolíthatja a lekérdezés kiértékelése előtt.  ábra - A változó eliminálás algoritmus Bayes-hálós lekérdezések megválaszolására A változó eliminálás algoritmus Bayes-hálós lekérdezések megválaszolására
A H hipotézistér ezek után az összes olyan hipotézis {H[1], …, H[n]} halmaza, amelyek kezelésére a tanuló algoritmust tervezték. Például Döntési-Fa-Tanulás algoritmus az adott attribútumokkal kialakítható összes döntési fa hipotézissel képes foglalkozni; így hipotézistere az összes döntési fát tartalmazza. Feltehetően a tanuló algoritmus hisz abban, hogy az egyik hipotézis korrekt, azaz hisz a
A kommunikáló ágens feladata annak eldöntése, hogy mikor van szükség egy szólásaktusra, és hogy melyik a helyénvaló az összes lehetséges közül. A szólásaktusok megértésének problémája hasonlatos más megértési (understanding) problémákhoz, mint például a képek megértése vagy a betegségek diagnosztizálása. Kapunk egy halmazt többértelmű bemenetekkel, amikből visszafelé haladva azt kell eldöntenünk, hogy a világ mely állapota hozhatta létre őket. Azonban mivel a beszéd tervezett cselekvés, a megértés magában foglalja a tervfelismerést is.
8.9 Írjon egy általános tény- és axiómahalmazt, hogy reprezentálja a következő állítást: "Wellington hallott Napóleon haláláról"; és hogy helyesen megválaszolhassuk a kérdést: "Hallott Napóleon Wellington haláláról?".
A predikátum kalkulus segítségével egy szöveg mondatainak jelentését kísérelhetjük megfogalmazni a logika nyelvén, hogy aztán belőlük következtetéseket vonhassunk le. Ez bizonyos mondatok esetében akár igen jól is működhet, ám a természetes nyelvnek vannak olyan elemei, amelyek nem fejezhetőek ki hatékonyan a predikátumlogika segítségével. Ilyen elemek: a modalitás (lehetőség, szükségesség), a hit (feltételezés) és az időbeliség. ([5], 97. old.) Egy konkrét problémán illusztrálva a feltételezések kezelésének nehézségét: „Egy liter tej 500 Ft.” Ennek a mondatnak a vizsgálata esetén az „egy liter tej” kifejezést helyettesíthetjük annak értékével, ami például 199 Ft. A kapott kifejezés: 199 Ft = 500 Ft, ami alapján a mondat hamisnak tekinthető. Az „Azt hiszem, egy liter tej 500 Ft.” mondat igazságtartalma azonban már nem vizsgálható ugyanezzel a módszerrel, hiszen ez a mondat már aligha helyettesíthető az „Azt hiszem, 199 Ft = 500 Ft” megállapítással. Az időbeliség kifejezésére már dolgoztak ki viszonylag jól működő kiegészítéseket, mint pl. a szituáció kalkulust. Hasonlóképpen a modalitás esetében is léteznek megoldások, mint például az (egyszerű) modális predikátum logika. [6][7] A már említetteken kívül további olyan nyelvi elemek is léteznek, amelyek kifejezése pusztán a predikátum kalkulus segítségével problémás (például az előfeltételezések és a bizonytalanság („fuzziness”) ([5], 98-99. old.). Ezekre a problémákra rendre megoldásokat kell találni, bővíteni a predikátum kalkulust, ha hatékonyan akarjuk vele kifejezni a természetes nyelvű szövegekben megfogalmazott információt. Ilyen értelemben tehát a predikátumlogika igen korlátozott tudásreprezentációs eszköz. Viszont ahogy azt a számos továbbfejlesztéséből is láthatjuk, mégis az egyik nagyon fontos alapja a tudás alapú megközelítésben használt modelleknek.
A felidézést és pontosságot akkor definiálták, amikor az IR-kereséseket elsődlegesen könyvtárosok végezték, akik alapos, pontos találatokban voltak érdekeltek. Manapság a legtöbb (napi több százmillió) lekérdezést az internetfelhasználók végzik, akik kevésbé érdekeltek az alaposságban, sokkal inkább abban, hogy azonnal választ kapjanak. Számukra jó mérték az első releváns találat átlagos reciprokrangja (reciprocal rank). Azaz, amennyiben a rendszer első találata releváns, 1-es pontszámot kap a lekérdezésre, és amennyiben az első kettő nem releváns, de a harmadik az, akkor 1/3-ot. Egy alternatív mérték a válaszidő (time to answer), ami azt méri, hogy mennyi ideig tart a felhasználónak a problémára kívánt választ megtalálni. Ez kerül a legközelebb ahhoz, amit mérni szeretnénk, azonban azzal a hátránnyal rendelkezik, hogy minden egyes kísérlethez új emberi tesztalanycsoportra van szükség.
A rendszer kiépítése drága, egyszeri nagy beruházást, és folyamatos működtetést igényel, valamint hogy állandóan monitorozni is szükséges ahhoz, hogy az adott pillanatbeli állapotának ismeretében pontosan meg lehessen határozni a továbblépés feltételeit.
A P ⇔ Q ekvivalencia igazságtáblája azt mutatja, hogy akkor igaz, ha mind P ⇒ Q és Q ⇒ P igaz. Ezt gyakran úgy írjuk le, hogy „P akkor és csakis akkor, ha Q” vagy matematikában szokták jelölni „P aa Q”-nak. A wumpus világ szabályait legjobban az ⇔ használatával tudjuk felírni. Például, egy négyzet szellős, ha a szomszédos, ha a szomszédos négyzetben csapda van, és egy négyzet csakis akkor szellős négyzetben csapda van. Így ekvivalenciákra van szükségünk, mint a
A jelenlegi orvosi gyakorlat a már bekövetkezett eseményekre (például tünetek, panaszok megjelenése) reagál. A terápia iteratív titrálása átmeneti kudarcokon keresztül halad előre, ami természetesen megterheli a beteget, és szükségtelen költségekkel jár. Természetesen más volna a helyzet, ha egyes genetikai eltérésekről tudnánk, hogy elősegítik bizonyos betegségek kialakulását, mert ilyenkor megfelelő intézkedésekkel megelőzhetnénk (de legalábbis késleltethetnénk) a kórfolyamatokat. Ez jelentős gazdasági haszonnal is járna, mert a megelőzésre fordítandó pénz sokkal kisebb annál az összegnél, amelyet a krónikus betegségek terápiájára kell költeni.
A valószínűségi környezetfüggetlen nyelvtanok (PCFG) megválaszolják Chomsky valószínűségi modellekkel kapcsolatos összes ellenvetését, és a CFG-khez képest előnyökkel rendelkeznek. A PCFG-ket Booth (Booth, 1969) és Salomaa (Salomaa, 1969) vizsgálták. Jelinek bemutatja a veremdekódoló algoritmust, amely a Viterbi-keresés olyan variációja, amely arra használható, hogy megtalálja egy PCFG-vel a legvalószínűbb elemzést (Jelinek, 1969). Baker vezette be a belső–külső algoritmust (Baker, 1979), Lari és Young leírta használhatóságát és korlátait (Lari és Young, 1990). Charniak (Charniak, 1996), valamint Klein és Manning (Klein és Manning, 2001) a treebank nyelvtanokkal történő elemzést tárgyalják. Stolcke és Omohundro megmutatták, hogyan lehet nyelvtani szabályokat tanulni Bayes-modellek egyesítésével (Stolcke és Omohundro, 1994). További PCFG-algoritmusokat mutatott be Charniak (Charniak, 1993), valamint Manning és Schütze (Manning és Schütze, 1999). Collins a terület áttekintő tanulmányát kínálja, továbbá az egyik legsikeresebb statisztikai elemzőprogram magyarázatát (Collins, 1999).
A pótlásnak lehetnek negatív következményei is, ezeket az elemzés során figyelembe kell venni. * A pótlás révén teljes lesz az adathalmaz, de a rajta végzett következtetés csak akkor lehet érvényes, ha a modellre jellemző feltevések nem sérülnek általa. * A pótlás megváltoztathatja az egyes változók közötti függéseket. * Ha a pótolt értékeket megfigyeltnek tekintjük, akkor a változó varianciáját nagymértékben alulbecsülhetjük.
„A forrásnyelven megírt programot ebben az esetben nem fordítjuk le. A program futtatásához egy interpreter (értelmezőprogram) szükséges, mely utasításonként olvassa, majd értelmezi a forrásnyelven megírt programot. Az értelmezett utasítást azonnal végre is hajtja, majd megkeresi a végrehajtásban következő utasítást.”
Lucas ellentmondana önmagának, ha azt állítaná, hogy a fenti (teljes) mondat igaz, tehát Lucas nem tudja ezt a mondatot konzisztensen állítani, tehát a mondat mindenképpen igaz. (A mondat nem lehet hamis, mert ha hamis volna, akkor Lucas nem tudná konzisztensen állítani, tehát igaz lenne.) Bebizonyítottuk tehát, hogy van egy mondat, amelyet Lucas nem tud konzisztensen állítani, mások viszont (beleértve a gépeket is) igen. De ez előttünk semmit sem von le Lucas értékéből. Egyetlenegy ember sem tudja, hogy egy másik példát vegyünk, élete során kiszámítani tízmilliárd tízjegyű szám összegét, egy számítógép viszont másodpercek alatt megteszi ezt. Mégsem tartjuk ezt az emberi gondolkodási képesség alapvető korlátjának. Évezredeken át, mielőtt felfedezték volna a matematikát, az emberek ugyanúgy intelligens viselkedést tanúsítottak, nem valószínű hát, hogy a matematika a periferiálisnál fontosabb szerepet játszana az intelligencia meghatározásában.
Bár a szerencsejátékok már i. e. 300 körül is ismertek voltak, az esélyek és a valószínűségek matematikai analízise jóval későbbre tehető. Mahaviracarya Indiában nagyjából az i. e. 9. században kezdte meg a téma vizsgálatát. Európában az első kísérletek csak az olasz reneszánsz idejére, kb. 1500-ra tehetők. Az első jelentős következetes elemzés Girolamo Cardano nevéhez fűződik (1565), amely azonban 1663-ig nem került nyilvánosságra. Ekkorra Blaise Pascalnak a valószínűségek szisztematikus kiszámítására vonatkozó felfedezése (Pierre Fermat-val levelezve 1654-ben) megalapozta a valószínűség-számítást, mint a matematika széles körben és eredményesen vizsgált ágát. Az első valószínűségekkel kapcsolatos tankönyv a De Ratiociniis in Ludo Aleae volt (Huygens, 1657). Pascal bevezette a feltételes valószínűség fogalmát is, amelyről szintén szó van Huygens könyvében. Thomas Bayes anglikán lelkész (1702–1761) nevéhez fűződik a később róla elnevezett szabály, amely feltételes valószínűségek számítására alkalmas. Eredménye csak halála után jelent meg nyomtatásban (Bayes, 1763). Kolmogorov volt az első (Kolmogorov, 1950; első megjelenés németül 1933), aki a valószínűség-számítást szigorúan axiomatikus keretek között tárgyalta. Rényi (Rényi, 1970) olyan axiómarendszert vezetett be, amely nem a feltétel nélküli, hanem a feltételes valószínűség fogalmára épül.
Példa: A Hittérítő- Kannibál feladat állapotterének eleme az összes olyan állapot, amelyben a hittérítők száma összesen három, a kannibálok száma összesen három, és a csónak a bal és jobb part közül pontosan az egyiken van. (A feladat megoldása során ennél valamivel kevesebb állapot lesz valóban elérhető.)
Az eljárás ciklusa mind X, mind Y összes értéke fölött fut, számba véve minden olyan elemi eseményt, amely e rögzített értéke mellett lehetséges, az együttes valószínűségi tábla alapján összegzi ezek valószínűségeit, és normalizálja az eredményeket.
Az elme filozófiai képének utolsó eleme a tudás és a cselekvés kapcsolata. Az MI szempontjából ez a kérdés elsődleges fontosságú, mert az intelligencia cselekvést is és következtetést is igényel. Továbbá ahhoz, hogy világos legyen előttünk, hogyan kell egy olyan ágenst építeni, amelynek cselekvései jogosak vagy racionálisak lesznek, meg kell értenünk, hogy hogyan igazolhatók a cselekvések. Arisztotelész a cselekvéseket azért tartotta jogosnak, mert a célok és a cselekvések kimenetelei között logikai kapcsolat lelhető fel (az alábbi idézet záró része a könyv borítóján is megtalálható):
A humor eredeti jelentése testnedv; az ókori-középkori elképzelések szerint a különböző jellemeket a testnedvek(humores) különböző arányú keveredése határozza meg.[2] A „homour” szót az angolok a 16. század második felétől már mint hangulat, kedélyállapot használják, utána jelentése bevettől eltérő, excentrikus magatartásra módosul. A 18. századtól a humor már mint esztétikai kategória is megjelenik, főleg Jean Paul, F. W. J. von Schelling, A. W. és F. Schlegel munkáinak köszönhetően. Ekkor a humor már a romantikus képzelőerő alapeleme. Jean Paul azonosoítja is a humorost a romantikussal, illetve összekapcsolja azt a komikussal. Szerinte a humor fő forrása az egyén és a világ meghasonlása, ezt a motívumot utána S Kierkegaard viszi tovább. A 19-20. században már mint esztétikai elem jelenik meg a humor, a történelmi jelentése teljesen háttérbe szorul. S. Freud fejleszti tovább ezután a humor elméletét, szerinte a humor indulat-megtakarítás, a feszültség oldásának eszköze.
Azaz, az [1, 3] (és a szimmetria miatt a [3, 1]) nagyjából 31% valószínűséggel tartalmaz csapdát. Hasonló számításokból (amelyet az olvasó könnyen elvégezhet) a [2, 2]-re 86%-os valószínűséggel adódik csapda. A wumpus ágensnek határozottan el kell kerülnie a [2, 2]-t!  ábra - C[2,2] és C[3,1 ]peremváltozók, az egyes modellek C(perem) értékét mutató konzisztens modelljei: (a) három, két vagy három csapdát jelző modell C[1,3] = igaz mellett, és (b) két, egy vagy két csapdát jelző modell C[1,3] = hamis mellett. C2,2 és C3,1 peremváltozók, az egyes modellek C(perem) értékét mutató konzisztens modelljei: (a) három, két vagy három csapdát jelző modell C1,3 = igaz mellett, és (b) két, egy vagy két csapdát jelző modell C1,3 = hamis mellett.
i) Véges egy állapottér, ha az állapotok száma véges. Azonban ha cselekvések egy sorozata újra és újra végrehajtható és így adott állapotokat ismételhetünk meg tetszőlegesen sokszor, akkor a keresési fa már végtelen mély lehet!
Látszólagos egyszerűsége ellenére keresés (valamilyen konkrét algoritmikus formában) intelligens ágensek legfontosabb algoritmusa. Azért is olvashatunk róla mindjárt a tankönyv elején, az ágens fogalmának bevezetése után, és ha a későbbi anyagban mintha nem is esne már róla szó, ne tévesszen ez minket. Intelligens viselkedés kutatása során kereséssel mindig, mindenhol fogunk találkozni, legfeljebb sokszor burkolt, elfedett formában.
Mint már említettem, a fényceruza kettős célt szolgál, mégpedig a felvinni kívánt objektumok kialakításánál illetve meglévő képelemek módosításánál. A kezdéskor, hogy a ceruza követése elindulhasson, a hagyományos tollak analógiájára, „tintába kell mártanunk”, azaz a kép megfelelő, célkereszttel jelölt részéhez kell illesztenünk, rá kell mutatnunk. Ez csak alakzatok esetén lehetséges, ezért ha üres lenne a képernyő, ahelyett egy INK felirat jelenik meg, aminek segítségével az említett kezdeti lépés elvégezhető.
Gondoljuk át először az árveréseket. Az árverés a legáltalánosabb formájában egy működésmód bizonyos áruknak egy ajánlattevő csoport tagjai számára történő eladására. Az árajánlatok, a stratégiák és a kimenetel meghatározza, hogy ki kapja az árukat és mennyit fizet. A mesterséges intelligencában például az árverések akkor jelennek meg, amikor ágensek egy csoportja eldönti, hogy együttműködjenek-e egy közös terven. Hunsberger és Grosz megmutatta, hogy ez hatékonyan elérhető árveréssel, amiben az ágensek ajánlatokat tesznek a közös tervbeli szerepükre (Hunsberger és Grosz, 2000). Egyelőre olyan árveréseket tekintünk át, amelyekben (1) egyetlen áru van, (2) mindegyik ajánlattevőnek van egy v[i] hasznosságértéke az árura, és (3) ezek az értékek csak az ajánlattevő számára ismertek. Az ajánlattevők megteszik a b[i] ajánlataikat, és a legmagasabb ajánlat nyeri el az árukat, de a működésmód határozza meg, hogyan tehetők meg az ajánlatok, és mi a győztes által fizetendő ár (ami nem szükségszerűen b[i]). Az árverések legismertebb típusa az angol árverés (English auction), amelyben az árverező mindaddig növeli az áruk árát, amíg csak egyetlen ajánlattevő marad, ellenőrizve közben, hogy vajon az ajánlattevők érdekeltek-e még. Ennek a működésmódnak az a tulajdonsága, hogy a legnagyobb v[i] értékkel bíró ajánlattevő nyeri el az árukat b[m] + d áron, ahol b[m] a legmagasabb ajánlat az összes többi játékos között, és d az árverező növekménye az ajánlatok között.^[179] Az angol árverésnél az ajánlattevőknek egyszerű domináns stratégiájuk van: addig tegyünk árajánlatokat, ameddig a jelenlegi költség a személyes érték alatt van. Emlékezzünk arra, hogy a „domináns” azt jelenti, hogy a stratégia minden más stratégia ellen működik. Ez viszont azt jelenti, hogy egy játékos a többi stratégiától függetlenül választhatja ezt. Ezért a játékosoknak nem kell időt és energiát vesztegetniük a többi játékos stratégiáján való elmélkedéssel. Egy működésmódot stratégiamentesnek (strategy-proof) nevezünk, ha a játékosoknak van domináns stratégiája, ami magában foglalja a valódi indítékok felfedését is.
Az algoritmus minden esetben elér egy pontot, ahonnan már nem tud továbblépni. Egy véletűlen módon generált 8-királynő állapotból kiindulva a legmeredekebb emelkedő hegymászó algoritmus az esetek 86%-ában megakad, a problémaeseteknek csupán 14%-át oldja meg. Az algoritmus gyorsan dolgozik, átlagosan 4 lépést tesz, amikor sikerrel jár, és 3 lépést, amikor megakad. Ez egyáltalán nem is olyan rossz a 8^8 ≈ 17 millió állapotot tartalmazó állapottérben.
Látható, hogy az előretekintés szerencsére (közvetve a jó változó-választó heurisztikának köszönhetően) most sem eredményezett egyetlen változó esetében sem üres értékkészletet, így az algoritmus újra meghívja önmagát az aktuális {NSW=RED, Q=GREEN, SA=BLUE, NT=RED} behelyettesítéssel.
a három almából (mint részből, de nem mint elemből) álló összetett objektumot jelöli. A köteget egy közönséges, bár nem strukturált objektumként használhatjuk. Jegyezzük meg, hogy Köteg({x}) = x.Továbbá, hogy a Köteg(Alma) az összes almából álló összetett objektum, amit az Alma kategóriával összetéveszteni nem szabad.
Az Othello vagy más néven Reversi számítógépes játékként talán népszerűbb, mint táblajátékként. Keresési tere kisebb, mint a sakké, általában 5-15 megengedett lépés van, de a kiértékelő szaktudást a semmiből indulva kellett kifejleszteni. 1997-ben a Logistello program (Buro, 2002) az emberi világbajnokot, Takeshi Murakamit, képes volt hat nullára megverni. Általánosságban elfogadott, hogy az Othellóban az emberek nem ellenfelei a számítógépeknek.
NP(s[1]) ∧ VP(s[2]) ⇒ S(s[1] + s[2]) Fontos Itt s[1] + s[2] két karaktersorozat összefűzését jelenti, így ez a szabály azt mondja, hogy ha van egy s[1] füzér, ami egy NP és egy s[2] füzér, ami egy VP, akkor az összekapcsolásukkal keletkező füzér egy S, ami pontosan megegyezik azzal, ahogy a CFG-szabályt korábban értelmeztük. Fontos észrevennünk, hogy a DCG-k lehetővé teszik számunkra, hogy az elemzésről mint logikai következtetésről beszéljünk. Ez lehetővé teszi, hogy nyelvek és karaktersorozatok felett sok különböző módon következtessünk. Például ez azt jelenti, hogy egy lentről felfelé elemzést előrefelé következtetéssel, egy fentről lefelé elemzést pedig hátrafelé következtetéssel végezhetünk el. Azt is látni fogjuk, hogy ez azt is jelenti, hogy ugyanazt a nyelvtant elemzésre és generálásra is használhatjuk.
Az ágensre mindig úgy gondolunk, mint ami része egy terv végrehajtásának, az élete nagy tervének. Cselekvései tartalmazzák a végrehajtásra készen álló terv lépéseinek végrehajtását, a nyitott előfeltételek kielégítésére vagy a konfliktusok feloldására szolgáló tervfinomítást, a végrehajtás során nyert új információk fényében a terv módosítását. Nyilvánvaló, hogy amikor először új célt formál, az ágensnek nincsenek végrehajtásra kész cselekvései, ezért egy kis időt a részleges terv elkészítésével tölt. Nagyon is lehetséges azonban, hogy az ágens a terv teljes befejezése előtt elkezdi a végrehajtást, különösen ha független részcélokat kell elérni. A folytonos tervkészítő ágens folyamatosan monitorozza a világot, az új megfigyelések alapján módosítja ennek modelljét, még akkor is, ha elképzelései még mindig változnak.
A Rule 30 különösen érdekes sejtautomata, mivel komplex, látszólag véletlenszerű mintázatot állít elő egyszerű, jól definiált szabályokból. Wolfram ebből a modellből kiindulva állapította meg, hogy egyszerű szabályok létrehozhatnak nagy, komplex struktúrákat és viselkedéseket a természetben is. Bizonyítékként szokták emlegetni az egyik tengeri csiga fajt, a Conus textile-t, amelyen a Rule 30 által előállított kimenethez hasonló minta található (lásd az  ábrát).
Bizonytalan relációkat gyakran jellemezhetünk úgynevezett „zajos” logikai relációkkal. A mintapélda erre az úgynevezett zajos-VAGY (noisy-OR) reláció, ami a logikai VAGY reláció általánosítása. Ítéletlogikában kijelenthetjük, hogy a Láz akkor és csak akkor igaz, ha a Megfázás vagy az Influenza vagy a Malária igaz. A zajos-VAGY modell megenged bizonytalanságot, hogy egyes szülők okozhatják-e a gyermekek igaz értékét – az okozati kapcsolat a szülő és gyermek között gátolt lehet, és így lehet, hogy a páciens meg van fázva, de nincs láza. A modell két feltevésre épül. Elsőként feltételezi, hogy az összes lehetséges ok fel van sorolva. (Ez nem annyira szigorú megkötés, mint amilyennek tűnik, mivel mindig létrehozhatunk egy úgynevezett szivárgáscsomópontot (leak node), ami „vegyes okokat” fed le.) Másodikként felteszi, hogy bármely szülő gátlása független a többi szülő gátlásától: például akármi is gátolja, hogy a Malária lázat okozzon, ez független attól, hogy mi gátolja az Influenzá-t, hogy lázat okozzon. Ezekkel a feltevésekkel a Láz akkor és csak akkor hamis, ha az összes igaz értékű szülő gátolt, aminek a valószínűsége a gátlás-valószínűségek szorzata. Tételezzük fel, hogy ezek az önálló gátlási valószínűségek a következők:
Egy egyszerű perceptron nem képes az xor függvény (vagy általánosabban a paritásfüggvény) ábrázolására. Mutassa be, hogy hogyan alakulnak egy négybemenetű, ugrásfüggvényt használó perceptron súlyai, amikor sorban a paritásfüggvényből származó minták érkeznek (kezdetben minden súly 0,1 értékű volt).
Hazánkban hagyományosan az orvosi dokumentáció vezetése évtizedeken át manuálisan vagy írógéppel írt, papír alapú kartonokon, a fekvőbeteg ellátásban kórlapborítókban tárolt kézzel írt leleteken, konzíliumi véleményeken, írógéppel írt zárójelentéseken történt és csak egy évtizede tért át az orvos szakma az informatikai rendszerekbe illesztett szövegszerkesztő használatára. A beszédfelismerés alkalmazása így forradalmi lépés és lehetőség a magyar orvosi dokumentáció fejlődésében.
Jegyezzük meg, hogy a $(1) nem egy egydolláros bankó! Az egydolláros bankóból lehet kettő, de a $(1) nevű objektumból csak egy van. Jegyezzük meg azt is, hogy míg a Hüvelyk(0) és Centiméter(0) ugyanarra a zérushosszra hivatkozik, más zérusmértékekkel, mint például a Másodperc(0) nem azonosak.
Kezdetnek egy üres diagramm jelenik meg illetve a Palette browser (paletta böngésző,  ábra).  ábra - Az Xcos kezdeti állapota, egy üres diagramm és a Palette browser Az Xcos kezdeti állapota, egy üres diagramm és a Palette browser
A mentális objektumok szintaktikai elméletét részletesen először Kaplan és Montague tanulmányozták, akik megmutatták, hogy az elmélet, megfelelő elővigyázatosság hiányában, paradoxonokhoz vezet (Kaplan és Montague, 1960). Mivel ez a hiedelmeket egy fizikai rendszer konfigurációival kapcsolja össze, és ehhez rendelkezik egy természetes modellel egy emberi agy vagy egy számítógép formájában, ez az elmélet az MI területén az utóbbi években igen népszerű volt. Az elméletet korlátos hatékonyságú következtető gépek leírására Konolige és Haas használta, Morgenstern viszont megmutatta, hogyan használható előfeltételek ábrázolására tervkészítés esetén (Konolige, 1982; Haas, 1986; Morgenstern, 1987). A megfigyelési cselekvések tervezésének a  fejezetben található módszerei szintaktikai elméleten alapulnak. A tudás szintaktikai és modális elméleteinek kiváló összehasonlítása található a (Davis, 1990)-ben.
Magyarul, ha van egy csapda az [1, 1], [2, 2], [3, 1] négyzetek egyikében, és ez a csapda nem a [2, 2]-ben van, akkor ez az [1, 1]-ben vagy a [3, 1]-ben van. Hasonlóan a ¬C[1,1 ]literál az Sz[1]-ben rezolvál a C[1,1 ]literállal az Sz[16]-ban, amiből adódik:
Természetesen amennyiben a felsorolt ágensek számottevően más környezetbe kerülnének, úgy programjuk már nem feltétlen biztosítaná korlátozott optimalitásukat. Az is elképzelhető, hogy ahhoz, hogy a megváltozott környezetben érdemben helyt tudjanak állni, még architektúrájukon is változtatni kellene (pl. amennyiben a taxisofőr ágenst éjszakai üzemre is fel szeretnénk készíteni, úgy elképzelhető, hogy egy sötétben látó kamerát is célszerű volna rá felszerelni hasonlóan az alkatrész-válogató robothoz, ha a megvilágítás esetleges kimaradása esetén további folyamatos működésre szeretnénk felkészíteni, vagy ha az orvosi diagnosztikai rendszernek immár látássérült páciensekkel is érintkezésbe kellene lépnie, úgy célszerű volna hangképzésre alkalmas eszközökkel, hangszóróval, hangszintetizátorral is felszerelni).
a. viszont nem következménye a kiinduló mondatnak, hiszen semmi sem biztosítja, hogy az OlyanMagasMint() reláció szimmetrikus, és az egzisztenciális példányosítás sem alkalmazható, hiszen az Everest konstans már szerepel a tudásbázisban.
Olyan neveket használtunk itt a formális rendszer szintjén, amik számunkra ugyanazzal a jelentéssel bírnak, mint amit a formális rendszerbe „be szeretnénk építeni”. Azt fogjuk mondani, hogy ez a használt elnevezések szándékolt interpretációja. (u.u. a használt műveleti szimbólumok szándékolt interpretációja az, amit róluk „szokás” a logikából tudni). Meg kell jegyezni, hogy a használt interpretáció (magyar)nyelvfüggő, de nem egyénfüggő, azaz a tudásbázis mondanivalójával minden magyarul tudó ember tisztában lesz (ha azt súgjuk neki, hogy az interpretáció szándékolt).
A Strukturális tartalmazási algoritmus két AL-fogalom(C és D) közötti tartalmazási viszony fennállását dönti el. Ha C fenékjel, akkor D tartalmazza C-t. Ha nem, akkor meg kell nézni, hogy akkor tartalmazza D C-t, ha C metszet tagjai között van olyan értékkorlátozás, amelyhez tartozik D metszetben is egy értékkorlátozás. Ha ez fennáll, akkor ezekre az értékkorlátozott tagokra rekurzívan vizsgáljuk a tartalmazási viszonyt.[16]
 Feltételezhetnénk, hogy az egyesítés során a változó konfliktus problémáját elkerülhetjük úgy, ha minden mondatnál egyszerre átnevezzük az összes változót a tudásbázisban. Mutassa meg, hogy léteznek olyan mondatok, amelyekre ez a megközelítés nem alkalmazható. (Segítség: tekintsünk egy olyan mondatot, amelynek egyik része egyesíthető a többivel.)
Egy változó Markov-takaróját (Markov blanket) az 14. szakasz - Feltételes függetlenségi relációk Bayes-hálókban definiáltuk. a. Bizonyítsa be, hogy a változó független a háló összes többi változójától, ha Markov-takarója ismert. b. Vezesse le a   egyenletet.
A FSzR hipotézis sokak által vitatott (ld. a könyv  fejezete) folyománya, hogy az intelligenciát univerzális számítógépen meg lehet valósítani (ennek történelmi lépcsői a formális logika megszületése, a Turing gép gondolata és a digitális számítógép megjelenése, majd képességeiben rohamos fejlődése a tárolt program, listakezelés, stb. mérföldkő ötletek nyomán).
Eshetőségi problémák esetén az algoritmusok az e fejezetben található standard keresési algoritmusoknál bonyolultabbak. Ezekkel a  fejezet foglalkozik. Az eshetőségi problémák egy kissé eltérő ágens tervezési sémát is támogatnak, amelyekben az ágens egy garantált terv megtalálása előtt is cselekedhet. Ez hasznos, mert ahelyett, hogy a végrehajtás során előforduló minden eshetőséget előre figyelembe venne, gyakran jobb elkezdeni a végrehajtást, és megnézni, hogy mely eshetőségek következnek be valójában. A járulékos információ megadása után az ágens folytathatja a probléma megoldását. A keresés és a végrehajtás ily módon történő összefésülése (interleaving) a felderítéses problémák (lásd  alfejezet) vagy a játékok ( fejezet) esetén is hasznos.
Bármely időpontban minden ágens pontosan egy cselekvést hajt végre (beleértve a NoOp(Üres) operátort is). Ezen konkurens cselekvések halmazát együttes cselekvésnek (joint action) nevezzük. A tenisz problémában (12. szakasz - Kooperáció: közös célok és tervek részben) például a 〈NoOp(A), Üt(B, Labda)〉 az A és B ágenseknek egy együttes cselekvése. Egy együttes terv együttes cselekvések részben rendezett gráfja. Például a tenisz probléma második terve az együttes cselekvések következő sorozatával írható le:
A lokális keresés témaköre a nagy kényszerkielégítési problémák, mint például az n-királynő (Minton és társai, 1992) és a logikai következtetés (Selman és társai, 1992) területén elért meglepően jó eredményeknek és a véletlenség, a többszörös egyidejű keresés és más javítás beépítésének köszönhetően az utóbbi években megélénkült. Az ilyen algoritmusok reneszánsza, melyeket Christos Papadimitriou „New Age” algoritmusoknak nevez, felkeltette az elméleti számítógép-tudományokkal foglalkozó kutatók érdeklődését is (Koutsoupias és Papadimitriou, 1992; Aldous és Vazirani, 1994). Az operációkutatás területén népszerűségnek örvend a hegymászó keresés egy változata, amit tabukeresési algoritmusnak (tabu search) hívnak (Glover, 1989; Glover és Laguna, 1997). Az emberi rövid távú memóriamodellre alapozva ez az algoritmus karbantartja az előbb meglátogatott k állapot tabulistáját, melyeket újra meglátogatni nem szabad. Ezáltal az algoritmus hatékonysága megnőtt a gráfkeresésben, és képes egyes lokális minimumokból kimenekülni. A hegymászó keresés másik hasznos javítása a Stage algoritmus (Boyan és Moore, 1998). Az ötlet az, hogy használjuk a véletlen újraindítású hegymászó keresés révén nyert lokális maximumokat a tájfelszín általános alakjának felderítésére. Az algoritmus a lokális maximumokra sima felületet illeszt, és a globális maximumot analitikusan számítja ki. Ez lesz az új újraindítási pont. Az algoritmus a gyakorlatban működőképesnek bizonyult nehéz problémák esetén. Gomes (Gomes és társai, 1998) azt mutatta meg, hogy a futási idő eloszlása a szisztematikusan visszalépő algoritmusok esetén sokszor lassan lecsengő eloszlás (heavy-tail distribution), ami azt jelenti, hogy a nagyon hosszú futási idők valószínűsége nagyobb, mint amit a normális eloszlás alapján jósolni lehetne. Ez megadja a véletlen újraindítási mechanizmus elméleti igazolását.
Feltéve, hogy hisz az érme szabályos voltában, a játék várható pénzügyi értéke (VPÉ) (expected monetary value)  , és az eredeti díj, a VPÉ-je természetesen 1 000 000 dollár, ami kisebb. De ez nem jelenti szükségszerűen, hogy a hazárdjáték elfogadása a jobb döntés. Tételezzük fel, hogy S[n][ ]jelöli az n dollárt birtokló állapotot, és a jelenlegi vagyon k dollár. Ekkor a két cselekedetnek, a hazárdjáték elfogadásának vagy visszautasításának a várható hasznossága:
Napjainkban is nagyon kedvelt kutatási téma a sejtautomaták köre, és az elért eredmények fényében ez valószínűleg továbbra is igaz lesz, hiszen nem egy problémát sikerült megoldani az automaták segítségével. Talán a nem is olyan távoli jövőben a sejtautomaták alapvető építő elemei lesznek a mesterséges intelligenciának és tudatnak, vagy csak egyszerűen a segítségével a legapróbb részletekig megismerjük a Világegyetemet.
A fény nélkülözhetetlen a látáshoz: nélküle minden kép egyformán sötét lenne, függetlenül attól, mennyire érdekes a jelenet. A fotometria (photometry) a fény tanulmányozása. Saját céljainkra azt fogjuk modellezni, hogy a jelenet fénye hogyan képződik le az időben a képsík fényintenzitására, amit I(x, y)-nal^[270] jelölünk. A látás rendszere ezt a modellt visszafelé alkalmazza, a képek intenzitásából kiindulva a világ tulajdonságai felé. A  ábra egy asztalon lévő tűzőgép digitalizált képét és a tűzőgép egy 12 × 12 képpontból álló részletét mutatja. Egy számítógépes program, amely értelmezni próbálja a képet, egy ilyen intenzitásmátrixból indulna ki.  ábra - (a) Egy asztalon lévő tűzőgép fényképe. (b) Az (a) egy 12 × 12 képpontból álló, nagyított részlete. (c) A részletnek megfelelő képfényességértékek egy 0-tól 255-ig tartó skálán. (a) Egy asztalon lévő tűzőgép fényképe. (b) Az (a) egy 12 × 12 képpontból álló, nagyított részlete. (c) A részletnek megfelelő képfényességértékek egy 0-tól 255-ig tartó skálán.
Különböző megközelítéseket kell alkalmazni, ha objektum kategóriát kell felismerni – például arcokat, embereket – vagy konkrét objektum példányokat, ugyanis más feltételezések tehetők a két esetben. Például színinformáció, textúraanalízis a példányok esetében sokkal gyakrabban alkalmazható, mivel ezek általában jellemeznek egy példányt, de ritkán állandóak az egész kategóriára nézve.
A  ábrán szereplő És-Vagy-Gráf-Keresés algoritmus csak a gyökér és az aktuális állapot közötti útvonalat ellenőrzi le ismétlődő állapotokra. Mindehhez tegyük fel, hogy az algoritmus eltárolt minden meglátogatott állapotot, és összeveti ezzel a listával (lásd például  ábra Gráf-Keresés). Határozza meg, hogy milyen információt kellene tárolni, és azt, hogy az algoritmus hogyan használja ezt fel, amikor ismétlődő állapotot talál. (Segítség: szüksége lesz arra, hogy különbséget tegyen azon állapotok, melyekhez egy sikeres részterv készült korábban, és azon állapotok között, melyekre nem talált résztervet.) Magyarázza meg, hogyan használhatunk címkéket (labels), hogy elkerüljük a résztervek duplikálását.
Az AC3 algoritmus tehát (a tankönyv  ábrája szerint) sorra halad végig ezeken az éleken, és az élek kiindulópontjában lévő változók értékkészletéből kiveszi azokat az értékeket, amelyekhez az élek végpontjában lévő változók értékkészletében nem tartozik olyan érték, amivel a két változó közti bináris kényszer kielégíthető.
A tudás és a következtetés alapvető szerepet játszanak a részben megfigyelhető környezetek kezelésénél is. A tudásbázisú ágens képes összekombinálni az általános tudást a pillanatnyi érzetekkel, hogy kikövetkeztesse a pillanatnyi állapot rejtett aspektusait, mielőtt cselekvést választ. Ilyen például, amikor egy orvos diagnosztizál egy beteget, azaz kikövetkeztet egy közvetlenül nem megfigyelhető betegségállapotot, mielőtt meghatározná a kezelés módját. A tudás egy része, amelyet az orvos használ, könyvekből vagy tanároktól megtanult szabályok formájában áll rendelkezésre, más része pedig asszociációs minták formájában van, amelyeket lehet, hogy az orvos nem is tud tudatosan leírni. Ha ezek is az orvos fejében vannak, akkor tudásnak számítanak.
A CLP-rendszerek különféle kényszermegoldó algoritmusokat is tartalmaznak a nyelvben megengedett kényszerek kezelésére. Például egy valós értékű változókon értelmezett, lineáris egyenlőtlenségeket megengedő rendszer tartalmazhat egy lineáris programozási algoritmust ezen kényszerek feloldására. A CLP-rendszerek egy sokkal rugalmasabb megközelítést is adoptálnak a standard logikai programozási lekérdezések megoldására. Például a mélységi, balról jobbra történő visszalépéses keresés helyett használhatják bármelyik ennél hatékonyabb, az  fejezetben tárgyalt algoritmust, beleértve a heurisztikus konjunktok sorba rendezését, a visszaugrást, a vágási halmaz kondicionálást és így tovább. A CLP-rendszerek tehát vegyesen alkalmazzák a kényszerekkielégítési algoritmusok, a logikai programozás és a deduktív adatbázisok elemeit.
Az elméleti számítástudomány kutatói régóta érdeklődtek az iránt, hogy számítási cselekvések szekvenciájaként értelmezett programok tulajdonságait hogyan lehetne formalizálni. A modális logikát számítógépes programokról való következtetésre Burstall vezette be (Burstall, 1974). Rövidesen utána Vaughan Pratt a dinamikus logikát (dynamic logic) dolgozta ki (Pratt, 1976), amelyben a modális operátorok programok vagy más cselekvések hatását jelzik (lásd még Harel, 1984). Így például ha dinamikus logikában α egy program neve, akkor „[α] p” azt jelenti, hogy „p igaz lesz a világ minden olyan állapotában, amely az α programnak a jelenlegi állapotból való indításából származik”. Az „〈α〉p” pedig azt jelenti, hogy „p igaz lesz a világ legalább egy olyan állapotában, amely az α program mostani állapotából való indításából származik”. Programok konkrét elemzésére a dinamikus logikát Fischer és Ladner használta (Fischer és Ladner, 1977). Pnueli programokról való következtetésre klasszikus temporális logikát javasolt (Pnueli, 1977).
E leírásból látszik, hogy a tankönyv 5.8-as ábráján szereplő pszeudokód alapján nem feltétlen teljesen egyértelmű, hogy mit értünk konfliktust eredményező változón; hogy pontosan hogyan is adunk ennek újabb értéket; illetve hogy az algoritmus lépésszámába vajon beleszámít-e a kiindulási behelyettesítés véletlenszerű előállítása, vagy nem (szerintünk nem, mivel az előző szakaszokban bemutatott módszerek sem feltétlen kellett volna, hogy üres behelyettesítéssel induljanak (pl. egy következő megoldás keresésénél), és ekkor ennek a kiindulási behelyettesítésnek az előállítását nem számítjuk bele az algoritmus lépésszámába - inicializációs lépés).
További osztályozást John. R. Searle vezetett be a gyenge és az erős MI definíciójával. Az erős MI szerint az intelligencia algoritmusok segítségével kialakítható, önálló gondolkodásra képes, az intelligencia mértéke az algoritmus bonyolultságától függ. Algoritmusok segítségével előállíthatóak az érzelmek, az értelem, a gondolkodás folyamata. A gyenge MI nem rendel agyat a rendszerhez, elegendő, ha úgy cselekszenek, mintha intelligensek lennének.
A fókusz távolság a képsík és a lencse távolsága megközelítőleg. Ez a feltételezés akkor jogos, ha az objektumok távol vannak a kamerától. mivel a kérdés nem a pontok y koordinátáira vonatkozik, így mindkét kamera egy 512 pixel hosszú vonalat láthat. a. Hasonló háromszögek felhasználásával a fél méteres és 16 méteres oldalakból: ez a háromszög hasonló a 16 cm oldalhosszú háromszöggel a képsíkon, így a magassága 0,5 cm. Ez a távolság a kép közepétől. A másik kamerán az eltolás szintén 0,5 cm. A teljes diszparitás tehát 1 cm, amely 512 pixel/10 cm, azaz 51,2 pixel, kerekítve 51 pixel. A közelebbi objektumoknak szintén hasonló diszparitása lesz. Ha ezt d diszparitással jelöljük, és Z az objektum távolsága:
Kövessük végig a Graphplan működését a  alfejezet kerékcsere problémáján! A teljes gráf a  ábrán látható. A Graphplan első lépésben inicializálja a tervkészítési gráfot egy egyszintű (S[0] szint) gráfra, ami a kiindulási állapot öt literálját tartalmazza. Az Ott(Pótkerék, Tengely) célliterál nincs jelen az S[0]-ban, ezért nem kell meghívnunk a Megoldás-Kinyerést, hiszen biztosak vagyunk benne, hogy még nincs megoldás. Ehelyett a Gráf-Bővítés hozzáad három cselekvést, melyeknek az előfeltétele már teljesül az S[0] szinten (például az összes cselekvés, kivéve a Felszerel(Pótkerék, Tengely) – cselekvést). Hozzáadja továbbá a megőrző-cselekvéseket S[0] összes literáljához. A cselekvések következményeit az S[1] szinthez adjuk hozzá. A Gráf-Bővítés ezek után mutex kapcsolatokat keres, és hozzáadja ezeket a gráfhoz.  ábra - A kerékcsere probléma tervkészítési gráfja az S[2] szintre bővítés után. A mutex kapcsolatokat szürke vonalak jelölik. Csak néhány fontos mutexet mutatunk, mert az ábra olvashatatlanná válna, ha az összeset jelölnénk. A megoldást megvastagított vonalak és kiemelések jelölik. A kerékcsere probléma tervkészítési gráfja az S2 szintre bővítés után. A mutex kapcsolatokat szürke vonalak jelölik. Csak néhány fontos mutexet mutatunk, mert az ábra olvashatatlanná válna, ha az összeset jelölnénk. A megoldást megvastagított vonalak és kiemelések jelölik.
A kórházi POC irányítási csoport döntése hatékonynak bizonyul: * A klinikai igény bővül, a rendszer folyamatosan ki lesz terjesztve a többi kórházi osztályra is. * A laboratórium folyamatosan ellenőrzés alatt tudja tartani a megfelelősséget. * A betegellátás eredményessége az egységes POC rendszer bevezetésével szervezési és gazdasági téren is hatékonyabb a korábbi rendszernél * A helyben végzett vizsgálatok eredményeit jóval rövidebb időintervallum után már tudja értékelni a klinikus.
A 3D-s estben, ha egy tetszőleges objektum 2D-s levetítését tekintjük,akkor az nagyon változó lehet, az objektum helyzetének függvényében. De sima 2D objektum esetén, a kép mindig „ugyanaz”, kivéve az orientációja és a helyzete. Minden jellemző pont megjelenik, mert nincs takarás, így m = n. Feltételezve, hogy a kép és a modellpontok súlypontját számoljuk ki, ezt offline módon is megtehetjük O(n). Ezután ezt megtehetjük két súlypontra, amely tetszőleges képpont és egyike az n modellpontnak, ami belátható. Így a teljes folyamat O(n^2log n).
Minden egyes kategória …-tal végződik annak jelzésére, hogy vannak más szavak is a kategóriában. Azt is meg kell jegyeznünk azonban, hogy két különálló ok van a szavak hiányára. A főnevek, az igék, a jelzők és a névmások esetében elméletileg sem lehetséges az összes felsorolása. Nemcsak azért, mert minden osztálynak ezernyi vagy tízezernyi tagja lehet, hanem azért is, mivel folyamatosan hozzáadódnak újak, mint például az MP3 vagy az anime. Ezt a négy kategóriát nyitott osztályoknak (open classes) nevezzük. A többi kategóriát (névmások, névelők, elöljáró- és kötőszók) lezárt osztályoknak (closed classes) hívjuk. Mindegyikben kevés számú szó van (néhánytól pár tucatig), amelyek elméletileg teljesen felsorolhatók. A lezárt osztályok évszázadok folyamán változnak, nem hónapok alatt. Például a „thee” és a „thou” a 17. században elterjedt névmások voltak, hanyatlóban voltak a 19. században, és manapság csak versekben és egyes régiók dialektusában lehet találkozni velük.  ábra - ℰ[0] szókincse ℰ0 szókincse
A MATLAB alapcsomagnak is van programozási nyelve, amely a matematikai műveletek, függvények, relációk értelmezéséből adódik. Parancsvezérelt üzemmódban, vagyis interpreter-ként dolgozik. A parancsokat áttekinthetjük különböző csoportosítások szerint a megnyitott MATLAB csomag Help menüjében.
Az RVM-ek kifejezési erejének megnövelésére számos módszer kínálkozik. Megengedhetünk rekurzív függéseket (recursive dependencies) a változók között, hogy bizonyosfajta visszatérő kapcsolatokat kezelni tudjunk. Például tegyük fel, hogy a gyorséttermi ételektől való függőséget egy McGén okozza. Ekkor minden x-re McGén(x) függ a McGén(Apa(x)) és McGén(Anya(x))-től, amelyek viszont függnek a McGén(Apa(Apa(x))) és a McGén(Anya(Apa(x)))-től és így tovább. Bár az ilyen tudásbázisok végtelen változót tartalmazó Bayes-hálókhoz tartoznak, fixpont-egyenletekből olykor megoldásokhoz juthatunk. Például a kiszámítható a McGén egyensúlyi eloszlása, az öröklődés adott feltételes valószínűsége mellett. A rekurzív tudásbázisok egy másik igen fontos családja a  fejezetben leírt időbeli valószínűségi modelleket (temporal probability models) tartalmazza. Ezekben a modellekben a t időpillanatbeli állapot tulajdonságai a t – 1 időpillanatbeli állapot tulajdonságaitól függnek és így tovább.
A helymeghatározási feladat három, egyre nehezebb problémaként jeletkezik. Ha a tárgy kezdeti pozíciója és iránya (orientációja) ismert, akkor a helymeghatározás tulajdonképpen követési feladatot (tracking) jelent, és ez korlátos bizonytalansággal jellemezhető. Ennél nehezebb a globális helymeghatározás (global localization), amikor is a kezdeti pozíció egyáltalán nem ismert. A globális helymeghatározás követési problémává egyszerűsödik, ha sikerül lokalizálni a kívánt tárgyat, de itt is előfordulhatnak olyan esetek, amikor a robotnak nagyon nagy bizonytalanságokkal kell megbirkóznia. Végezetül lehetünk gonoszak is a robotunkkal, ha elvesszük, „elraboljuk” előle az éppen lokalizálni próbált tárgyat. Ezt elrablásos problémának (kidnapping problem) hívják, és gyakran tesztelik vele a robot lokalizációs algoritmusának robusztusságát extrém körülmények között.
Az  ábra Visszalépéses-Keresés algoritmusa egy elég egyszerű intézkedést alkalmaz akkor, amikor egy keresési ág meghiúsul: visszalép az előző változóra, és megpróbál számára egy másik értéket találni. Ezt időrendi visszalépésnek (chronological backtracking) nevezik, mert a legutolsó döntési pontot keresi fel újra. Ebben az alrészben látni fogjuk, hogy adódik erre sokkal jobb módszer is.
Először néhány elnevezés: a filozófiában gyenge MI-hipotézisnek (weak AI) nevezik azt az állítást, miszerint a gépek valószínűleg képesek intelligensen cselekedni (vagy jobban mondva, képesek úgy cselekedni, mintha intelligensek lennének), míg azt az állítást, hogy a gépek valóban intelligensen cselekszenek, erős MI-hipotézisnek (strong AI) hívják.
Észrevehetjük, hogy az aszimptotikus korlátozott optimalitás lényegében úgy viszonyul a korlátozott optimalitáshoz, mint ahogyan a számítható racionalitás viszonyul a tökéletes racionalitáshoz. Sőt, az előbbi sakkos példa esetében az említett aszimptotikusan korlátozott optimális ágens-programunk valójában egyben számítható racionális is, illetve az ott korlátozottan optimális ágens-program egyben tökéletesen racionális is lenne. Mindez természetesen a sakk (mint viszonylag egyszerű, nem túl valószerű) környezet esetében igaz, ám komplexebb környezetek esetén ezek a fogalmak már nem feltétlen esnének egybe.
Lehetséges világok. Megvizsgálva az elsőrendű logikában alkalmazott kijelentéseket, viszonylag természeten adódik, hogy egy adott mondathoz (pl. pred(X)) rendelt valószínűséget a mondat igazságába vetett hitünk mértékeként kezeljünk. Ebben az ún. lehetséges világok szemantikában úgy tekinthetjük, mintha adott lenne a tárgyterület lehetséges modelljeinek (világoknak) egy halmaza, a felette értelmezett eloszlással, és az adott kijelentés minden egyes világban a hagyományos értelemben véve igaz vagy hamis. A kijelentés valószínűsége ezek után az azt támogató világok összesített valószínűségeként adódik [Nilson 1986].
[4] Birta Zsolt: Arisztotelész kategória elmélete és szubsztancia felfogása, Jegyzet kivonat, letöltés dátuma: 2010. december 8., http://szabadbolcsek.uw.hu/documentumok/filotori/okor/filtor-okor-arisz totelesz-kategoriaelmelet-es-szubsztancia-felfogas-by-bzs.doc
Segítségével mintegy 5 000, túlnyomórészt angol nyelvű indexelt folyóiratban kereshetünk (1950-ig visszamenően). Az adatbázist ma már több szolgáltatótól, és felhasználói felületről elérhetjük. Bibliográfiai adatok találhatók a következő forrásokból: Index Medicus, International Nursing Index, Index to Dental Literature, PREMEDLINE®, AIDSLINE®, BIOETHICSLINE®, HealthSTAR.
Komplexitás: Be lett bizonyítva, hogy a Hex PSPACE-teljes. A PSAPCE egy jelenleg ismert [8] terjedelmes probléma kör (természetesen vannak még nála nagyobbak is), mely magába foglalja az NP és így a P problémákat is. A sakk például EXPTIME-teljes, ami magába foglalja a PSPACE-t is. Ennek ellenére a Hex bonyolultsága így is erőteljesen megmutatkozik.
Eddig a szélességi keresésnek csak a jó tulajdonságait láttuk. Ahhoz, hogy megértsük miért nem mindig ezt a stratégiát választjuk, meg kell vizsgálnunk a keresés végrehajtásához szükséges idő és memória mennyiségét. Ehhez egy olyan hipotetikus állapotteret veszünk alapul, amelyben minden egyes állapotot kifejtve b új állapot keletkezik. A keresési fa gyökércsomópontja b csomópontot generál az első szinten, amelyek mindegyike újabb b csomópontot, összesen b^2 csomópontot generál a második szinten. Ezek mindegyike újabb b csomópontot generál, összesen b^3 csomópontot a harmadik szinten és így tovább. Tételezzük fel, hogy ezen probléma megoldása d mélységben található. Ekkor a legrosszabb esetben a d-edik szinten az utolsót kivéve (mert a célt magát nem fejtjük ki) a csomópontok mindegyikét ki kell fejtenünk, a (d + 1)-edik szinten b^d^+1 – b csomópontot generálva. A generált csomópontok össz-száma így:
A tudásbázis most Sz[1]-től Sz[5]-ig tartalmaz mondatokat. Tekinthetjük ezt úgy is, mint egyetlen mondatot – az Sz[1 ]∧ Sz[2 ]∧ Sz[3 ]∧ Sz[4 ]∧ Sz[5 ]konjunkciót –, mivel ez azt is kijelenti egyben, hogy minden egyes mondat is igaz.
A  feladat annak megmutatását kéri, hogy az első megoldás – a folyamat rendjének a megnövelése – mindig átfogalmazható az állapotváltozók halmazának megnövelésére, változatlanul hagyva a rendet. Vegyük észre, hogy állapotváltozók hozzáadása javíthatja a rendszer előrejelző erejét, de megnöveli a predikciós követelményeket is: ekkor már az új változókat is jósolni kell. Így a változóknak egy „önmagában elégséges” halmazát keressük, ami valójában azt jelenti, hogy meg kell értenünk a modellezett folyamat „fizikáját”. A folyamat pontos modellezése iránti követelmény nyilvánvalóan mérsékeltebb, ha új érzékelőket vehetünk fel (például a hőmérséklet és a nyomás mérésére), amelyek közvetlenül az új állapotváltozókról szolgáltatnak információt.
Az egyenlet azt fejezi ki, hogy az X állapotváltozó feletti a posteriori eloszlást a t + 1 időpillanatbani rekurzívan számítjuk az egy időlépéssel korábbi megfelelő becslésből. A számításokhoz felhasználjuk a korábbi cselekvést, a[t]-t, és az aktuális szenzoros megfigyelést, z[t+1]-et. Például ha célunk egy futballozó robot fejlesztése, akkor X[t+1] lehet a labda relatív helyzete a robothoz képest. A posteriori P(X[t]|z[1:t], a[1:t–1]) mindazon állapotok felett értelmezett valószínűség-eloszlás, amelyek megőrzik mindazt, amit a korábbi érzékelő mérésekből és irányításokból tudunk. A   egyenlet megmondja, hogyan becsüljük rekurzívan ezt a pozíciót, folyamatosan felhasználva az újabb szenzoradatokat (például kameraképek) és a robot mozgásparancsait. A P(X[t+1]|x[t], a[t]) valószínűséget állapotátmenet-modellnek (transition model) vagy más néven mozgásmodellnek (motion model) hívjuk, míg P(z[t+1]|x[t+1]) az érzékelő modell (sensor model).
Ha egy bábu elér a túloldalra, onnantól kezdve megkülönbözetett szerepet kap és a dáma nevet viseli. A dáma átlósan akármilyen irányba léphet, akár visszafele is. A dámajátékot a sakkal ellentétben a fekete szín nyitja [1].
A ∼ B ⇒ [p, A; 1 – p, C] ∼ [p, B; 1 – p, C] * Monotonitás (monotonicity). Tételezzük fel, hogy két szerencsejátéknak ugyanaz a két kimenetele van, A és B. Ha az ágens A-t preferálja B-vel szemben, akkor az ágensnek azt a szerencsejátékot kell preferálnia, ami nagyobb valószínűséggel eredményezi A-t (és fordítva is).
Ezek a valószínűségek a Bayes-hálókra kifejlesztett tetszőleges következtetési algoritmussal számíthatók. Naiv Bayes-hálókra, mint amilyen példánkban is szerepel, a következtetés „kézzel” elvégezhető, a Bayes-szabály és a feltételes függetlenség felhasználásával:
1995-ben alakult meg a Magyar Aneszteziológiai és Intenzív Terápiás Társaság Informatikai Szekciója. A kezdeti időszak munkájának egyik legfontosabb eredménye az volt, hogy felépített egy dedikált rendszert Anesztinfo néven. Ez a Docinfo szisztéma szerkezetét követte azzal a különbséggel, hogy a Docinfo a családorvosok számára teremtett kapcsolatot egy központi tudásbázissal, az Anesztinfo pedig a kórházak Aneszteziológiai és Intenzív terápiás Osztályai részére hozott létre információs központot. Az akkori lehetőségek a BBS, vagyis telefon –behívásos rendszer használatát tették lehetővé.
(Az f[M] azt jelöli, hogy f létrehozásában M-et felhasználtuk.) * Hasonlóan, tároljuk a J-hez tartozó tényezőt egy kételemű f[J](A) vektorban. * Az A-hoz tartozó tényező, P(a∣B, e), ami egy 2 × 2 × 2-es mátrix lesz, f[A](A, B, E). * Most ki kell összegeznünk A-t ennek a három tényezőnek a szorzatából. Ez egy 2 × 2-es mátrixot eredményez, aminek indexei már csak B és E felett futnak. A mátrix nevében A-nál egy felülvonással jelezzük, hogy A már ki lett összegezve:
Próbáljuk ki kisebb társaságban (ahhoz, hogy érezzük a játékban rejlő motiváló tényezőket, és kényszerítő erőket)! 1 dollár helyett nyugodtan használhatunk akár 100 Ft-os érmét, amit első körben 1 Ft-ért kiálthatunk ki. ...vajon hogyan alakul egy-egy lejátszás?
Mivel egy magas szintű cselekvésnek, mint amilyen a HázatÉpít, számos lehetséges dekompozíciója lehet, elkerülhetetlen, hogy a Strips cselekvésleírása elrejtse ezen dekompozíciók néhány előfeltételét vagy következményét. A magas szintű cselekvés előfeltételeit a dekompozícióiban szereplő külső előfeltételek metszete, míg a következményit a dekompozíciók külső következményeinek metszete adja. Másképpen, a magas szintű előfeltételek és következmények garantáltan részhalmazai minden egyszerű implementáció valós előfeltételeinek és következményeinek.  ábra - A házépítési probléma cselekvéseinek leírása és a HázatÉpít cselekvés részletes dekompozíciója. A leírások a pénzzel kapcsolatban egy egyszerűsített, az építőkkel kapcsolatban egy optimista nézetet alkalmaznak. A házépítési probléma cselekvéseinek leírása és a HázatÉpít cselekvés részletes dekompozíciója. A leírások a pénzzel kapcsolatban egy egyszerűsített, az építőkkel kapcsolatban egy optimista nézetet alkalmaznak.
A mélységi keresés (depth-first search) mindig a keresési fa aktuális peremében a legmélyebben fekvő csomópontot fejti ki. A keresés lefolyását a  ábra illusztrálja. A keresés azonnal a fa legmélyebb szintjére jut el, ahol a csomópontoknak már nincsenek követőik. Kifejtésüket követően kikerülnek a peremből és a keresés „visszalép” ahhoz a következő legmélyebben fekvő csomóponthoz, amelynek vannak még ki nem fejtett követői.
Az, hogy ez mennyire automatizálható, a keretaxiómák pontos formátumán múlik. Hogy az ilyen axiómákra alapozva hatékony következtetési eljárást dolgozhassunk ki, három dolgot kell még megtenni: 1. Indexeljük a PozHatás és a NegHatás predikátumokat az első argumentumaik szerint, hogy amikor adott egy t időpontban bekövetkező cselekvés, a hatásait O(1) időben meg lehessen keresni. 2. Indexeljük az axiómákat úgy, hogy amikor világos már, hogy egy cselekvés hatása F[i], O(1) idő alatt ki lehessen keresni az F[i]-re vonatkozó axiómákat. Ekkor a cselekvés hatásai között nem szereplő folyó esemény axiómáival nem is kell törődni. 3. Minden szituációt reprezentáljunk egy megelőző szituáció plusz egy növekményként. Így, amikor lépésről lépésre nem változik semmi, nem kell semmiféle munkát végeznünk. A régebbi megközelítésben O(F) munka ment volna rá, hogy minden folyó esemény esetén a megelőző F[i](s) feltételezésből F[i](Eredmény(a, s)) feltételezést generáljunk.
Talán első pillantásra meglepő, hogy egy Bayes-i Nash-egyensúly egyszerre több cellát is kijelöl a fenti táblázatban, de gondoljunk arra, hogy itt végső soron egy stratégia-profil kombinációval van dolgunk. Esetünkben az f* BNE az 1-es játékos számára olyan f1* stratégia-profilt ír elő, amely az egyetlen t11 típusához a Fel stratégiát rendeli, azaz az (1,0) kevert stratégiát, amely szerint 1 valószínűséggel játssza a Fel-t, és 0 valószínűséggel a Le-t.
(a) A cselekvés időtartama nem feltétlenül meghatározott egy adott cselekvéshez. Feltételes következmények esetén a különböző következményekhez tartozhat különböző időtartam. Például a motor beszerelése elvégezhető akkor is, ha a csavarok feje el van kopva, de így a művelet lényegesen tovább tarthat.
Még ha a szülők maximális száma, k meglehetősen kicsi is, egy csomópont feltételes valószínűségi táblájának kitöltése akár O(2^k) számú értéket és az összes lehetséges feltételes esetet figyelembe véve is nagy szakértelmet igényelhet. Valójában azonban az a legrosszabb eset, amikor a kapcsolat a szülők és a gyermek között teljesen önkényes. Általában az ilyen kapcsolatok egy kanonikus eloszlással (canonical distribution) írhatók le, amelyek valamilyen szabványos mintát követnek. Ilyen esetekben a teljes tábla megadható a mintázat és esetleg néhány paraméter meghatározásával – sokkal könnyebben, mint exponenciális számú paraméter megadásával.
A lokális minimumokból kilépni nyílván igen egyszerű: vegyük a legkevésbé rossz lehetséges lépést. A probléma abból adódik, hogy ha ezt a lépést nem jegyezzük meg, legközelebb ugyanúgy visszalépünk a lokális minimumba. Ezt igyekszik elkerülni a TRTA* algoritmus. Amikor az ágens először érkezik meg egy adott pontba, még a h() heurisztikát használja, de ezután bevezet egy tapasztalati heurisztikát, H()-t, amit később módosítani tud. Ha épp elhagy egy lokális minimumot, növeli annak H() értékét (kilapítja a síkot). Így ha elégszer próbálkozik ugyanazzal a lokális minimummal (zsákutcával), a H() érték elég nagyra nő, hogy ne próbálkozzon arra többet (megtanulja elkerülni a zsákutcát).
A logikai következtetések komplexitásáról szerzett tudásunk legnagyobb része a deduktív adatbázisokkal foglalkozó tudományos közösségből származik. Chandra és Merlin (Chandra és Merlin, 1977) mutatták ki először, hogy egy egyszerű nem rekurzív szabály (vagyis egy konjunktív lekérdezés (conjunctive query) az adatbázisok terminológiájában) illesztése NP-nehéz lehet. Kuper és Vardi (Kuper és Vardi, 1993) javasolták az adatkomplexitás (data complexity) használatát – annak a mértéknek a használatát, ami a komplexitást, mint az adatbázis méretének egy függvényét méri, miközben a szabály méretét konstansnak tekinti – a lekérdezések megválaszolásának mértékére. Gottlob és társai (Gottlob és társai, 1999b) a konjuktív lekérdezések és a kényszerkielégítés közötti kapcsolatot vizsgálták, megmutatva, hogy a hiper-fák lebontása hogyan optimizálja az illesztési folyamatot.
Ezen okokból a legtöbb tanulással foglalkozó eddigi kutatás a viszonylag egyszerű hipotézisreprezentációkra helyezte a fő hangsúlyt. Ebben a fejezetben az ítéletlogikára és az ehhez kapcsolódó nyelvekre koncentrálunk. Az elsőrendű logikai reprezentációt használó tanulás elméletével a  fejezet foglalkozik. Látni fogjuk, hogy a kifejezőképesség-komplexitás kompromisszum nem is olyan egyszerű, mint először gondolnánk. Gyakran az a helyzet – mint azt a  fejezetben is láttuk –, hogy egy kifejezőnyelv lehetővé teszi az adatokra illesztett egyszerű elméletet, míg a nyelv kifejezőképességének korlátozása azt is jelenti, hogy bármely konzisztens elmélet szükségszerűen bonyolult lesz. A sakk szabályait például egy-két oldalon le tudjuk írni elsőrendű logika segítségével, míg az ítéletlogikával történő leírás több ezer oldalt igényelne. Ilyen esetekben kell arra lehetőségnek lennie, hogy a kifejezőbb nyelven sokkal gyorsabb legyen a tanulás.
Tehát egy számpéldával: N beteget észleltünk, van 17 hipotézisünk, hogy mi okozhatta a betegséget. (Pl. az egyik hipotézisünk, hogy a betegséget a vacak fogyasztása okozza. A másik hipotézisünk, hogy a helytelen vakarózási szokások okozzák a bajt stb. stb.) Mekkora kell legyen N, hogy legfeljebb δ  =0,01 (1%) valószínűsége legyen annak, hogy valamelyik rossz hipotézisünk (vagy akár több rossz hipotézisünk is) fennáll az N minta mindegyikére? (Tehát pl. mind az N betegünk fogyasztott vacak-ot, vagy mind az N betegünk helytelenül vakarózott, pedig ezeknek semmi vagy kevés köze van a betegséghez.) Meg kell mondjuk, hogy mit értünk rossz elmélet alatt! Itt most rossz hipotézis alatt értsük azt, hogy a jövőben ki fog kiderülni, hogy az új megfigyelt betegeinknek pl. legalább ε  =0,1 részére (10%-ára) nem teljesül a hipotézis. A könyvben erre adott becslés:
ahol az Előre a   egyenlet által leírt frissítést hajtja végre. Fontos Ha az összes állapotváltozó diszkrét, minden frissítés ideje állandó (azaz t-től független), és a tárigény is állandó. (Ezek az állandók természetesen függnek az állapottér méretétől és a tárgyalt időbeli modell konkrét típusától.) A frissítés idő- és tárigényének állandónak kell lennie, ha egy korlátos memóriájú ágensnek követnie kell az aktuális állapot eloszlását a megfigyelések egy korlátlan sorozata esetén.
Akkor melyek a nehéz problémák? Feltételezhető, hogyha növeljük a klózok számát, miközben a szimbólumok számát rögzítve tartjuk, a problémát erősebben határozottá tesszük, és a megoldás megtalálása egyre nehezebbé válik. Legyen m a klózok száma és n a szimbólumok száma. A  (a) ábra mutatja annak a valószínűségét, hogy egy véletlenszerűen választott 3-CNF mondat kielégíthető-e a klóz/szimbólum arány (m/n) függvényében rögzített n = 50 mellett. Ahogy vártuk, kis m/n aránynál a valószínűség közel van 1-hez, és nagy m/n aránynál közel van a 0-hoz. A valószínűség viszonylag élesen esik le az m/n = 4,3 értéknél. Azok a CNF mondatok, amelyek közel vannak ehhez a kritikus ponthoz (critical point), „alig kielégíthetőnek” vagy „alig kielégíthetetlennek” jellemezhetők. Ez volna az, ahol a nehéz problémák vannak?
Referencia: C.F. Aliferis, I. Tsamardinos, A. Statnikov. "Causal Explorer: A Probabilistic Network Learning Toolkit for Biomedical Discovery." The 2003 Int. Conf. on Mathematics and Engineering Techniques in Medicine and Biological Sciences (METMBS '03), June 23-26, 2003.
Példánkban feltételezzük, hogy a tervezőnek pontos képe van a környezetről, de korlátos hiba is bevezethető a modellbe a következők szerint: amennyiben a hibák paraméterekkel leírhatók, úgy azok a paraméterek szabadságfokok formájában hozzáadhatók a konfigurációs térhez.  ábra - Az első mozgásparancs és a lehetséges robotmozgások burkolója. A hibától függetlenül tudjuk, hogy a végső konfiguráció a lyuk bal oldalán lesz. Az első mozgásparancs és a lehetséges robotmozgások burkolója. A hibától függetlenül tudjuk, hogy a végső konfiguráció a lyuk bal oldalán lesz.  ábra - A második mozgásparancs és a lehetséges mozgások. Még hibák esetén is végül bele fogunk találni a lyukba. A második mozgásparancs és a lehetséges mozgások. Még hibák esetén is végül bele fogunk találni a lyukba.
A potenciáltereket mint újabb költségfüggvényeket vezettük be a robotmozgás-tervezés során, de alkalmasak arra is, hogy közvetlenül generálják a robot mozgását, kiegészítve a pályatervezési fázist. Ahhoz, hogy ezt elérjük, definiálnunk kell egy vonzó erőt, ami a robotot a célkonfiguráció felé vonzza, valamint egy taszító potenciálteret, amely távol tartja az akadályoktól. Egy ilyen teret már bemutattunk a  ábrán. Az egyetlen globális minimumhelye a célkonfiguráció, értéke pedig az e céltól való távolság, valamint az akadályok közelségének összege. Az ábrán bemutatott potenciáltér kialakítását nem előzte meg tervezés. Éppen ezért a potenciálterek jól alkalmazhatók valós idejű megoldásokhoz. A  ábrán két trajektória látható, ahogy egy robot két különböző kezdeti konfigurációból megpróbálja megmászni a potenciálteret. Nagyon sok alkalmazás esetén a potenciálteret hatékonyan ki lehet számítani bármilyen adott konfiguráció esetében. Sőt adott robotkonfiguráció esetén a potenciálok kialakulásával a potenciálgradienst is meghatározhatjuk. Ezek a számítások általában különösen hatékonyak, főleg a pályatervező algoritmusokhoz hasonlítva, amelyek mind exponenciálisak a konfigurációs tér dimenziójában (a szabadságfokokban).  ábra - Potenciáltér-vezérlés. A robot egy potenciáltérben halad, amely az akadályoktól taszító erőkből és a célkonfigurációhoz vonzókból áll. (a) Sikeres útvonal. (b) Lokális optimum. Potenciáltér-vezérlés. A robot egy potenciáltérben halad, amely az akadályoktól taszító erőkből és a célkonfigurációhoz vonzókból áll. (a) Sikeres útvonal. (b) Lokális optimum.
Az FMT-k felügyelt mozgások (guarded motion) sorából tevődnek össze. Minden egyes felügyelt mozdulat (1) egy mozgásparancsból és egy (2) leállási feltételből áll, ami a robot szenzorainak adatain alapul. A folyamat igaz értékkel tér vissza, ha sikeresen befejeződött a felügyelt mozgás. A mozgásutasítások jellemzően engedékeny, azaz önbeálló mozgások (compliant motion), amelyek lehetővé teszik, hogy a robot kitérjen, ha az utasítás végrehajtása ütközést eredményezne. A  ábrán látható példa egy kétdimenziós konfigurációs teret mutat, középen egy szűk, függőleges lyukkal. Ez egy olyan feladat konfigurációs tere lehetne, amikor egy szögletes csapot kell beilleszteni egy nála alig szélesebb lyukba. A mozgásutasítások konstans sebességeket írnak elő. A leállási feltétel a felülettel történő érintkezés. A vezérlés bizonytalanságának modellezésénél feltételezzük, hogy a robot nem pontosan az utasításban megadott irányba mozdul el, hanem a körül egy C[v] kúpon belül, tetszőlegesen. A  ábra azt mutatja, mi történne, ha adott sebességgel elindítanánk a műveletet az s kezdőpontból. A mozgás bizonytalansága miatt a robot a szürke területen belül bárhova megérkezhet a felülethez. Van rá esély, hogy beletalál a lyukba, de nagyobb valószínűséggel valamelyik oldalán áll meg. Mivel a robot nem fogja tudni, hogy a lyuk melyik oldalán van, ezért azt sem fogja tudni, hogy merre mozduljon.  ábra - Egy kétdimenziós környezet, a sebesség vektor bizonytalanságát ábrázoló kúppal. A vezérlés bizonytalansága miatt az eredeti szándéknak megfelelő v sebesség valójában bármilyen vektor lehet a C[v] kúpon belül. Ennek eredményeképpen a robot végső konfigurációja bárhol lehet a burkolón (a világosabban árnyalt területen), azaz nem tudjuk, hogy eljutottunk-e a lyukba, vagy sem. Egy kétdimenziós környezet, a sebesség vektor bizonytalanságát ábrázoló kúppal. A vezérlés bizonytalansága miatt az eredeti szándéknak megfelelő v sebesség valójában bármilyen vektor lehet a Cv kúpon belül. Ennek eredményeképpen a robot végső konfigurációja bárhol lehet a burkolón (a világosabban árnyalt területen), azaz nem tudjuk, hogy eljutottunk-e a lyukba, vagy sem.
Ha a legbaloldalibb kérdéses négyzeten akna van, akkor a mellette lévőn biztosan nincs, mert különben a pirossal jelölt 1-esnek túl sok akna szomszédja lenne. Hasonlóan, ha nincs ott akna, akkor a szomszédján kell lennie. Azaz a két mező „akna-állapota” épp ellentétes: ha az egyiken akna van, akkor a másikon nincs, és fordítva. Ezt úgy jelöljük, hogy az egyik mezőbe „A”-t, a másikba „a”-t írunk. Ezt a jelölést a továbbiakban is használni fogjuk: a kis- és nagybetűk mindig ellentétes állapotú mezőket jelölnek, a különböző betűk pedig független mezőket. Az ábrát tovább töltve láthatjuk, hogy a vezeték két végén tényleg ugyanolyan állapotú mezők vannak.
A CAM-6 tovább fejlesztett, kompatibilis változata a CAM-PC. 16 bites keresési táblát (lookup table) használt, és támogatta a komplex programozható futási ciklusokat, így a felhasználó többlépéses sejtautomata szabályokkal is dolgozhatott [HHG].
alakú konjunktból áll. Természetesen e teljes halmaz részhalmazából is tudnánk tanulni.  ábra - Az (a) és (b) ábra a fehérjemolekula összehajtogatásának tárgyterületén a „négy-helikális fel-és-le köteg” fogalom pozitív és negatív példáját mutatja. Mindkét példa struktúráját egy olyan kb. 100 konjunktív tagot tartalmazó logikai kifejezésbe kódolták, mint amilyen például a TeljesHossz(D2mhr, 118) ∧ HelikálisSzám(D2mhr, 6) ∧ … kifejezés. Az (a) és (b) ábra a fehérjemolekula összehajtogatásának tárgyterületén a „négy-helikális fel-és-le köteg” fogalom pozitív és negatív példáját mutatja. Mindkét példa struktúráját egy olyan kb. 100 konjunktív tagot tartalmazó logikai kifejezésbe kódolták, mint amilyen például a TeljesHossz(D2mhr, 118) ∧ HelikálisSzám(D2mhr, 6) ∧ … kifejezés.
A teljes algoritmus a  ábrán látható. Egyesével sorban végigfuttatja a mintákat a hálón, és minden egyes példa után a hiba csökkentése érdekében kissé módosítja a súlyokat. A mintahalmaz egyszeri végigfuttatását epochnak (epoch) nevezzük. Az epochokat addig ismételjük, amíg valamilyen leállási feltétel nem teljesül – tipikus, hogy akkor állunk le, amikor a súlyváltozások már nagyon kicsivé válnak. Más módszerek esetén eredő gradienst számítunk az egész tanító halmazra, egyszerűen összeadva az egyes példáknál a   származó gradienseket, és az eredő gradiens alapján frissítjük a súlyokat. A sztochasztikus gradiens (stochastic gradient) módszer nem ciklikusan veszi a tanító halmaz mintáit, hanem inkább véletlenszerűen választ mintákat a tanító halmazból.
A következő problémás él az (Q, ÚDW) él lesz, hiszen ÚDW már csak a 'vörös' értéket veheti fel, így ha Q-nak ezt választjuk, az inkonzisztenciához vezet. Vegyük tehát el Q tartományából a 'vörös' értéket!
Az első – ezen probléma tanulását célzó – munkát Michie és Chambers végezte (Michie és Chambers, 1968). Az ő Boxes nevű algoritmusuk mindössze 30 próbálkozás után képes volt a rudat több mint egy óráig egyensúlyban tartani. Ráadásul, sok későbbi rendszertől eltérően, a Boxes egy valódi kocsit és rudat használt, nem szimulációt. Az algoritmus először is kvantálta, azaz tartományokra („dobozokra”) bontotta a négydimenziós állapotteret, innen jött az algoritmus neve is. Ezek után addig végeztek egy-egy kísérletet, amíg a rúd leesett vagy a kocsi elérte a pálya végét. Negatív megerősítést rendeltek az utolsó állapottér-tartományban az utolsó cselekvéshez, és ezt terjesztették vissza a megelőző szekvenciára. Azt találták, hogy a kvantálás akkor okoz bizonyos problémákat, ha a berendezést más pozícióból indították, mint amiket a tanítás során használtak, ami arra utal, hogy az általánosítás nem volt tökéletes. Jobb általánosítóképesség és gyorsabb tanulás érhető el, ha egy olyan algoritmust használunk, amely adaptívan osztja részekre az állapotteret, a jutalom megfigyelt változásainak megfelelően. Manapság egy háromszoros invertált inga egyensúlyozása közönséges feladatnak számít – ez már a legtöbb ember ügyességét messze meghaladja.
Az információgyűjtő és az azt felhasználó terveket sokszor röviden futási idejű változóknak (runtime variables) nevezzük, ami a korábban leírt értékbehelyettesítő változó konvenciójához szorosan kapcsolódik. Béla telefonszámának a kikeresésére, majd a tárcsázására vonatkozó tervet az alábbi alakban fel lehet írni:
Bemutattuk az érzeteket és a cselekvéseket, ezután itt az ideje, hogy bemutassuk magát a környezetet. Kezdjük az objektumokkal. A négyzetek, a csapdák és a wumpus nyilvánvaló szereplők. Elnevezhetnénk minden egyes négyzetet – Négyzet[1,2] és így tovább – de akkor az a tény, hogy a Négyzet[1,2 ]szomszédos a Négyzet[1,3]-mal, egy extra tényt igényelne, és minden egyes négyzetpárnál szükségünk lenne egy ilyen tényre. Jobb egy komplex termet használni, amelyben a sor és az oszlop egész számokként jelenik meg; például egyszerűen használhatjuk a lista termet: [1, 2]. Bármely két négyzet szomszédossága meghatározható így:
Megmutattuk, hogy egy ágens hogyan képes észlelni egy szófüzért, és hogyan képes egy nyelvtant használni a lehetséges szemantikai értelmezések halmazának előállítására. Most annak a problémáját vizsgáljuk, hogy az értelmezés hogyan tehető teljessé az egyes jelöltek értelmezéshez az adott szituációt leíró kontextusfüggő információk hozzáadásával.
A vizsgált adatokat a matematikai analízis előtt összesítették, majd minden egyes paraméter átlagának meghatározása következett és megvizsgáltuk standard deviációt is, a vizsgált három csoportban. (1.sz táblázat)
Emlékeztetőül: relaxált egy probléma, ha az operátorokra kevesebb megkötést teszünk, mint az eredeti problémában. Ekkor a relaxált probléma optimális megoldásának költsége egy elfogadható heurisztika az eredeti problémára. (Lásd még  fejezet, „Elfogadható heurisztikus függvények kitalálása”.)
A háromszög egy sokszög, a sokszög pedig zárt görbe, amihez tartozik terület és kerület tulajdonság: * zárt görbén belül: egyenlő szárú, x területű háromszög * zárt görbén kívül: egyenlő szárú, y területű háromszög
Az F fehérje a „Négy-helikális fel-és-le köteg” összehajtogatási osztályhoz tartozik, ha egy hosszú cs[1] helikálist tartalmaz a másodlagos struktúra 1. és 3. közötti pozícióban, valamint cs[1] a második helikálissal szomszédos.  ábra - Egy tipikus családfa Egy tipikus családfa
A rekurzív logikai programok felesleges ciklusainak elkerüléséhez egymástól függetlenül Smith (Smith és társai, 1986), valamint Tamaki és Sato (Tamaki és Sato, 1986) fejlesztettek ki módszereket. Az utóbbi tanulmány tartalmazta a memók gyűjtését is a logikai programokra, egy olyan módszert, amelyet teljes mértékben David S. Warren fejlesztett ki a táblázatos logikai programozás (tabled logic programming) módszerében. Swift és Warren (Swift és Warren, 1994) bemutatták, hogy hogyan terjesszük ki a WAM-ot táblázatok kezelésére, amely képessé tette a Datalog programokat, hogy egy nagyságrenddel gyorsabban fussanak, mint az előrefelé láncolást alkalmazó deduktív adatbázisrendszerek.
Az értékiteráció konvergenciájának megmutatásában használt alapfogalom az összehúzás (contraction). Az összehúzás nagyjából egy olyan egyváltozós függvény, ami két különböző bemeneti értékre alkalmazva, két olyan kimeneti értéket ad, amelyek „egymáshoz közelebbiek”, mint az eredeti értékek, legalább egy bizonyos állandó mennyiséggel. Például a „kettővel való osztás” egy összehúzás, mivel bármely két szám elosztása után a különbségük megfeleződik. Vegyük észre, hogy a „kettővel való osztásnak” van egy fix pontja, nevezetesen a nulla, ami a függvény alkalmazásával nem változik. Ebből a példából az összehúzás két fontos tulajdonságát vehetjük észre: * Egy összehúzásnak egyetlen fix pontja van: ha két fix pontja lenne, nem kerülnének egymáshoz közel a függvény alkalmazásakor, így nem is volna összehúzás. * A függvény tetszőleges argumentumra történő alkalmazásakor a függvényértéknek közelebb kell kerülniük a fix ponthoz (mivel a fix pont nem mozdul el), így egy öszszehúzás ismételt alkalmazása a fix pontot adja határértékben.
A válasz viszonylag egyszerű, és kis gyakorlás után egészen mechanikus (akár algoritmizálható is). Először is soroljuk fel a ternáris korlátban szereplő változóink lehetséges érték-kombinációit, majd minden egyes értékkombinációhoz rendeljünk egy egyedi értéket, és mondjuk azt, hogy ezek az egyedi értékek egy további segédváltozó értékei valójában. Ezek után felsorolható, hogy a ternáris korlátban szereplő 3 változó és a további egy imént bevezetett változó, azaz összes 4 változó egyszerre milyen értékeket vehet fel. Ez tehát egy kvaternális érték-korlát lenne, amely megszabná, hogy a 4 változó együtt milyen értékeket vehet fel. Ezt a kvaternális korlátot viszont már nagyon könnyen binárissá tehetjük. Egyszerűen csak vegyük az imént bevezetett segédváltozót, és a megfelelő értékeihez rendeljük hozzá a másik három változó közül valamelyiknek a megfelelő értékét. Ezt tegyük meg a segédváltozó minden értékére, és aztán a három változó mindegyikére. Így tehát bináris érték-korlátokat kapunk rendre a segédváltozó és a másik három változó között, páronként. Lássuk mindezt konkrétan a fentebbi, ChesterfieldsHouse változóval kapcsolatos ternáris korlátunk esetében.
Az idők folyamán mind a négy irányzat követőre talált. De ahogy ez várható volt, feszültség uralkodik az embercentrikus és a racionalitáscentrikus irányzatok között.^[1] Az embercentrikus irányzat szükségképpen empirikus tudomány, hipotézisekkel és empirikus igazolással.
Egész gyorsan kiszorítanak minket a létezésből. Másokkal szemben engem ez a lehetőség nem tölt el aggodalommal, én ezeket a jövőbeli gépeket a leszármazottainknak, „elmebeli gyermekeink”-nek tartom, akiket képünkre és hasonlatosságunkra építettünk, önmagunkként egy jóval nagyobb képességű alakban. A korábbi generációk biológiai gyermekeihez hasonlóan az emberiség hosszú távú jövőről alkotott legjobb reményét testesítik meg. Belső kötelességünk minden előnyt megadni nekik és ellépni útjukból, ha már többet nem tudunk segíteni.
A FASTUS első lépése a tokenizálás (tokenization), amely a karakterfüzéreket tokenekbe (szavak, számok és írásjelek) szegmentálja. Az angol nyelv esetén a tokenizálás elég egyszerű lehet, pusztán a szóközök (white space) vagy írásjelek mentén történő szegmentálás viszonylag jó hatékonyságú. A japán nyelv esetén a tokenizálásnak szegmentálást kell végeznie, valami olyasmit használva, mint a Viterbi algoritmus (lásd  ábra). Egyes tokenizálók az olyan jelölőnyelveket is kezelik, mint a HTML, az SGML és az XML.
Az ágens lehetőségeit az állapot megváltoztatására cselekvéseknek nevezzük. A feladat megoldása tehát cselekvések egy sorozata lesz, amelyek közbülső állapotokon keresztül elvezetnek a kezdőállapotból egy célállapotba.
Néha a célorientált cselekvés választás egyszerű, amikor a cél teljesülése azonnal egyetlen cselekvéssel elérhető. Máskor sokkal trükkösebb, amikor az ágensnek befordulások és visszakanyarodások sorozatát kell megfontolnia a cél eléréséhez. A keresés (search) (lásd 3– fejezet) és a tervkészítés (planning) (lásd 11– fejezet) a mesterséges intelligencia azon területei, amelyek az ágens céljait elérő cselekvéssorozat megtalálásával foglalkoznak.
Annak érdekében, hogy a robot valós környezeti viszonyok között bejárja az előírt pályát elengedhetetlen a pontos helymeghatározás. A helymeghatározásnak megkülönböztetik statikus és dinamikus változatait (Takeda). Az első esetben a robot szenzorjelek által megadott pozícióját hasonlítják a lehető legpontosabban össze a környezeti modell előzetesen ismert leírásával. A dinamikus helyzet-meghatározás ezzel szemben verifikálás jellegű: a korábbi szenzorjellemzők által szolgáltatott (helyzet, sebesség, stb.) információkból kalkulált helyzetnek és a mozgó robot aktuális állapota közötti különbséget újra és újra ismétlődően kiértékelik és ha kell, módosítják az adatokat.
 Fontos A definíció fontosságát az optimális döntés megválasztásában a következő tulajdonság adja: ha A[1] sztochasztikusan dominálja A[2]-t, akkor bármely U(x) monoton nem csökkenő hasznosságfüggvény esetén, A[1] várható hasznossága legalább olyan nagy, mint A[2] várható hasznossága. Azaz, ha egy cselekvést egy másik cselekvés minden attribútum esetén sztochasztikusan dominál, akkor az első cselekvés figyelmen kívül hagyható.
A mélység kiszámításához több képkocka használata szükséges. Ha a filmre egy merev testet veszünk fel, akkor ennek alakja képkockáról képkockára nem fog változni, és így az optikai folyam természeténél fogva zajos méréseit jobban kezelhetjük. Az egyik ilyen megközelítés eredményét a  és a  ábrán láthatjuk (Tomasi és Kanade, 1992).
A kulcsmomentum azt észrevenni, hogy a geometriai transzformációk, mint például az elfordítás, a méretváltás és a forgatás, vagy a kép fényességének megváltozásai a fényforrások fizikai mozgatása által, más jellemzőkkel rendelkeznek, mint a kategórián belüli variációk, mint például az emberek arcainak különbözőségei. Nyilvánvaló, hogy a tanulás az egyetlen mód az emberi arcok különbözőségeinek megtanulására vagy a 4-es számjegy különböző írásmódjainak felismerésére. Másrészről a geometriai és fizikai transzformációk hatásai szisztematikusak és kiküszöbölhetők a tanuló minták paramétereit leíró tulajdonságok megfelelő tervezésével.
Mivel az ágens szinte minden cselekvését a +time(_) triggereli és mivel ez a terv szerepel először az ágens forráskódjában, ezért ez tekinthető a legmagasabb prioritású tervnek, hiszen ha a feltételei teljesülnek (az energia kevesebb mint 25), akkor mindenképpen ez hajtódik végre.
Az eljárásmód egy sorozat, mivel ez a probléma determinisztikus a hiedelmi állapotok terében – nincsenek megfigyelések. Azt a „trükköt” alkalmaztuk, hogy az ágens egyetlen Balra mozgással biztosítja, hogy nincs (4, 1)-ben, azért, hogy aztán meglehetős biztonsággal mozoghasson Fel és Jobbra a +1 kijárat eléréséhez. Az ágens a +1 kijáratot 86,6% gyakorisággal éri el, és sokkal gyorsabban, mint az alfejezet elején megadott eljárásmód esetén, így a várható hasznossága 0,38, szemben az előző 0,08-as értékével.
 ábra - A tervkészítés két megközelítése. (a) Előrefelé (progresszív) állapottér-keresés, a kiinduló állapotból indulva és a probléma cselekvéseit használva halad a cél felé. (b) Visszafelé (regressziós) állapottér-keresés: valószínűségi állapot keresés (lásd 3. szakasz - Szenzor nélküli problémák részben) a célállapot(ok)ból indulva a cselekvések inverzét alkalmazva keresi visszafelé a kezdeti állapotot. A tervkészítés két megközelítése. (a) Előrefelé (progresszív) állapottér-keresés, a kiinduló állapotból indulva és a probléma cselekvéseit használva halad a cél felé. (b) Visszafelé (regressziós) állapottér-keresés: valószínűségi állapot keresés (lásd 3. szakasz - Szenzor nélküli problémák részben) a célállapot(ok)ból indulva a cselekvések inverzét alkalmazva keresi visszafelé a kezdeti állapotot.
b) Magyarázza el röviden, hogy egy sokszög csúcsát egy másikkal a síkban összekötő legrövidebb útnak miért kell egy olyan egyenes vonalszakaszokból állnia, amelyek az egyes sokszögek csúcsait kapcsolják össze? Adjon most egy jó állapottér definíciót. Milyen nagy ez a tér?
A rajzolás folyamatát egy szabályos hatszögháló kivitelezésén mutatom be. Egy egyenes rajzolásához a fényceruzát a képernyőnek kell szegezni és a draw (rajzolás) billentyűt kell lenyomni. Ekkor az alkalmazás rögzíti a kezdőpontot és a fényceruza mozgását követve keresi a végpontot, melyek között, mint egy gumiszalag, egyenest húz, ahogy balra is látható az  ábrán.  ábra - Egyenes rajzolása Egyenes rajzolása
Más szavakkal, a döntési fák bizonyos függvények esetén jók, mások esetén roszszak. Van bármilyen olyan reprezentáció, amely mindenfajta függvény esetén hatékony? A válasz sajnos az, hogy nincs. Ezt általánosan is meg tudjuk mutatni. Vizsgáljuk az összes n bemeneti attribútummal rendelkező logikai (Boole) függvényt. Hány különböző függvény van ebben a halmazban? Ez éppen a lehetséges felírható igazságtáblák számával egyezik meg, hiszen egy függvényt az igazságtáblája ad meg. Az igazságtáblának 2^n sora van, mivel minden bemeneti esetet n attribútummal adunk meg. Úgy tekinthetjük a tábla „válasz” oszlopát, mint egy 2^n bites számot, amely definiálja a függvényt. Mindegy, hogy a függvények milyen reprezentációját választjuk, néhánynak (valójában szinte mindegyiknek) a reprezentálásához tényleg szükség lesz ennyi bitre.
A legtöbb esetben a szabad tér alakja nagyon bonyolult a konfigurációs térben, még akkor is, ha a munkateret egyszerű poligonokkal reprezentáljuk. Éppen ezért a gyakorlatban inkább feltérképezik a konfigurációs teret, semmint pontosan kiszámítják. A térképezés során generálnak egy konfigurációt, és megnézik, hogy az a szabad térben van-e. Ehhez alkalmazzák a robot kinematikáját, és annak alapján már látszik, ha az adott konfiguráció ütközéssel jár a munkatérben.  ábra - Három különböző robotkonfiguráció a munkatérben és a konfigurációs térben Három különböző robotkonfiguráció a munkatérben és a konfigurációs térben
Valahogy nyilván kell tartanunk a legenerált, kifejtésre váró csomópontokat is – ezt a gyűjteményt, listát peremnek (fringe) nevezik. A perem minden eleme egy levélcsomópont (leaf node), azaz egy olyan csomópont, amelynek a fában nincsenek követői. A  ábrán a fák pereme a vastagon bekeretezett csomópontokból áll. A perem legegyszerűbb reprezentációja egy csomóponthalmaz lenne. A keresési stratéga ekkor olyan függvény lenne, amely a következő lépésben kifejtendő csomópontot ebből a halmazból választaná ki. Bár elvi szempontból ez nyilvánvaló megoldás, számításigény szempontjából drága lenne, mert a keresési stratégia függvénynek a halmaz minden egyes elemét végig kellene néznie, hogy ki tudja választani a legjobb csomópontot. Ezért a továbbiakban feltesszük, hogy a csomópontgyűjtemény egy várakozási sorként (queue) van megvalósítva. A soron végezhető műveletek az alábbiak: * Sort-Létrehoz(elem, …) létrehoz egy az adott elemeket tartalmazó sort. * Üres?(sor) csak akkor ad vissza igaz értéket, ha a sor üres. * Első-Elem(sor) visszaadja a sor első elemét. * Távolítsd-El-Az-Első-Elemet(sor) visszaadja az Első-Elem(sor)-t és ezt eltávolítja a sorból. * Beszúr(elem, sor) egy elemet szúr be a sorba. * Beszúr-Mind(elemek, sor) egy elemhalmazt beszúr a sorba.
Vagyis mindegyik axióma említ tehát néhány cselekvést, amelyek a folyó eseményt igazzá, és néhányat, amelyek azt hamissá teszik. Ezt a körülményt formalizálhatjuk a PozHatás(a, F[i]) és a NegHatás(a, F[i]) predikátumok bevezetésével. A PozHatás(a, F[i]) azt jelenti, hogy az a cselekvés az F[i]-t igazzá, a NegHatás(a, F[i]) pedig azt, hogy hamissá teszi. A fenti axióma ezzel a következőképpen írható át:
Hasznosság mint a preferenciák kifejezése. A racionális döntéseket az elvárt hasznosság maximalizálására alapozni elméletileg teljesen általános, és megóv a tisztán célorientált megközelítések számtalan hibájától (mint például az ellentmondó célok és a bizonytalan elérésük). Mindeddig azonban kevés munka folyt a realisztikus hasznossági függvények előállítása terén – képzeljük el például az egymással kölcsönhatásban lévő preferenciák komplex hálózatát, amelyeket az emberek irodai asszisztenseként működő ágensnek meg kell értenie. Elég nehéznek bizonyult az összetett állapotokkal kapcsolatos preferenciákat dekomponálni az összetett állapotokról szóló bizonyosságok Bayes-hálós dekompozíciójához hasonlóan. Ennek az egyik oka az lehet, hogy az állapotok preferenciái valójában az állapotok múltbeli preferenciáiból lettek összeszerkesztve, amelyeket jutalomfüggvények (reward functions) írnak le (lásd  fejezet). A keletkező hasznossági függvény akkor is nagyon összetett lehet, ha a jutalomfüggvény egyszerű. Mindez azt sugallja, hogy komolyan kell vennünk a jutalomfüggvények tudásmérnöki konstrukcióját, mert ezzel tudjuk az ágens értésére adni, mit akarunk tőle.
Figyeljük meg, hogy mindegyik példa egy objektumpáros, hiszen a Nagyszülője egy bináris predikátum. Egészét tekintve a családfában 12 pozitív és 388 negatív (a személyek minden más párosítása) példa található.
Ezt az állítást megfogalmazhatjuk szemantikus hálóval, feltéve hogy a kapcsolatok reifikáltak (reified), azaz önálló objektumnak tekinthetők. Így például VanHúga objektumot az Inverze éllel lehetne összekapcsolni a Húga objektummal. Ha egy lekérdezés azt firtatná, hogy ki Jánosnak a Húga, az öröklődéses algoritmus felfedezheti, hogy a VanHúga a Húga inverze, és a lekérdezést úgy megválaszolhatja, hogy a VanHúga kapcsolatot János-tól Máriá-ig követi végig. Az inverz információ nélkül esetleg szükséges lehetne az összes nőnemű személyt megvizsgálni, van-e netán Húga kapcsolata Jánossal. Ez amiatt van így, mert a szemantikus háló közvetlen indexelést csak az objektumok, a kategóriák és a belőlük induló kapcsolatok esetén biztosít. Az elsőrendű logika nyelvében ez annak felel meg, mintha a tudásbázist a predikátumok csakis első argumentumai szerint indexelnénk.
Az így megkonstruált összetett heurisztikus függvény mindig azt a függvényt használja, amelyik az adott csomópontra a legpontosabb. Mivel az alkotóelemként felhasznált heurisztikus függvények mind elfogadhatók, ezért h is elfogadható. Ugyancsak könnyű bizonyítani, hogy h konzisztens. Továbbá h dominálja az összes, benne alkotóelemként felhasznált heurisztikus függvényt.
Nyelvmegértés és problémamegoldás: A Proverb rendszer a legtöbb embernél jobb keresztrejtvényfejtő (Littman és társai, 1999). Ehhez rendelkezik a lehetséges megfejtő szavakra vonatkozó korlátozásokkal, a régebbi keresztrejtvények nagy adatbázisával és sokféle információs forrással, szótárakat és az olyan online adatbázisokat is beleértve, mint mozicímlisták a bennük szereplő színészekkel. Így például képes megállapítani, hogy a „Nice Story” meghatározás megoldása „ETAGE”, mert az adatbázisában „Story in France/ETAGE” meghatározás/megoldás pár szerepel, és felismeri, hogy a „Nice X” és „X in France” mintáknak sokszor azonos a megoldása. A program természetesen nem tudta, hogy Nice egy város Franciaországban, a keresztrejtvényt mégis képes volt megoldani.^[18]
A gazdasági ellátás egyre fokozódó mértékben igényli a számítógépes hátteret, mivel napjainkban a kórházaknak „modern nagyüzemként” kell működniük, naprakészen figyelemmel kell kísérni az intézmény gazdasági mutatóit, a pénzügyi forrásokat, a költségeket, az emberi erőforrások felhasználását, a raktárkészletet, az osztályonkénti anyagfelhasználást, a bérek alakulását stb..
A műszaki fejlődés következtében igény érkezett a hagyományos nyelvészet felől a terület kiterjesztésére, vagyis nyelvi jelenségek tudományos magyarázatára a technika és számítástudomány adta eszközökkel. [3] Ugyanakkor az igény fordítva is felmerült: a technológia és számítástechnika irányából olyan feladatok megoldására, melyben szükség van a természetes nyelv feldolgozására (felismerésére, értelmezésére), amihez a hagyományos nyelvészet számos módszert kínál. [3] Mindezen túl a számítógépek fejlődésével és elterjedésével terítékre kerültek általánosabb, mindennapi feladatok (szöveg felolvasása, javítása, benne található utasítások értelmezése és végrehajtása stb.), melyeket jó lenne, ha egy gép el tudna végezni, vagy legalábbis segítene az elvégzésükben. [1]
Eddig úgy tekintettünk a következtető rendszerekre, mint független ágensre, amelynek önállóan kell döntéseket hoznia és cselekednie. A tételbizonyítások másik felhasználása, amikor segítőként használjuk, és tanácsokkal lát el, mondjuk egy matematikust. Ebben a használati módban a matematikus felügyelőként viselkedik, feltérképezi a következő lépés meghatározásának stratégiáját, hogy mi legyen, és megkéri a tételbizonyítót, hogy töltse ki a részleteket. Egy bizonyos fokig mellékessé teszi a félig eldönthetőség problémáját, mivel a felügyelő megszüntethet egy lekérdezést, és próbálhat egy másik megközelítést, ha a lekérdezés túl sok időt vesz igénybe. A tételbizonyítás bizonyítás-ellenőrzőként (proof checker) is működhet, ahol a bizonyítást mi adjuk meg vázlatosan, nagyobb lépések sorozataként, és a rendszer tölti ki az egyedi következtetések részleteit, amelyek igazolják lépéseink helyességét.
Tegyük fel, hogy a Kíváncsiság nem ölte meg Tunát. Tudjuk, hogy vagy Jankó, vagy a Kíváncsiság tette; tehát biztosan Jankó tehette. Mármost, Tuna egy macska, és a macskák állatok, tehát Tuna egy állat. Mivel bárkit, aki megöl egy állatot, senki sem szeret, tudjuk, hogy Jankót nem szereti senki. Másrészt, Jankó minden állatot szeret, tehát valaki szereti őt, tehát ez az ellentmondás áll fenn. Így tehát a Kíváncsiság ölte meg a macskát.
Emlékezzünk a  fejezetben a tanulási problémára bemutatott étterem példára: egy olyan döntési szabály megtanulására, hogy vajon érdemes-e egy asztalra várni. A példákat olyan attribútumokkal (attributes) írtuk le, mint az Alternatíva, Bár, Péntek/Szombat stb. Logikai megközelítésben egy példa egy logikai állítással leírt objektum; az attribútumok pedig unáris predikátumok. Nevezzük általánosságban az i-edik példát X[i]-nek. A  ábra első példáját az alábbi állítások írják le:
Az algoritmusban a θ-ban tárolt aktuális változólekötéseket összevonjuk azokkal a lekötésekkel, amelyeket a célnak a klózfejjel való egyesítése eredményez, megkapva így egy új aktuális lekötéshalmazt a visszafelé történő híváshoz.
Végezetül megemlítjük, hogy a logisztikus regresszió egy gyakori neurális hálózati modell, a többrétegű perceptron alapmodelljeként is szemlélhető. A többrétegű perceptron egy komplex nemülineáris leképzést valósít meg egymást követő összegző és nem-lineáris transzformációt végző rétegekkel. Például egy egyetlen rejtett rétegű L csomópontú többrétegű perceptron definíciója a következő:
Ha 2^n bitre van szükség a függvény megadásához, akkor n attribútum esetén  a lehetséges függvények száma. Ez megrázóan nagy szám. Például csupán hat Boole-változó esetén is  különböző logikai (Boole) függvény állítható elő. Szükségünk lesz néhány szellemes algoritmusra, hogy egy ilyen hatalmas térben konzisztens hipotézist tudjunk találni.
d) Implementálja az algoritmust és alkalmazza a 8-as játék és az utazó ügynök probléma konkrét eseteire. Hasonlítsa össze az algoritmus hatékonyságát az egyenletes költségű algoritmus hatékonyságával és értelmezze az eredményeit.
Ennek a korlátnak a formalizálásánál korlátokba ütközünk, méghozzá az eddigi formalizmus korlátaiba, a korlátokat leíró nyelv kifejezőerejének határaiba. Miért is? Mert, ha megnézzük, itt valami olyasmit kellene írnunk, hogy:
A DPLL algoritmust mutatja a  ábra. Az ábrán a lényeges elemeket tartalmazó vázat adtuk meg, ami bemutatja magát a keresési folyamatot. Nem szerepel az adatstruktúra leírása, amelyet fenn kell tartani, hogy a keresési lépéseket hatékonyan lehessen elvégezni, sem azoknak a trükköknek a leírása, amelyeket a teljesítmény fokozása céljából az alapalgoritmushoz hozzá lehet adni: klóztanulás, változó választási heurisztika és a véletlenszerű újraindítások technikája. Ha ezeket is beépítjük az algoritmusba, akkor a DPLL az egyik leggyorsabb kielégíthetőségi algoritmus, antikvitása ellenére is. A Chaff implementáció millió változós hardververifikációs problémák megoldására szolgált.
Vegyük most szemügyre a mentális állapotok egy fajtáját, az intencionális állapotoknak (intentional states) is nevezett ítéletlogikai attitűdöket (propositional attitudes) (amelyekről már szó esett a  fejezetben). Ezek az állapotok – mint például a vélekedés, a tudás, a vágyakozás, a félelem és társai – olyanok, hogy bizonyos szempontból a külső világra utalnak. Az a vélekedés például, miszerint Bécs Ausztria fővárosa, egy bizonyos városra és annak státusára vonatkozó vélekedés. Azt fogjuk vizsgálni, hogy rendelkezhetnek-e a számítógépek intencionális állapotokkal, ezért hasznos áttekinteni ezen állapotok jellemzését. Azt is lehet mondani például, hogy az a mentális állapot, hogy egy hamburgerre vágyom, különbözik attól az állapottól, hogy egy pizzát szeretnék, hiszen a hamburger és a pizza a valódi világ különböző tárgyai. Másfelől viszont amellett érveltünk éppen néhány bekezdéssel korábban, hogy a mentális állapotok agyállapotok, tehát az azonosságukat meg kellene tudnunk határozni csupán „a fejen belül” maradva, minden hivatkozás nélkül a külső világra. A dilemma megoldásához vizsgáljunk meg egy olyan gondolatkísérletet, amely megpróbálja az intencionális állapotokat elkülöníteni a hozzájuk tartozó külső tárgyaktól.
A játékelmélet felhasználásának az ágenstervezés szélesebb területén eddig számos akadálya volt. Elsőként vegyük észre, hogy egy Nash-egyensúlyban egy játékos teljes mértékben feltételezi azt, hogy az ellenfél egyensúlyi stratégiát játszik. Ez azt jelenti, hogy a játékos semmilyen elvárását sem képes felhasználni a többi játékos valószínű cselekvésével kapcsolatban, és így lehet, hogy olyan fenyegetések ellen való védekezésre vesztegeti el az értékeit, amelyek soha nem is lépnek fel. A Bayes–Nash-egyensúly (Bayes–Nash equilibrium) fogalma részben ezt a hiányosságot pótolja: ez egy olyan egyensúly, ami megfelel egy játékos a priori valószínűség-eloszlásának a többi játékos stratégiái felett – másképpen fogalmazva, kifejezi egy játékos elvárásait a többi játékos valószínű stratégiájáról. Másodszor, jelenleg nincs jó módszer a játékelméleti és az RMMDF irányítási stratégiák összekapcsolására. Ezek és más problémák miatt a játékelméletet elsődlegesen inkább olyan környezetek elemzésére használják, amelyek egyensúlyban vannak, a környezetben lévő ágensek irányítása helyett. Hamarosan látni fogjuk, hogy a játékelmélet hogyan segítheti a környezetek tervezését.
Mint minden játékban itt is a játék célja a győzelem. A győzelemhez vezető út pedig a jó stratégiákban rejlik. Ahogyan eddig is szó volt róla, és ezután is lesz, a Hex nem tartozik az egyszerű játékok körébe. Egy igen kicsi táblán is már rengeteg típusú játékmenet is létrejöhet. Ha növeljük a tábla méretét, akkor ez pedig csak drasztikusan nő. Ezek közül kiválasztani a legjobbakat nagyon nehéz. Általános nyerő stratégia ezért nem is létezik a játékra. Adott méretű pályákra lehet megadni azt, hogyan tegyük a figuráinkat a cél eléréséhez. A rengeteg megoldás viszont emberek számára igen nehézkes lehet, míg gépek számára talán előnyösebb is.
Ezt kiterjesztéslemmának (lifting lemma) nevezik, mivel kiterjeszti a bizonyítási lépést az alap klózokról az általános elsőrendű klózokra. A kiterjesztési lemma bizonyításához Robinsonnak szüksége volt az egyesítés módszerének megalkotására és a legáltalánosabb egyesítő tulajdonságainak levezetésére is. A bizonyítás áttekintése helyett a következőkben illusztráljuk a lemmát:
A CLSI (korábban:NCCLS) ajánlása a kontrollanyagok minőségére vonatkozóan: * A kontrollok a klinikailag releváns szinteket képviseljék. * Általában olyan anyag vizsgálata a legelőnyösebb, mely a vizsgált mintával azonos, vagy ahhoz hasonló összetételű (szérum, plazma, vizelet, liquor, teljes vér, stb.) és a vizsgálati módszerekkel a betegmintákhoz hasonlóan viselkedik.
Az egyes döntéstámogató szolgáltatásokat az elektronikus konzulensek tanácsokkal segítik az orvosokat abban, hogy betegeiket a számukra legmegfelelőbb eljárásokkal gyógyítsák. A felügyelő programok folyamatosan figyelemmel kísérik az ellátás menetét, és üzenetekkel jelzik az elkövetetett hibákat és mulasztásokat. Végül a kritikai programoktól azt várhatjuk el, hogy véleményt mondjanak az általunk helyesnek gondolt diagnózisról és/vagy terápiáról.
Vég nélküli viták folytak a valószínűségi számok forrása és értelme körül. A frekvencionista (frequentist) álláspont szerint ezek az értékek csak kísérletekből származhatnak: ha 100 ember vizsgálata során az derül ki, hogy 10-nek szuvas a foga, akkor azt mondhatjuk, hogy a szuvas fog valószínűsége körülbelül 0,1. E szemlélet szerint az az állítás, miszerint „a lyuk valószínűsége 0,1” azt jelenti, hogy ha végtelen sok mintát megvizsgálnánk, határértékét tekintve 0,1 lenne annak a résznek a hányada, ahol ezt tapasztalnánk. Tetszőleges, de véges számú minta esetén pedig megbecsülhetjük a valós arányt, valamint kiszámíthatjuk azt is, hogy várhatóan mekkora lesz a becslésünk pontossága.
Formális bizonyítás: A megoldás akkor biztosan optimális, ha f(n) ≤ f(n’), azaz a kiértékelő függvény monoton. Ez azt jelenti, hogy h(n) ≤ h(n,n’) + h(n). Jelölje a két oldal különbségét E. Ekkor a fenti egyenlőtlenséget részletesen felírva:
Tegyük fel, hogy a Foil program egy klózhoz egy literált úgy add hozzá, hogy egy bináris P predikátumot használ, és hogy az előbbi literálok (a klóz fejrészét beleértve) öt különböző változót tartalmaznak. a. Hány funkcionálisan különböző literált lehet generálni? A két literál funkcionálisan egybeesik, ha csupán az általuk tartalmazott új változók neveiben különböznek. b. Meg tudna adni általános képletet a különböző, r argumentummal rendelkező predikátumot használó literálok számának meghatározására, ha előzőleg n változót használtunk? c. A Foil miért nem enged olyan literálokat használni, amelyek az előzőleg használt változókat nem tartalmazzák?
Néhány további trükk szükséges ahhoz, hogy a DCG működjön; például szükségünk van egy módszerre, hogy záró szimbólumokat adhassunk meg, és kényelmes egy olyan módszert is alkalmaznunk, amely nem engedi az automatikus karaktersorozat-argumentum hozzáadását. Mindent összerakva, a definit klóz nyelvtant a következőképpen definiálhatjuk: * Az X → Y Z … jelölés fordítása Y(s[1]) ∧ Z(s[2]) ∧ … ⇒ X(s[1 ]+ s[2]+ …) * Az X → Y | Z | … jelölés fordítása Y(s) ∨ Z(s) ∨ … ⇒ X(s) * Mindkét megelőző szabályban bármelyik Y nem záró szimbólum egy vagy több argumentummal kiterjeszthető. Bármely argumentum lehet változó, konstans vagy argumentumok függvénye. A fordításban ezen argumentumok megelőzik a karakterfüzér argumentumot (például az NP(eset) fordítása NP(eset, s[1])). * A {P(…)} jelölés szerepelhet egy szabály jobb oldalán, és változás nélkül átalakul P(…)-vé. Ez lehetővé teszi a nyelvtan írója számára, hogy egy P(…)-re vonatkozó tesztet anélkül elhelyezzen, hogy ahhoz automatikusan hozzáadódjon a karakterfüzér-argumentum. * Az X → word jelölés fordítása X([word]).
Ha a környezet (vagy a stratégia) sztochasztikus, akkor a helyzet nehezebbé válik. Tegyük fel, hogy hegymászó módszert próbálunk alkalmazni, ami azt kívánja, hogy összehasonlítsuk ρ(θ)-t ρ(θ + Δθ)-val valamilyen kis Δθ esetén. Az a probléma, hogy a teljes jutalom nagyon nagyokat változhat kísérletről kísérletre, így a kisszámú kísérletből számított stratégiaérték nagyon megbízhatatlan lesz – két ilyen becslés összehasonlítása pedig még megbízhatatlanabb. Egy lehetséges megoldás, ha egyszerűen sok kísérletet futtatunk le, és a minta variancájával mérjük, hogy elegendő kísérletet futtattunk-e már ahhoz, hogy megbízhatóan jelezni tudjuk a ρ(θ) javításának irányát. Sajnálatos módon ez sok valós probléma esetén nem praktikus, mert az egyes kísérletek drágák, időigényesek és esetleg veszélyesek lehetnek.
Az előrecsatolt hálókat rendszerint rétegekbe (layer) szervezzük oly módon, hogy minden egyes egység csak a közvetlenül megelőző réteg egységeitől kap bemeneti jelet. A következő két alfejezetben egyrészt az egyrétegű hálózatokkal foglalkozunk, amelyeknek nincsenek rejtett egységei, másrészt a többrétegű hálókkal, amelyek egy vagy több rejtett réteggel rendelkeznek.
Az egyik technika néhány előre meghatározott mindenképpen szükséges kérdés feltevése. A válaszok segítségével a rendszer elkezdi a megoldáskeresést, de kizárja az érvrendszerből azokat a célokat, amik teljesen lehetetlenek az adott tünetek mellett.
A  ábra arra mutat példát, ahogy az Őse definíciójának tanulása közben egy új P predikátumot generálunk. Ha már előállítottuk a P-t, akkor ez használható az inverz rezolúció későbbi lépéseiben is. Egy későbbi lépés például feltételezheti, hogy az Anyja(x, y) ⇒ P(x, y). Az új predikátum jelentését így a predikátumot felhasználó hipotézis generálása korlátozza. Egy másik példa elvezethet az Apja(x, y) ⇒ P(x, y)-ig. Más szóval a P predikátum olyan valami, ami minket általában a Szülője relációra emlékeztet. Korábban említettük, hogy új predikátumok bevezetése a célpredikátum definíciójának a méretét lényegesen csökkentheti. Az új predikátumok kialakításának beépített képességével, az inverz rezolúciós rendszer sokszor olyan tanulási problémákkal is megbirkózik, amelyekre más módszerek alkalmatlanok.
Rengeteg algoritmust javasoltak arra, hogy egy kisebb hálóból nagyobbat növesszenek. Egyikük, a csempézés (tiling), a döntési fa tanulásra emlékeztet. Az ötlet az, hogy kezdjünk egyetlen neuronnal, amely a legjobbját nyújtja, hogy annyi tanító mintára adjon helyes választ, ahányra csak lehet. További neuronokat adunk hozzá, hogy megoldjuk azokat a példákat, amelyekre az első neuron rossz választ adott. Az algoritmus csak annyi neuront ad hozzá, amennyi az összes minta megoldásához szükséges.
 ábra - 10. lépés: az előbbi, WA=GREEN értékadás következményeként a WA, NT, Q, SA, és V változók értékkészlete egy-eleműre szűkült az AC3/MAC következtetés eredményeképpen 10. lépés: az előbbi, WA=GREEN értékadás következményeként a WA, NT, Q, SA, és V változók értékkészlete egy-eleműre szűkült az AC3/MAC következtetés eredményeképpen
Ezekből az egyenletekből látható, hogy az előre-hátra algoritmus (lásd  ábra) időkomplexitása egy t hosszúságú sorozatra történő alkalmazásnál O(S^2t), mivel minden lépés egy S elemű vektor szorzását igényli egy S × S mátrixszal. A tárigény pedig O(St), mivel az előrefelé fázis t darab S méretű vektort tárol el.
Vegyük a következő feladatot: egy páciens olyan tünetekkel érkezik az orvoshoz, amelyet kiszáradás vagy D fertőzés okozhat (de nem mindkettő). Két lehetséges cselekvés van: Iszik, ami feltétel nélkül kúrálja a kiszáradást, és Gyógyszert szed, amely a D fertőzést kúrálja ki, de káros mellékhatása van, ha a páciens kiszáradt. Írja fel a probléma leírást PDDL-ben, és az összes releváns világ számbavétele mellett ábrázolja az érzékelőmentes tervet, ami megoldja a problémát.
A pályabejárás, vagy navigáció során a megtervezett pályát követjük a robottal. Legnagyobb problémát a robot tervezett és realizált helyzete közötti különbség jelenti. Megoldást a nagyszámú környezeti szenzor által szolgáltatott adatok összehangolt feldolgozásával érhetünk el.
A fentebbi fólián látható játékhoz egy lehetséges informális interpretáció/történet lehet a következő: az 1-es játékos egy monopol telekommunikációs piacra újonnan belépő vállalkozás. Ha nem lép be (jobbra), akkor haszna 0, viszont az addigi monopol játékos, az inkumbens vállalkozás (2-es játékos) haszna 20. Azaz, ha 1-es nem lép be, akkor 2-es nagy bevételre tesz szert. Ha viszont 1-es belépne (balra), akkor a monopol vállalkozás megtehetné, hogy annyira leviszi az árait, hogy lehetetlen helyzetbe hozza vele az új belépőt. Ez felelne meg annak, ha a 2-es játékos balra lépve a (-2, 0) kimenetelt generálná. Mindazonáltal ez valójában egy hiteltelen fenyegetés, hiszen amennyiben az 1-es játékos belép a piacra (nem riad vissza ettől a fenyegetéstől), úgy a 2-es játékos, az addig monopol vállalat úgysem fog ésszerűtlen döntést hozni, és önérdekét követve kénytelen lesz osztozni a piacon az új belépővel, azaz jobbra lép, aminek hatására a (4,4) kimenetel fog előállni, hiszen 4 nagyobb, mint 0. Ésszerű esetben 2-es (is) haszonmaximalizálásra törekszik, versengve az 1-es játékossal. A játék megoldása tehát a (4,4) kimenetel, és az ahhoz tartozó (Balra, Balra) stratégia-kombináció, amit az előző példához hasonlóan pillanatok alatt megkaphatunk hátráló indukcióval.
A fenti módszer tehát két fő részből áll. A 0‑tól 8‑ig tartó lépésekben (n+1) darab lokális koordináta-rendszert {L[0], L[1], ..., L[n]} rendelünk a kartagok végeihez. Az algoritmus második felében (9‑től 13. lépés), a kinematikai paraméterek értékét határozzuk meg az adott struktúrára vonatkozóan.
A tulajdonságértékek összeadása értelmes dolognak tűnik, azonban ezzel valójában egy igen erős feltételezéssel élünk: t. i. hogy az egyes tulajdonságok hozzájárulásai más tulajdonságok értékeitől függetlenek. A 3-as érték hozzárendelése a futóhoz figyelmen kívül hagyja például azt, hogy a végjátékban, amikor sok tér áll rendelkezésre a manőverezéshez, a futó erősebb. Ezért a jelenlegi sakk- és más játékokat játszó programok a tulajdonságok nemlineáris kombinációit is használják. Egy futópár egy kicsit értékesebb lehet például, mint egy futó értékének kétszerese, és egy futó a végjátékban értékesebb, mint a nyitásnál.  ábra - Két, kissé eltérő sakktáblaállás. (a) Feketének egy huszár és két gyalog előnye van, és a játékot megnyeri. (b) Fekete veszít, miután Fehér a vezérét leüti. Két, kissé eltérő sakktáblaállás. (a) Feketének egy huszár és két gyalog előnye van, és a játékot megnyeri. (b) Fekete veszít, miután Fehér a vezérét leüti.
A keresés tehát a sikeres következtetést követően újra meghívja önmagát az eddigi {NSW=RED, WA=RED} behelyettesítéssel és az imént módosított KKP-vel. Ezt követően az első lépés egy újabb, még behelyettesítetlen változó kiválasztása. Az algoritmus a változók fix sorrendjét követve az NT változót választja, és ehhez az első lehetséges értéket, a GREEN-t rendeli (NT=GREEN).
Ha a környezet geográfiája ismeretlen, akkor nyílván érdemes annak feltérképezése és megtanulása, így elkerülve azt, hogy az ágens olyan mezőket feleslegesen látogasson meg, amelyeken már kitakarított. Kombinálható az előző problémával (azaz hogy tiszta mezők bepiszkolódhatnak), ekkor érdemes előbb feltérképezni (és közben megtisztítani) a környezetet, majd ezután foglalkozni az esetleges újra bekoszolódó mezőkkel!
A kisebb szinguláris értékekhez tartozó egyváltozós függvények elhagyásával (lásd  ábra) simító hatás érhető el, míg a nagy szinguláris értékekhez tartozó függvények elhagyása élkeresőt eredményez [1][4]. Az ortonormált mátrixok oszlopaiban levő függvényértékek közé további értékeket interpolálva a kép méretezésére kapunk hatékony eljárást (lásd  ábra) [1].  ábra -  ábra: Eredeti kép  ábra: Eredeti kép  ábra -  ábra: Nagyított kép  ábra: Nagyított kép
A LESH egy arcfelismerésre kifejlesztett módszer. Olyan kulcspontokat keres, ahol a frekvenciatartománybeli komponensek egy fázisban vannak. Az ilyen tulajdonságú pontokat Gabor wavelet transzformáció segítségével keresi. A területek jellemzésére a frekvenciatartományból számolható lokális energia hisztogramjait használja a különböző irányok mentén. A detektálandó és a megtanult képek összehasonlítását két részre bontja: első lépésben a fej pozícióját becsli egy osztályozó segítségével, majd ezt az információt felhasználva keres egyezést, így adva egy robusztusabb megoldást.
A következő lépés annak megértése, hogyan kell helyes döntéseket hozni. Nyilvánvalóan továbbra is azt a lépést szeretnénk kiválasztani, ami a legjobb álláshoz vezet. A lehetséges állások azonban most nem rendelkeznek egy jól meghatározott minimax értékkel. Ehelyett csak egy átlagos vagy várható értéket (expected value) tudunk kiszámítani, ahol az átlagolást az összes lehetséges kockadobásra végezzük el. Ezzel a determinisztikus játékok minimax értékét (minimax value) a véletlen csomópontokat tartalmazó játékok várhatóminimax értékére (expectiminimax value) általánosítottuk. A végállapotok és a max és min állapotok (ahol a kockadobás eredménye ismert) ugyanúgy viselkednek, mint eddig. A véletlen csomópontok kiértékeléséhez az összes lehetséges kockadobás figyelembevételével számított súlyozott átlagot kell venni, vagyis:
Az egyik algoritmus, amelyet a program futása során használ az alfa-béta vágás. Az algoritmus lényege, hogy leszűíti a lehetőségeket a tudás birtokának függvényében. Vagyis ha az ellenfélnek van olyan erős lépési lehetősége, ami miatt egy lépést mi biztosan nem játszanánk meg, akkor további vizsgálatok abban az irányban elvethetők, vagyis a keresési tér szűkíthető. Ezáltal jelentős memórianyereséget érhetünk el, nem beszélve arról, hogy a felesleges kiértékelésre nem pazarolunk időt [10].
Lényege, hogy a fenyegető kórfolyamatokat az egyéni genetikai eltérések alapján igyekszünk előrelátni, és ezek alapján tudunk mindent megtenni ezek megelőzéséért és/vagy késleltetéséért, a lehető legkorábban igyekszünk észlelni a kezdődő elváltozásokat, és a páciens egyéni sajátosságaihoz alkalmazkodva. Meg tudjuk választani azokat a beavatkozásokat, melyek lehetőség szerint normalizálják/stabilizálják a beteg állapotát, és csökkentik a további kockázatokat.
Az előző levezetés szemléletesen bemutatta a Gauss-eloszlásoknak azt az alaptulajdonságát, ami a Kalman-szűrés működését lehetővé teszi: azt a tényt, hogy az exponens négyzetes alakú. Ez nem csak az egyváltozós esetre igaz; a teljes többváltozós Gauss-eloszlás alakja:
A robot szót a cseh drámaíró, Karel Capek népszerűsítette 1921-ben írt R.U.R. (Rossum’s Universal Robots) c. színművével. Az ő robotjait nem mechanikusan szerelték össze, hanem kémiai úton állították elő. A műben a robotok megharagszanak mestereikre, és elhatározzák, hogy átveszik az uralmat. Úgy tűnik (Glanc, 1978), hogy valójában Capek bátyja, Josef alkotta meg a robot szót a cseh „robota” (robotolás) és „robotnik” (jobbágy) szó összevonásával, 1917-es Opilec c. novellájában.
A fogolydilemmát mint tantermi példát Albert W. Tucker alkotta meg és Axelrod tárgyalja részletesen (Axelrod, 1985). Az ismétlődő játékokat Luce és Raiffa vezette be (Luce és Raiffa, 1957) mint részleges információjú játékokat (Kuhn, 1953).
Folyékony és nem folyékony események közötti megkülönböztetés pontosan analóg a szubsztanciák, avagy az anyag és az egyedi objektumok közötti különbséggel. Tény, hogy egyes kutatók folyékony eseményeket temporális szubsztanciáknak (temporal substances) nevezték el, míg a vajszerű dolgok térbeli szubsztanciák (spatial substances).
A 8-királynő problémát először 1848-ban a Schach német sakkszaklapban névtelenül publikálták, később a problémát Max Bezzel nevéhez kapcsolták. A problémát 1850-ben újra publikálták, ami akkor felkeltette a kiváló matematikus, Karl Friedrich Gauss figyelmét, aki megkísérelte felsorolni az összes megoldást, azonban csak 72 megoldást talált meg. Az összes, 92 megoldást Nauck publikálta 1850-ben. A problémát Netto n királynő esetére általánosította (Netto, 1901), Abramson és Young pedig találtak egy O(n) algoritmust (Abramson és Young, 1989).
Az ilyen, és ehhez hasonló helyzetek modellezésére dolgozta ki Neumann János és Oscar Morgenstern a Játékelméletet. Kiindulási pontjuk főleg a közgazdaságtan volt, a vállalati versengés modellezése, és azon belül a helyes versenystratégia meghatározása. Mára azonban már szinte a legtöbb természettudományos területre (fizikába, biológiába, pszichológiába, matematikába, és természetesen a mesterséges intelligenciába is) beszivárgott ez az elgondolás. Közel 100 évre visszanyúló története többek között a következő főbb mérföldkövek köré csoportosul.
Közúton el lehet jutni A városból B városba. (Ez a térképen úgy fejezhető ki, hogy a két várost összeköti a megfelelő színű vonal, elsőrendű logikában azonban problémát jelent, ha különböző állomásokon keresztül tudunk csak eljutni az egyik városból a másikba.)
A listák (lists) tárgyköre nagyon hasonló a halmazok tárgyköréhez. A különbség az, hogy a listák rendezettek, és ugyanaz az elem több helyen is előfordulhat egy listában. Használhatjuk a Lisp szótárát a listákra: a Nil az elem nélküli listát jelölő konstans, a Cons, az Append, a First és a Rest függvények. A Find egy predikátum, amely azt hajtja végre a listáknál, mint az Eleme a halmazoknál. A List? egy predikátum, amely csak a listákra igaz. Úgy, mint a halmazoknál, a listákról szóló logikai mondatokban is szokás használni a szintaktikai édességet. Az üres lista a: [ ]. A Cons(x, y) termet, ahol az y nem üres lista, úgy írjuk, hogy: [x|y]. A Cons(x, Nil) termet (egy lista, amely tartalmazza az x elemet) úgy írjuk, hogy: [x]. Több elem listája, mint az [A, B, C], ugyanazt jelenti, mint az egymásba ágyazott term: Cons(A, Cons(B, Cons(C, Nil))). A  feladat tárgya az axiómák megírása a listák tárgyköréhez.
Egy online mélységi kereső ágenst a  ábra mutat. Ez az ágens a térképét egy eredmény[a, s] táblázatban tárolja, amely az s állapotban végrehajtott a cselekvés hatására előálló állapotot tartalmazza. Ha az aktuális állapotban marad még ki nem használt cselekvés, az ágens ezzel próbálkozik. A nehézség akkor áll be, ha az ágens egy állapotban már minden cselekvést kipróbált. Offline mélységi keresésnél ezt az állapotot a sorból egyszerűen elhagyjuk. Online keresésnél az ágensnek fizikailag kell visszalépnie. A mélységi keresésnél ez azt jelenti, hogy abba az állapotba kell visszakerülnie, amelyből az ágens legutóbb belépett a vizsgált állapotba. Ezt egy olyan táblázat karbantartásával lehet megoldani, amely minden állapot számára azokat az ősállapotokat listázza, melyekhez az ágens még nem lépett vissza. A keresés teljes, ha az ágens a viszszaléphető állapotokból kifut.
Egy terület feltérképezéséhez egy sztereorendszer használatát fontolgatjuk. A rendszer két CCD kamerából fog állni, mindegyik 512 × 512 pixel felbontású egy 10 × 10 cm négyzet alakú érzékelőn. A felhasználandó lencsék fókusztávolsága 16 cm, és a lencsék a végtelenben fixálnak. A bal oldali kép (u[1], v[1]) és a jobb oldali kép (u[2], v[2]) egymáshoz tartozó pontjaira v[1] = v[2], hiszen a két képsíkon az x tengelyek az epipoláris vonalakkal párhuzamosak. A két kamera optikai tengelye párhuzamos. A két kamera közötti bázisvonal 1 m. a. Ha a legközelebbi mérendő táv 16 m, mi a tapasztalható legnagyobb diszparitás (képpontban)? b. Mi a távmérés pixelfelbontásból adódó felbontása 16 m távolságban? c. Milyen táv tartozik az egy képpontos képeltéréshez?
Az n-gram modellek a korpuszon belüli közös előfordulási statisztikát használják ki, azonban nincs semmilyen információjuk a nyelvtanról n-nél nagyobb távolságra. A valószínűségi környezetfüggetlen nyelvtan – PCFG^[242] (probabilistic context-free grammar) – egy alternatív nyelvi modell, ami egy olyan CFG, melyben minden átírási szabályhoz valószínűséget rendelünk. Az azonos bal oldallal rendelkező szabályok valószínűségének összege 1. A  ábrán az ℰ[0] nyelvtan egy részletének PCFG-je látható.  ábra - Az ℰ[0] nyelvtan egy részletének valószínűségi környezetfüggetlen nyelvtana (PCFG) és szókincse. A szögletes zárójelben levő szám jelzi a valószínűségét annak, hogy az adott bal oldali szimbólumot a megfelelő szabály szerint írjuk át Az ℰ0 nyelvtan egy részletének valószínűségi környezetfüggetlen nyelvtana (PCFG) és szókincse. A szögletes zárójelben levő szám jelzi a valószínűségét annak, hogy az adott bal oldali szimbólumot a megfelelő szabály szerint írjuk át  ábra - A „Minden wumpus bűzlik” mondat elemzési fája, megadva minden egyes részfa valószínűségét. A teljes fa valószínűsége 1,0 × 0,5 × 0,05 × 0,15 × 0,60 × 0,10 = 0,000225. Mivel a mondatnak ez az egyetlen elemzése, ezért ennyi a mondat valószínűsége is. A „Minden wumpus bűzlik” mondat elemzési fája, megadva minden egyes részfa valószínűségét. A teljes fa valószínűsége 1,0 × 0,5 × 0,05 × 0,15 × 0,60 × 0,10 = 0,000225. Mivel a mondatnak ez az egyetlen elemzése, ezért ennyi a mondat valószínűsége is.
Gondoljunk arra a szóra, hogy „tomato (paradicsom)”. Gershwin szerint ezt úgy ejtik ki, hogy [t ow m ey t ow] (Gershwin, 1937), én viszont úgy ejtem, hogy [t ow m aa t ow]. A  ábra felső részén látható egy olyan állapotátmenet-modell, ami gondoskodik erről a változatról. A modellen keresztül csak két út létezik, egy, ami a [t ow m ey t ow] beszédhang szekvenciához tartozik, és a másik a [t ow m aa t ow] szekvenciához. Egy út valószínűsége az útvonalat alkotó nyilakon szereplő valószínűségek szorzata:
Az előálló {SA=RED, NSW=GREEN, NT=GREEN, WA=BLUE} behelyettesítés immár konzisztens, mehetünk tovább. Azonban még mielőtt tovább haladnánk, figyeljük meg, hogy az előző szakaszban tárgyalt sima visszalépéses kereséshez képest most ennél a problémánál a fokszám heurisztikának köszönhetően jóval kevesebb ellentmondásba ütköztünk. Nem csak, hogy gyorsabban közelítünk a probléma megoldásához, hanem egy-egy lépésben legfeljebb egy korlátnak mondtunk eddig ellent, míg a sima visszalépéses keresés során, láthattuk, néha 2-3 korlát is sérült. Ez is még tovább gyorsítja az algoritmus végrehajtását, még hogyha esetleg ez a gyorsulás nem is mérhető az algoritmus által megtett lépések számában. Az algoritmus szoftveres megvalósítása viszont nyilván gyorsul, ha kevesebb korláttal történik ütközés, és így kevesebb adminisztrációra van szükség.
Az MR-vizsgálatot eredetileg nem képalkotásra, hanem olyan kíméletes anyagelemzési célra dolgozták ki, amelynek során a minta nem károsodik. Legalább 1,5 Tesla mágneses térerejű berendezésekkel a szövetek anyagi összetétele és anyagcseréje az élő szervezet bármely részében, mintavétel nélkül, kémiailag elemezhető. A technika széleskörű klinikai alkalmazása még várat magára.
Általános esetben a komplexitás O(n^2d^3), mivel a gráfnak legfeljebb O(n^2) éle van. Egy fastruktúrájú gráfnak ezzel szemben legfeljebb n-1 éle lehet, ahol n a csúcsok száma, hiszen ellenkező esetben biztosan lenne legalább egy kör a gráfban. Így tehát a komplexitás legrosszabb esetben O((n-1)d^3). Ezzel a legrosszabb eset n szerint négyzetes időről n szerint lineárisra csökkent!
Erre példa a  (a) ábra hexapodja, azaz hatlábú robotja, amelyet nehéz terepen való közlekedésre terveztek. A robot szenzorai többnyire alkalmatlanok arra, hogy megfelelő pontosságú modellt alkossanak a környezetről, hogy a robot a korábban tárgyalt pályatervezési algoritmusok bármelyikét használhassa. De hiába látnánk el megfelelő szenzorokkal, a tizenkét szabadságfok (kettő lábanként) túl nagy számítási kapacitást igényelne pályatervezés esetén.  ábra - (a) Hexapod robot. (b) Kiterjesztett véges automata (KVA) (augmented finite state machine, AFSM) egyetlen láb vezérléséhez. Fontos, hogy ez a KVA-szenzoros visszacsatolásra reagál: ha a láb megakad az előremozgatás során, akkor magasabbra emeli. (a) Hexapod robot. (b) Kiterjesztett véges automata (KVA) (augmented finite state machine, AFSM) egyetlen láb vezérléséhez. Fontos, hogy ez a KVA-szenzoros visszacsatolásra reagál: ha a láb megakad az előremozgatás során, akkor magasabbra emeli.
A látást nemcsak objektumok, hanem cselekvések felismerésére is használjuk. Felismerhetünk járásmódokat (egy barát járását), kifejezéseket (egy mosolyt, egy grimaszt), gesztikulációt (egy integető embert), cselekvéseket (ugrást, táncot) és így tovább. A cselekvésfelismerést célzó kutatás még gyerekcipőben jár, így ebben az alfejezetben az objektumfelismerésre koncentrálunk.
A fügvényapproximációt használó megerősítéses tanulás konvergenciája kimondottan technikai kérdés. Lineáris approximáló függvények alkalmazása esetén az IK tanulási eredmények folyamatosan javulnak (Sutton, 1988; Dayan, 1992; Tsitsiklis és Van Roy, 1997), de nemlineáris függvények esetén számos divergenciát mutató példát találtak (a jelenség tárgyalását lásd a (Tsitsiklis és Van Roy, 1997)-ben). Papavassiliou és Russell a megerősítéses tanulás új formáját adták (Papavassiliou és Russell, 1999), amely tetszőleges struktúrájú függvényapproximátor esetén konvergál, feltéve, hogy található a megfigyelt adatokra egy legjobban illeszkedő approximáció.
7.10 Minden logikai mondat logikailag ekvivalens azzal az állítással, hogy minden lehetséges világ, amelyben hamis volna, nem az eset. Felhasználva ezt a megfigyelést, bizonyítsa be, hogy minden mondat átírható konjugált normált formára.
Azt az álláspontot, amit kritizáltak, Haugeland (Haugeland, 1985) nyomán elkezdték „Jófajta, régivágású MI”-nek nevezni (angol rövidítéssel: gofai). A gofai azt feltételezné, hogy minden intelligens viselkedés megragadható egy olyan rendszerrel, amely a tárgyterületet leíró tények és szabályok halmazaiból kiindulva logikai következtetést végez. Ezért a gofai a  fejezetben leírt legegyszerűbb logikai ágensnek felel meg. Dreyfusnak igaza van abban, hogy a logikai ágensek ki vannak téve a kvalifikációs probléma veszélyének. Amint a  fejezetben láttuk, a nyílt tárgyterületekhez sokkalta megfelelőbbek a valószínűségi következtető rendszerek. Dreyfus kritikája tehát nem per se a számítógépek, hanem csak a programozás egy meghatározott módja ellen irányul. Okkal feltételezhetjük persze, hogy nem lenne túl nagy hatású egy könyv Amire az elsőrendű logikai szabályalapú, nem tanuló rendszerek képtelenek címmel.
Az agyprotézis érv fontos része, hogy képesnek kell lennünk a kísérleti személy agyát visszaállítani a normális állapotába, hogy külső viselkedése ugyanolyan legyen, mintha a műtét nem is történt volna meg. Megalapozott lehet-e egy olyan szkeptikus ellenvetés, miszerint ez azzal járna, hogy módosítani kell a neuronoknak a tudatossághoz kötődő neurofiziológiai tulajdonságait, melyek különböznek a funkcionális viselkedésben résztvevőktől.
Vegyük észre, hogy a nyelvtan nagyon szigorú a zárójelezésnél: minden mondatot, amelyet bináris összekötőjellel hozunk létre, zárójelek közé kell tenni. Ez biztosítja, hogy a szintaxis teljesen egyértelmű. Ez azt is jelenti például, hogy ((A ∧ B) ⇒ C)-t kell írjunk A ∧ B ⇒ C helyett. Az olvashatóság javítása céljából gyakran elhagyjuk a zárójeleket, megbízva ehelyett az összekötőjeleknek egy precedencia-sorrendjében. Ez hasonló az aritmetikában alkalmazott precedenciához – például az ab + c-t ((ab) + c)-nek olvassuk, és nem a(b + c)-nek, mert a szorzásnak magasabb a precedenciája, mint az összeadásnak. Az ítéletkalkulus precedencia-sorrendje (a legmagasabbtól a legalacsonyabb felé: ¬, ∧, ∨, ⇒ és ⇔. Így a mondat:
 ábra - 8. lépés: az előbbi, NT=BLUE értékadás következményeként ellentmondásra jutottunk, mivel Q értékkészlete üresre redukálódott az AC3/MAC következtetés során 8. lépés: az előbbi, NT=BLUE értékadás következményeként ellentmondásra jutottunk, mivel Q értékkészlete üresre redukálódott az AC3/MAC következtetés során
Alkalmazási területek * Dokumentumosztályozás – a hagyományos osztályozási feladatok egyike: a szabadszöveges dokumentumokat előre definiált diszjunkt halmazokba kell sorolni. * Web keresés – előzőhöz hasonló feladat: pl.: vállalatok számár sokszor fontos, hogy egy kategóriába eső híreket naprakészen olvashassák. * Információkivonatolás (information extraction) – szabadszöveges dokumentumokban szavak annotálása (címkézése). * Email filtering – a nem kívánt leveleket el kell távolítani a postafiókból. * Relevance feedback – a tanulórendszer a felhasználóknak visszajelzést ad arról, hogy pl.: egy film adatbázis, vagy egy webáruház egyes elemei mennyire relevánsak számára. * Rákkutatásban – a tanulórendszer feladata megmondani, hogy mely biológiai jellemzők (biomárkerek) felelősek a rák kialakulásáért. * Betegségek szimptómáinak meghatározásánál – a mezőgazdaságban előforduló probléma, hogy pl.: fák esetén a betegség egy igen költséges eljárással állapítható csak meg. A cél, hogy kiválasszuk azokat a könnyen diagnosztizálható klinikai jellemzőket, melyek alapján a betegség ténye könnyebben meghatározható.
Például egy bináris Y  esetében célunk lehet, egy g( D N ,X)=Y  ( g:{ ℝ d x{0,1}x…x ℝ d x{0,1}x ℝ d }→{0,1}  döntési szabály megkonstruálása. Egyenletes veszteségfüggvény esetén ( L(0,1)=L(1,0)  ), akkor optimális döntést ad a következő indikátorfüggvény 1(0.5≤p(Y=1| D N ,X))  . Érdekes, hogy ugyanez az adat alapja lehet egy regressziós problémának is a következőképpen. Értelmezzük az f( x _ , ω _ ):{ ℝ d }→[0,1]  függvényeket mint a P(Y=1| x _ , ω _ )  feltételes valószínűségeket. Ekkor E P(Y| x _ , ω _ ) [Y|ω,x]=f( x _ , ω _ )  és a kimeneti érték értelmezhető így:
Mindkét agyállapot-azonosság elméletet, valamint a különböző formájú funkcionalizmusokat támadják azok a szerzők, akik szerint ezek az elméletek nem megfelelők a mentális állapotok qualia, azaz „milyen érzés is ez” aspektusának megmagyarázására (Nagel, 1974). Searle ehelyett az intencionalitás funkcionalizmusbeli állítólagos megmagyarázhatatlanságára koncentrált (Searle, 1980; 1984; 1992). Churchland és Churchland (Churchland és Churchland, 1982) mindkét fajta kritikát visszautasítják.
Egy logikai döntési fa által kezelhető példa a bemeneti attribútumok X vektorából és egyetlen logikai kimeneti értékből, y-ból áll. A  ábra egy (X[1], y[1])...( X[12], y[12]) példahalmazt mutat. Azokat nevezzük pozitív példáknak (X[1], X[3], ...), amelyekben a VárjunkE értéke igaz, és azok a negatív példák, amelyekben az értéke hamis (X[2], X[5], …). A példák teljes halmazát tanító halmaznak (training set) nevezzük.
A robotika tulajdoképpen nem más, mint intelligens ágensek, amelyek képesek megváltoztatni a fizikai világot. Ebben a fejezetben a következő alapvető ismereteket sajátítottuk el a robothardverrel és -szoftverrel kapcsolatban. * A robotok szenzorokkal vannak felszerelve, hogy érzékelhessék az őket körülvevő világot, és beavatkozó szervekkel, hogy fizikai erőkkel hathassanak környezetükre. A legtöbb robot vagy manipulátor, amely egy adott helyhez van rögzítve, vagy mobil robot, ami képes mozogni. * A robotikai érzékelés a döntéshez kapcsolódó mennyiségeknek a szenzoros adatokból való becslése. Ehhez szükségünk van egy belső reprezentációra, valamint egy olyan módszerre, amivel azt időnként frissíthetjük. A nehéz érzékelési problémák gyakori példái a helymeghatározás és a térképezés. * Valószínűség-alapú szűrő algoritmusok, mint például a Kalman-szűrők és a részecskeszűrők hasznosak a robotérzékelés szempontjából. Ezek a technikák frissítik a belső világképet, például az állapotváltozók a posteriori eloszlását. * A robot mozgásának tervezése általában a konfigurációs térben történik, ahol minden pont egyértelműen meghatározza a robot egy adott pozícióját és orientációját, csuklóinak szögét. * A konfigurációs térben kereső algoritmusok között fontos megemlíteni a celladekompozíciót, amely a teljes konfigurációs teret véges sok cellára bontja fel, valamint a szkeletonizációs technikákat, amelyek az adott konfigurációs teret egy alacsonyabb dimenzióba vetítik le. A mozgástervezés problémáját ezután már egyszerűbb struktúrákban való kereséssel is meg lehet oldani. * A kereső algoritmus által megtalált pályán a robot akkor tud végighaladni, ha egy PID szabályozót teszünk a rendszerbe, aminek referenciapályája a kívánt útvonal. * A potenciáltér-alapú technikák a céltól és az akadályoktól való távolság szerint definiált potenciálfüggvények segítségével navigálják a robotot. Ezek a technikák megakadhatnak lokális minimumhelyeken, de képesek mozgásgenerálásra előzetes tervezés nélkül. * Néha egyszerűbb egy robotszabályozót közvetlenül megtervezni, mint a pályát a környezet explicit modelljéből levezetni. Egy ilyen szabályozót legtöbbször leírhatunk véges automataként. * Az alárendelt architektúra a programozók számára lehetővé teszi, hogy belső órajellel kiegészített véges automaták összekapcsolásával robotvezérlőket alkossanak. * A népszerű háromrétegű architektúra olyan robotszoftver fejlesztéséhez is keretet ad, amely integrálja a modellalapú tervezést, a részfeladatokra bontást és a szabályozást. * Léteznek feladatspecifikus robotprogramozási nyelvek, amelyek megkönnyítik a robotszoftverek fejlesztését. Ezek a nyelvek kész szerkezetekkel segítik a többszálú programok írását, a szabályozási irányelveknek a tervezésbe történő integrálását és a tapasztalatalapú tanulást.
Egészségügy. A sebészorvosok munkáját segítendő egyre többször használnak robotokat az orvosi eszközök pontos mozgatásához olyan kényes szerveket érintő műtétek esetében, mint az agy, a szív vagy a szem. A  (b) ábra egy ilyen rendszert mutat be. A robotok – nagy pontosságuknak köszönhetően – nélkülözhetetlen eszközökké váltak bizonyos csípőprotézisek beültetésénél. Pilottanulmányok kimutatták, hogy végbéltükrözés esetén a robotizált eszközök használata csökkenti a sérülés veszélyét. A műtőn kívül, a kutatók megkezdték olyan robotok kifejlesztését, amelyek idős vagy fogyatékos emberek segítségére lehetnek, mint például az intelligens robotizált járógépek vagy intelligens játékok, amelyek figyelmeztetnek a gyógyszerek bevételére.
 ábra - Címlap Címlap  ábra - Intelligencia elméletek – sok a jóból Intelligencia elméletek – sok a jóból  ábra - Intelligencia mérése Intelligencia mérése  ábra - Egy kis intelligencia teszt Egy kis intelligencia teszt  ábra - és a megoldás kibontakozása és a megoldás kibontakozása  ábra - Teljes megoldás Teljes megoldás  ábra - Bongard diagramok Bongard diagramok  ábra - és a feladvány és a feladvány  ábra - Intelligencia - mérnökül Intelligencia - mérnökül  ábra - Mesterséges intelligencia, mint empirikus tudomány Mesterséges intelligencia, mint empirikus tudomány  ábra - Miért kell egy intelligens gép? Miért kell egy intelligens gép?  ábra - Mesterséges intelligencia mérnöki megközelítésben Mesterséges intelligencia mérnöki megközelítésben  ábra - Mikor mit fogunk tanulni Mikor mit fogunk tanulni  ábra - A fő olvasmányok A fő olvasmányok  ábra - Tipikus követelmények Tipikus követelmények  ábra - MI gyökerei MI gyökerei  ábra - Filózofusok Filózofusok  ábra - Logikusok Logikusok  ábra - Logikusok2 Logikusok2  ábra - Logikusok3 Logikusok3  ábra - Komplexitás elmélet Komplexitás elmélet  ábra - Valószínűségelmélet Valószínűségelmélet  ábra - Pszichológia Pszichológia  ábra - Pszichológia2 Pszichológia2  ábra - Számítógép Számítógép  ábra - Számítógép2 Számítógép2  ábra - Számítógép3 Számítógép3  ábra - Számítógép4 Számítógép4  ábra - Számítógép5 Számítógép5  ábra - Számítógép6 Számítógép6  ábra - Nyelvészet Nyelvészet  ábra - MI kezdete MI kezdete  ábra - MI kezdete2 MI kezdete2  ábra - MI kezdete3 MI kezdete3  ábra - MI kezdete4 MI kezdete4  ábra - Hidegzuhany Hidegzuhany  ábra - Komplexitások Komplexitások  ábra - Komplexitások2 Komplexitások2  ábra - Komplexitások3 Komplexitások3  ábra - Komplexitások4 Komplexitások4  ábra - Komplexitások5 Komplexitások5  ábra - Komplexitások6 Komplexitások6  ábra - Komplexitások7 Komplexitások7  ábra - Komplexitások8 Komplexitások8  ábra - Tudásalapú rendszerek Tudásalapú rendszerek  ábra - Ma Ma  ábra - Ma2 Ma2  ábra - Ma3 Ma3  ábra - Ma4 Ma4  ábra - Racionális ágens Racionális ágens  ábra - Alkalmazások Alkalmazások  ábra - Alkalmazások2 Alkalmazások2  ábra - Alkalmazások3 Alkalmazások3  ábra - Alkalmazások4 Alkalmazások4
A lentről felfelé történő elemzés keresésként történő formalizálása a következő: * A kezdeti állapot (initial state) a bemeneti füzérben található szavak listája, mindegyiket egy olyan levezetési faként ábrázolva, melynek csak egy levele van, például: [the, wumpus, is, dead]. Általánosságban a keresés minden állapota levezetési fák egy listája. * Az állapotátmenet-függvény (successor function) megvizsgál minden i pozíciót a fák listájában a nyelvtan szabályainak minden lehetséges jobb oldalán. Ha a lista i pozíciójában kezdődő részsorozata illeszkedik a jobb oldalra, akkor a részsorozatot lecseréli egy új fára, amelynek kategóriája a szabály bal oldala, és amelynek gyerekei a részsorozat. „Illeszkedés” alatt azt értjük, hogy a csomópont kategóriája megegyezik a jobb oldal elemének kategóriájával. Például az Article → the szabály illeszkedik a [the, wumpus, is, dead] első csomópontjából álló részsorozatra, így a következő állapot az [[Article: the], wumpus, is, dead] lenne. * A célteszt (goal test) egy olyan állapotot keres, ahol egyetlen fa van, melynek gyökere az S.
A leghatékonyabb kockázatcsökkentés az elsődleges és másodlagos prevenció esetében a legnyilvánvalóbb  ábra - Kockázatelemzés lépései Kockázatelemzés lépései 1. táblázat - Kockázatok és menedzselési módok
Az elvégzett munkának értelme nincs, ha a nyert adatok nem kerülnek egy aneszteziológiai célokat kiszolgálni akaró adatbankba. A bank állományának kiértékelésével eleget tudunk tenni az éves adatszolgáltatási kötelezettségeinknek ugyanúgy, mint a minőségbiztosítás kívánalmának és nem utolsó sorban követelményének.
Fenti példán is látszik, hogy az adatot leíró információk nem nagyon hozhatóak kapcsolatba a megszokott adattípusokkal, a félreértések elkerülése végett vezették be az úgynevezett Sémákat. Fontos Az RDF sémák segítségével lehetőségünk van, hogy a szolgáltatásunknak megfelelő osztályokat és tulajdonságokat határozhassunk meg.[9]
A példában fényceruzát egyaránt használtuk arra, hogy a rajz egyes elemeit elhelyezzük (újak létrehozásánál), illetve rámutattunk meglévő részekre. Érdekesség, hogy a Terminate parancsot nem egy billentyű segítségével adhatjuk ki, hanem az említett beviteli eszközt hirtelen elkapásával a befejező pontban, gyorsabban, mint ahogy a számítógép követni tudná, ahogy az a [1] videón is látszik. A fényceruza használatán felül találkoztunk a subpicture (alkép), constraint (kikötés, megszorítás) illetve copying (másolás) fogalmával. Az alkép tulajdonképpen bármi lehet, a hatszögtől kezdve egy tranzisztoron át a repülőgép szárnyáig, bármennyi szimbólum rajzolható, akármilyen gyakran használhatjuk ezeket. Ami a megszorításokat illeti, egy ilyet definiáltunk, amikor megadtuk, hogy a hatszög csúcsai a köríven legyenek. Ezek az alapvető viszonyok (atomi kikötések) tesznek egyeneseket függőlegessé, vízszintessé, párhuzamossá, metszőket derékszögűvé, pontokat egyéb alakzatra illeszkedővé, alakzatokat függőlegessé, egymás alá rendezetté vagy azonos méretűvé, valamint megadja, hogy minden rajzolt objektum a pontból és az egyenesből származik. További, különleges megkötések szabadon felvehetők. A másolás során a mintapéldában megadtuk, hogy a hatszög oldalai legyenek egyforma hosszúak, ugyanezzel a nemes egyszerűséggel adhattuk volna meg azt, hogy párhuzamosok legyenek vagy derékszögben messék egymást. Ezeket az alapvető formákat a szoftver mindig megőrzi, ezt kisegítve kerülnek letárolásra az objektumaink. Amikor például a hatszög egyik csúcsát a körvonalra mozgattuk, a sokszögünk végig hatszög maradt, nem sérült ez a tulajdonsága, az élek követték az őket meghatározó csúcspontot. Miután pedig megmondtuk, hogy ezen csúcspontok mindegyikének a körvonalon kell lenniük, nem is mozdultak onnan a további változtatások hatására. Fontos megjegyezni, hogy a Sketchpad mindig az alképek formáját tárolja, azaz a példán bemutatott méhsejtrácsról azt tudjuk, hogy 7 darab hatszögből áll, amiről azt tudjuk hogy hatszögekből áll. Tehát ha megváltoztatjuk a kiindulási ábránkat egy félkörre, akkor halpikkely szerű elrendezést fogunk viszontlátni, amiben ugyan módosult az alapkomponens, de az ábra elrendezése nem.
formában adható meg. Nyilvánvaló, hogy bármely hiedelmi állapot pontosan befoglalható egyetlen logikai mondatba. Ha akarjuk, az összes konjuktív állapotleírás diszjunkcióját vehetjük, de a példánk mutatja, hogy ennél tömörebb mondatok létezhetnek.
Az ágensprogram, amelyet a  ábra mutat, Kijelenti a tudásbázisnak az összes szellő és bűz érzetet. (Ezenkívül frissíti néhány hagyományos változójának értékét is, amelyek azt őrzik, hogy az ágens merre van most és merre járt korábban – erre később még visszatérünk). Ezután a program kiválasztja, hogy melyik cellát nézze meg a peremen lévők közül – olyan négyzetek közül, amelyek szomszédosak valamelyik meglátogatottal. Egy peremen lévő [x, y] cella bizonyíthatóan biztonságos, ha a (¬C[i,j] ∧ ¬W[i,j]) mondat vonzata a tudásbázisnak. A „második legjobb dolog”, ha egy cella lehetséges, hogy biztonságos, amikor az ágens nem tudja bizonyítani, hogy ott egy csapda vagy egy wumpus van – azaz, hogy a (C[i,j] ∨ W[i,j]) nem vonzat.
Ha már az időben evolváló hiedelmeket le tudjuk írni, felhasználhatjuk az eseménykalkulus mechanizmusát, hogy hiedelmeket tartalmazó terveket készítsünk. A cselekvéseknek lehetnek tudás-előfeltételei (knowledge preconditions) és tudáshatásai (knowledge effects). Annak a cselekvésnek például, hogy valakinek a telefonszámát tárcsázzuk, az az előfeltétele, hogy tudjuk a számot, és annak a cselekvésnek, hogy a telefonszámot a könyvből kikeressük, az a hatása, hogy a számot tudni fogjuk. Ezt az utolsó cselekvést az eseménykalkulus mechanizmusával az alábbi módon írhatjuk le:
Sajnos, a rezolúció csak nem konstruktív bizonyítást (nonconstructive proofs) tud előállítani az egzisztenciális célmondatokra. Például a ¬Megöli(w, Tuna) rezolválható a Megöli(Jankó, Tuna) ∨ Megöli(Kíváncsiság, Tuna)-val, hogy a Megöli(Jankó, Tuna) mondatot megkapjuk, amely megint rezolválható ezzel: ¬Megöli(w, Tuna), és így kapjuk meg az üres klózt. Figyeljük meg, hogy ebben a bizonyításban a w-nek két különböző lekötése van. A rezolúció megmutatja azt, hogy igen, valaki megölte Tunát – vagy Jankó, vagy a Kíváncsiság. Ez nem túl meglepő! Egy lehetséges megoldás korlátozni a rezolúciós lépéseket úgy, hogy a kérdés változóinak csak egy lekötésük lehessen egy adott bizonyításban, majd képesnek kell lennünk arra, hogy visszakövessük a lehetséges lekötéseket. Egy másik megoldás egy speciális válasz literál (answer literal) hozzáadása a negált célhoz, amelyből ez lesz: ¬Megöli(w, Tuna) ∨ Válasz(w). Ilyenkor a rezolúciós folyamat mindig generál egy pontosan egy literált tartalmazó választ, amikor egy ilyan klóz generálódik. A  ábrán látható bizonyításra ez a Válasz(Kíváncsiság).A nem konstruktív bizonyítás ezt a klózt generálná: Válasz(Kíváncsiság) ∨ Válasz(Jankó), amely nem ad számunkra választ.  ábra - Egy rezolúciós bizonyítása annak, hogy a Kíváncsiság ölte meg a macskát. Figyeljük meg a faktorálás használatát a Szereti(G(Jankó), Jankó) klóz származtatásánál. Egy rezolúciós bizonyítása annak, hogy a Kíváncsiság ölte meg a macskát. Figyeljük meg a faktorálás használatát a Szereti(G(Jankó), Jankó) klóz származtatásánál.
A Foil-t és társait számos definíció megtanulására vetették be. Az egyik leghatásosabb demonstráció (Quinlan és Cameron-Jones, 1993) a listakezelő függvényekre vonatkozó feladatok hosszú sorának a megoldása volt Bratko Prolog-tankönyvéből (Bratko, 1986). A program mindegyik esetben képes volt a függvény helyes definícióját kisszámú példa alapján megtanulni, miközben háttértudásként használta a korábban megoldott függvényeket.
Egyelőre fennáll, hogy a robot minden esetben oda jut egy mozgási cselekvés után, ahova indult, tehát újratervezéssel és eltévedéssel egyelőre nem kell foglalkozni. A robot először megfigyeli a környezetét; ez alapján megállapítja, hol van; ezt az információt felhasználva készít egy tervet (az előző feladat szerinti módon) arra, hogy jusson el a jelenlegi helyzetéből a célállapotba, majd a megoldásként kapott cselekvéssorozatot végrehajtja. Ha a cselekvéssorozatot befejezte (a cselekvéseket tartalmazó sor üres), visszatér a környezet megfigyeléséhez, és így tovább.
1981-ben Japán bejelenti az 5. generációs japán projektet, mely projekt alapvetően az intelligens számítástechnikai rendszerek megvalósítását tűzte ki céljául [7] [15]. Terveik szerint 10 év alatt 10 milliárd dollárt áldoznának rá, hogy beváltsa a nem titkolt célját: a dominancia átvételét Európától. Az anyagban több magyar vonatkozás is említésre került. Azt lehet elmondani erről a projektről, hogy ugyan nem volt sikeres, de mellékhatásaként jó dolgok születtek, mert mind az Egyesült Államok, s mind Európa komolyan vette a kihívást, ezért létrehoztak olyan szervezeti kereteket, amelyek során akár sok nemzetet tartalmazó projektek indítása is lehetővé vált.  ábra - Szeredi Péter Szeredi Péter
Hogy megértsük, miért is működik a valószínűségi súlyozás, azzal kezdjük, hogy megvizsgáljuk a Súlyozott-Minta S[WS] mintavételi eloszlását. Emlékezzünk, hogy az E bizonyítékváltozók értéke (e) rögzített. A többi változót Z-vel jelöljük, azaz Z = {X} ∪ Y. Az algoritmus Z minden változóját mintavételezi, szülei adott értékei mellett:
Sokak szerint a Mesterséges Intelligencia legfőbb célja, hogy a robotokat és programokat az ember számára elfogadhatóvá tegyük, ne csak mint egy munkaeszköz, hanem mint társ is. Ennek elengedhetetlen feltétele, hogy mint minden embernek, ezeknek is legyen humorérzékük. Ez az ipar és a kutatások sok területén hasznos lenne. Elviselhetőbbé tennék a hosszú űrutazásokat az űrhajósok számára, és jobb munkamorált biztosítanának az olyan dolgozóknak, akik automata rendszerekkel és robotokkal dolgoznak nap mint nap. Egy ilyen robot létrehozása azonban ma még rendkívül komplex feladatnak tűnik. A rendszernek ismernie kéne az összes gyakori viccet, ami egy beszélgetés során előkerülhet, vallástól és kultúrától függetlenül. Ezen kívül a rendszernek képesnek kellene lennie rá, hogy felismerje, ha neki egy viccet mesélnek, és arra egy másik viccel reagálnia kéne. Ezt a viccet úgy kéne kiválasztania, hogy a legkisebb legyen a valószínűsége annak, hogy a beszélgetőpartner már hallotta a viccet, illetve el kéne tárolnia minden egyes emberhez, hogy milyen viccet mesélt nekik, illetve tőlük miket hallott a beszélgetések során. Mindez mai szemmel nézve még nagy feladatnak tűnik, de a technika rohamos fejlődése miatt valószínűleg a nem is olyan távoli jövőben mindez megvalósítható lesz. [6]
A felismerő algoritmusok elkülönülnek aszerint, hogy mennyire specifikusak az adott objektumra nézve, más szóval mennyire rugalmasak és általánosak. Léteznek egy adott – jellemzően egyszerűbb, például kör – alakzatra kifejlesztett algoritmusok, ilyen például a Hough, vagy a Radon transzformáció. Általánosabb megoldást jelentenek azok az algoritmusok, melyeknél a módszer és a működés alapelve kötött, de a konkrét alakzat nem ismert előre, hanem egy tanulási fázisban kerül felhasználásra. Ezeknél a módszereknél elég csak azt ismerni előre, hogy a felismerni kívánt alakzatok milyen tipikus tulajdonságokkal rendelkeznek, például éles határvonalúak, jól elkülönülő sarkokkal rendelkeznek. A legelterjedtebb képviselőjük a Scale Invariant Feature Transform (SIFT), de ide tartozik a Bag of Words algoritmus is. Végül a felismerési feladatokra alkalmazhatók közvetlenül a teljesen általános gépi tanulási módszerek is, ahol az objektumról készült képet némi előfeldolgozás – például simítás, élkiemelés, jellemző számítás – után megtanítják például egy Szupport Vektor Gépnek (SVM).
Kétdimenziós érzékelőt tartalmaznak a CCD videokamerák, illetve a hagyományos, csöves kamerák. Itt a kép minden egyes pontjához külön érzékelő tartozik. A képalkotás egyidejű, de az egyes pontokhoz tartozó adatok kiolvasása sorban történik. 4. Az érzékelés színessége alapján:
Multi Expression Programmin (MEP) segítségével konstans és összetett kifejezéseket felvéve MEP stringként géneket építünk, ezeket kromoszómákba szervezik. Nim játék esetén a kifejezések a halmok elemszámai, a rajtuk elvegezhető lehetséges műveletek +,-,*,div,mod,xor,and,or ,not. Ezen operátorok és operandusok segítségével írhatjuk le a játék egyes állapotait (többek között a P és N állásokat). Például a nyerő, vagyis P pozíciókat a következő egyenlet írja le:
Az  fejezet a racionalitás koncepciójának gyökereit a filozófiára és közgazdaságtanra vezette vissza. Az MI-ben a nyolcvanas évek közepéig ez a koncepció csak periferiális érdeklődést kapott, amikor elkezdte elborítani a terület megfelelő technikai megalapozásának tárgyalásait. Jon Doyle cikke azt jósolta, hogy a racionális ágensek tervezése képezi majd az MI fő küldetését, míg más népszerű területek leválnak új tudományágakat formálva (Doyle, 1983).
Az RMMDF-ekben a fogalmat kissé finomítjuk. Egy b hiedelmi állapot most egy valószínűség-eloszlás lesz az összes lehetséges állapot felett. Például a  (a) ábra kezdeti hiedelmi állapota úgy írható, hogy  . A b hiedelmi állapot által az aktuális i állapothoz rendelt valószínűséget b(s)-sel jelöljük. Az ágens úgy számíthatja ki a jelenlegi hiedelmi állapotát, mint egy feltételes eloszlást az aktuális állapotok felett, az eddigi megfigyelések és cselekvések sorozatának ismeretében. Ez lényegében a szűrési (filtering) feladat (lásd  fejezet). A rekurzív szűrési alapegyenlet (lásd   egyenlet a 15. szakasz - Szűrés és előrejelzés részben) megmutatja, hogy hogyan számolhatjuk ki az új hiedelmi állapotot az előző hiedelmi állapotból és az új megfigyelésből. Az RMMDF-eknél a jelölés eltérő, és meg kell gondolni még a cselekvést is, de az eredmény lényegében ugyanaz. Ha b(s) volt az előző hiedelmi állapot, és az ágens az a cselekvést hajtja végre és az o megfigyelést érzékeli, akkor az új hiedelmi állapot a következő:
Tehát végső soron a fentebb leírt módon integráltuk a tankönyv 5.3-as ábráján látható sima visszalépéses keresés FOR-ciklusába a KÖVETKEZTETÉS-t. Még egyszer: a KÖVETKEZTETÉS-t közvetlen az értékadás után hívjuk meg, és amennyiben HAMIS értéket ad, vagy ha nem, akkor ha az utána rekurzíve meghívott visszalépéses keresés tér vissza HAMIS értékkel, akkor visszacsináljuk a változtatásokat (az értékadásét és a következtetését), és tovább lépünk a FOR-ciklusban az aktuálisan vizsgált változó következő értékére. Tehát végső soron csak minimális mértékben módosítottuk/bővítettük az eddigi visszalépéses keresési algoritmust.
Csak részlegesen említettem az áramkör-tervezés kapcsán, hogy a Sketchpad óriási felhasználási területe, amikor a kimenetét más, modellező program bemeneteként használjuk. Ilyen például hidak statikájának az elemzése, ahol megadhatjuk az egyes szakaszok (gerendák) elhelyezkedését, hosszát, a rájuk ható erőt, és az alkalmazásunk kiszámolja az egymásra hatásokat.
Van egy másik probléma is, amely mellett eddig óvatosan elmentünk: a hurokmentesség (acyclicity) kérdése. Az áramkört hurokmentesnek tekintjük, ha minden olyan ág, amely egy regiszter kimenetét visszacsatolja a bemenetére, tartalmaz legalább egy közbülső késleltető elemet. Megköveteljük, hogy minden áramkör hurokmentes legyen, mivel a hurkokat tartalmazó áramkörök, mint fizikai eszközök, nem működnek! Az ilyen áramkör instabil oszcillációba kerülhet határozatlan értékeket eredményezve. Példaként a hurkot tartalmazó áramkörökre, tekintsük a   egyenletnek a következő kibővítését:
A következő ábra jelölései szerint a P[1]P[2] és P[3]P[4] szakaszok egyetlen közös pontja legyen P[3]. A csatlakozó szakaszoktól egyenlő távolságra a szakaszok által alkotott szög szögfelezője van. A keresett R(λ) függvény a GVD P[1]P[2] szakasztól mérhető távolságát adja meg a szakasz P[1 ]végpontjától mért λ távolság függvényében. A  ábrán X[1] a GVD tetszőleges pontját, X[2] pedig e pont merőleges vetületét jelöli.
A ROC görbe alkalmazható több diagnosztikai teszt összehasonlítására is, mikor ugyanazon betegség diagnosztizálására többfajta laboratóriumi teszt eredményei ismertek. Ilyenkor a különböző tesztek eredményei egy ROC ábrán jeleníthetők meg, a görbék egymáshoz és az egységnégyzetbeli elhelyezkedésük alapján hasonlíthatók össze.
Az elterjesztése Martin Gardner nevéhez fűződik, aki cikkével nagy publicitásnak tette ki a játékot az 1950-es évek végén. Ennél valamivel korábban Parker Brothers nevezte úgy először a játékot, hogy Hex. Később ebből kifolyólag ez a név meg is ragadt.
Miután az általános ontológiát megalkottuk, felhasználjuk az internetes bevásárlás tárgytartományra. Ez a tárgytartomány több mint alkalmas arra, hogy az ontológiánkkal kísérletezzünk. Sok helyet hagy az olvasó részére is, hogy a reprezentációba a saját kreativitását is bevihesse. Gondoljunk például arra, hogy egy interneten bevásárló ágensnek rengeteg témát és szerzőt kell ismernie, hogy az Amazon.com-tól könyveket vásároljon, élelmiszerek egész választékáról kell tudnia, hogy a Peapod.com-nál élelmiszert vásároljon, és mindenről, amit egy bolhapiacon találni lehet ismeretekkel kell rendelkeznie, hogy az Ebay.com^[93]-nál az alkalmi jó üzletekre vadásszon.
Gyűjtsön össze időre vonatkozó kifejezéseket, például „két órakor”, „éjfélkor”, „12:46-kor”. Találjon ki olyan nyelvtanilag helytelen példákat is, mint „huszonöt órakor” vagy „félnegyedháromkor”. Írjon nyelvtant az idő nyelvére.
Az a probléma, hogy a tanító halmaznak megfelelő döntési fát találjunk, bonyolultnak tűnik ugyan, de valójában van egy triviális megoldása. Egyszerűen egy olyan fát konstruálhatunk, amelyben minden példához egy külön saját utat hozunk létre egy – a példához tartozó – levélcsomóponthoz. A levélhez vezető út mentén sorra teszteljük az attribútumokat, és a példához tartozó tesztértéket követjük, a levél pedig a példa besorolását adja. Ha ismét ugyanazt a példát^[183] vesszük, akkor a döntési fa a helyes osztályozási eredményt adja. Szerencsétlen módon nemigen ad információt egyetlen más esetről sem!  ábra - Példák az étterem problématerületre Példák az étterem problématerületre
A szenzorok jelentik az érzékelési interfészt a robotok és környezetük között. A paszszív érzékelők (passive sensors), mint például a kamerák, ténylegesen puszta megfigyelői a környezetüknek: olyan jeleket vesznek, amelyeket a robot környezetében lévő más tárgyak generálnak. Az aktív szenzorok (active sensors), mint például a hanglokátor, energiát sugároznak környezetükbe, és érzékelik, ha ez az energia visszaverődik. Az aktív szenzorok általában több információt szolgáltatnak, de ugyanakkor többet fogyasztanak, és interferencia léphet fel, ha egyszerre többet is használunk. Akár aktív, akár passzív érzékelőkről van szó, három csoportba lehet őket osztani az alapján, hogy távolságot mérnek, teljes képet közvetítenek a környezetről vagy a robot egyes saját tulajdonságait figyelik.
Sokféle utalásfeloldó algoritmust találtak ki. Az egyik első (Hobbs, 1978) figyelemre méltó, mivel egy olyan mértékű statisztikai ellenőrzést végeztek rajta, ami abban az időben szokatlan volt. Három különböző szövegfajtát használva Hobbs 92%-os pontosságot mutatott ki. A módszer azt feltételezte, hogy egy elemző egy helyes elemzést készített, ami azonban nem állt rendelkezésére, ezért Hobbs kézzel készítette el az elemzéseket. A Hobbs algoritmus keresésként működik: az aktuális mondattól indulva visszafelé keres a mondatokban. Ez a technika biztosítja, hogy a legutóbbi jelölteket vizsgálja először. Egy mondaton belül szélességi keresést végez, balról jobbra haladva. Ez azt biztosítja, hogy az alanyokat a tárgyak előtt vizsgálja. Az algoritmus kiválasztja az első jelöltet, amely a most vázolt kényszereknek eleget tesz.
Mivel a cselekvésszimbólumok száma a cselekvéssémák aritásával exponenciálisan nő, megoldás lehet az aritás csökkentése. Ezt a szemantikus hálóktól (lásd  fejezet) kölcsönvett ötlet segítségével tehetjük meg. A szemantikus hálók csak bináris predikátumokat használnak, az ennél több argumentummal rendelkező predikátumokat egy bináris predikátumhalmazra képezzük le, ami külön ad meg minden predikátumot. Ezt az ötletet a Repül(P[1], SFO, JFK)^0 cselekvésszimbólumra alkalmazva, három új szimbólumot kapunk:
Ha az összes hasznosságfüggvény ilyen tetszőleges volna, mint ez, akkor viszont a hasznosságelméletnek nem lenne sok haszna, mivel meg kellene figyelnünk az ágens preferenciáit minden lehetséges körülmény esetén, mielőtt a viselkedésével kapcsolatban bármilyen előrejelzésre is képesek lennénk. Szerencsére az igazi ágensek preferenciái sokkal rendszerezettebbek. Ennek megfelelően szisztematikus módszerek léteznek a hasznosságfüggvények megtervezésére, amelyeket aztán egy mesterséges ágensbe beépítve az ágens a helyes, általunk elvárt viselkedést fogja produkálni.
A magyar lakosság egészségtudatossága alacsony, az egyén felelősség érzete minimális, és csak kevesen tesznek egészségük megóvásáért. Az egyik legfontosabb, egyéni és társadalmi szintű feladat lenne a jövőre vonatkozóan, hogy a preventív szemléletű élet- és magatartásforma előtérbe kerüljön, és általános igénnyé váljon a laikusok körében.
ahol N a szavak száma. Minél kisebb az összetettség, annál jobb a modell. Az az n-gram modell, amely minden szóhoz 1/k valószínűséget rendel, k összetettségű; az összetettséget úgy is lehet értelmezni, mint átlagos elágazási tényezőt.
Például ha az A^* algoritmus egy 5 mélységben fekvő megoldást 52 csomópont kifejtésével talál meg, akkor az effektív elágazási tényező 1,92. Az effektív elágazási tényező a problémaesetek függvényében változhat, megfelelően nehéz problémák esetén azonban általában nagyjából állandó. Így a b^* kisszámú problémahalmazon végzett kísérleti mérése jó fogódzót adhat a heurisztikus függvény általánosságban vett használhatóságáról. Egy jól megtervezett heurisztikus függvény effektív elágazási tényezője 1 körüli érték, ami lehetővé teszi felettébb nagy problémák megoldását is.
Az állapottér-tervkészítőkkel kapcsolatos érdeklődés újjáéledésének úttörője Drew McDermott UnPOP programja (McDermott, 1996), amely elsőként javasolta a törlési listát figyelmen kívül hagyó egyszerűsített problémán alapuló távolság heurisztikát. Az UnPOP név a részben rendezett tervkészítőket illető túlzott figyelem egyik reakciója volt; McDermott szerint más megközelítések nem kapták meg a megérdemelt figyelmet. Bonet és Geffner heurisztikus kereső tervkészítője (Heuristic Search Planner – HSP) és ennek későbbi leszármazottjai (Bonet és Geffner, 1999) elsőként tették az állapottér-keresést alkalmazhatóvá nagyméretű tervkészítési feladatokra. A mai napig legsikeresebb állapottér-kereső Hoffmann FastForward vagy FF keresője, az AIPS 2000 tervkészítő versenyének győztese (Hoffmann, 2000). Az FF egyszerűsített tervkészítési gráf heurisztikát használ egy nagyon gyors, az előrefelé és a lokális keresést újszerűen ötvöző algoritmussal.
Turing az intelligens gépek lehetősége ellen felhozható érvek széles választékát is megvizsgálta, beleértve szinte minden olyan ellenérvet, amely a cikk megjelenése óta eltelt fél évszázadban felmerült. A következőkben ezek közül nézünk meg néhányat.
Az üres klóz – egy diszjunkt nélküli diszjunkció – ekvivalens a Hamis értékkel, mert a diszjunkció akkor igaz csak, ha legalább az egyik diszjunkt igaz. ∨gy is beláthatjuk, hogy az üres klóz ellentmondást reprezentál, hogy megfigyeljük azt, hogy az üres klóz két kiegészítő egységklóz, mint amilyen az S és ¬S, rezolválásából származik.  ábra - Egy egyszerű rezolúciós algoritmus az ítéletkalkulushoz. Az IK-Rezolválás a két bemenetként megkapott állítás rezolválásából származó összes lehetséges klóz halmazát adja vissza. Egy egyszerű rezolúciós algoritmus az ítéletkalkulushoz. Az IK-Rezolválás a két bemenetként megkapott állítás rezolválásából származó összes lehetséges klóz halmazát adja vissza.
ErzsébetAnyjaAzElsőElem(〈Mami, Károly〉) Fontos A Nagyszülője-nek az ilyen attribútumokkal kifejezett definíciója nem lesz más, mint egy nagy diszjunkció, ami az egyes konkrét esetekből áll, és amit lehetetlen az új esetekre általánosítani. Az attribútumalapú tanuló algoritmusok képtelenek relációs predikátumokat megtanulni. Az ILP egyik legfontosabb előnye tehát az, hogy a problémák sokkal szélesebb választékában alkalmazhatók, beleértve a relációs problémákat is.
Még a fenti kis példa alapján is láthatónak kell lennie, hogy a trigram modell jobb, mint a bigram (amely pedig jobb, mint az unigram), mind az angol nyelv, mind egy MI-tankönyv témájának közelítésében. A modellek maguk is egyetértenek ezzel: a trigram modell a véletlen módon generált sztringjéhez 10^–10 valószínűséget rendel, a bigram 10^–29-et, az unigram pedig 10^–59-et.
A zárt világ feltételezés lehetővé teszi, hogy megtaláljuk egy reláció minimálmodelljét (minimal model). Ez azt jelenti, hogy a Tantárgy relációhoz a legkevesebb elemet tartalmazó modellt találhatjuk meg. A   egyenletben a Tantárgy minimálmodellje négyelemű, kevesebb már ellentmondáshoz vezet. Horn-klóz tudásbázisok esetén mindig létezik egy egyértelmű minimálmodell. Jegyezzük meg, hogy az egyedi elnevezések feltételezés mellett ez az azonossági relációra is vonatkozik: minden term csakis saját magával azonos. Paradox módon ez azt jelenti, hogy a minimál modellek egyben maximálisak abban az értelemben, hogy annyi objektumot tartalmaznak, amennyi csak lehetséges.
Mindez remekül használható determinisztikus esetekben, ahol a tulajdonságértékek biztosan ismertek. De mi történik abban az általános esetben, ahol a cselekvések kimenetele bizonytalan? A szigorú dominancia közvetlen analógiája könnyen megalkotható, ha a bizonytalanság ellenére, S[1] minden lehetséges kimenetele szigorúan dominálja S[2] minden lehetséges kimenetelét. (Ezt mutatja a  (b) ábra sematikus ábrázolása.) Természetesen ez valószínűleg sokkal ritkábban fordul elő, mint a determinisztikus esetben.
Az ábrán tehát látszik, hogy a Természet kezdetben előállít egy típus-kombinációt, amely minden játékoshoz társít egy-egy típust a játékos típuskészletéből, és aztán gyakorlatilag ettől függ, hogy melyik játékot játsszák a játékosok (vagy inkább típusok). A fentebbi fólián t1, t2, ..., tm jelöli ezeket a típus-kombinációkat, amelyek minden egyes játékoshoz rendelnek egy-egy típust. Ezek alapján összesen m különböző játék lenne lehetséges. Nézzük ezt formálisan!
Érdemes lehet figyelembe venni, hogy melyik bábut mikor érdemes játékba hozni. Természetesen nagyon különféle stratégiák léteznek, de alapvetően megállapítható, hogy a futót és a huszárt érdemes korán előre küldeni, a királlyal érdemes hamar sáncolni, a bástyák és a királynő pedig maradjon a helyén egészen addig, amíg valami meghatározó támadási lehetőségük nem lesz.
Megmutatható, hogy az alternatívák feletti preferenciákra való "racionális" megkötések alapján jogos egy bizonytalansági reláció feltételezése, ami az események feletti valószínűségi eloszlás írja le. Hasonló fontosságú eredmény származtatható az eseményekhez rendelt következményekre is, azaz hogy az u:C→ℝ  hasznosság függvény létezik és adott feltevések mellett egyértelmű. A hasznosság felhasználásával pedig az alternatívákra is kvantitatív jellemző származtatható, amely felhasználásával az eredeti alternatívák feletti preferencia relációnak megfelelő döntési elv származtatható:
∧ (és). Egy mondatot, amelynek fő kötőszava a ∧, mint például a W[1,3] ∧ C[1,3] konjunkció-nak (conjunction) nevezünk; ennek részei a konjunktok (conjuncts). (Az ∧ jel hasonlít egy A-ra, az angol And (és) szóból.)
A rejtett változókkal és a hiányzó adatokkal való valószínűségi modell tanulás általános problematikáját az EM algoritmussal kísérelték meg kezelni (Dempster és társai, 1977). Ezt számos meglévő módszerből absztrahálták, amelyek közt található a rejtett Markov-modell (HMM) tanulásra szolgáló Baum–Welch-algoritmus is (Baum és Petrie, 1966). (Maga Dempster az EM algoritmust inkább sémának tekinti, nem algoritmusnak, mivel jó adag elméleti matematikai munkára lehet szükség mielőtt egy új eloszláscsaládra alkalmazható lenne.) Manapság az EM egyike a tudományos kutatásban legelterjedtebben használt algoritmusoknak, McLachlan és Krishnan egy teljes könyvet szenteltek neki és tulajdonságainak (McLachlan és Krishnan, 1997). A kevert modellek – beleértve a kevert Gauss-modellek – tanulásának speciális problémáit Titterington és társai tárgyalták (Titterington és társai, 1985). Az Autoclass volt az első sikeres rendszer az MI-n belül, amely az EM-et alkalmazta kevert modellezésre (Cheeseman és társai, 1988; Cheeseman és Stutz, 1996). Az Autoclass-t egy sor valós tudományos osztályozási feladatra alkalmazták; ezek közül kettő: spektrális tulajdonságok alapján új csillagtípusok felfedezése (Goebel és társai, 1989); új fehérje- és intronosztályok felfedezése DNS/fehérjeszekvencia adatbázisokban (Hunter és States, 1992).
Vizsgáljuk ismét az „I smelled a wumpus in 2,2.” többértelmű példát! Egy preferenciaheurisztika a jobbra asszociáció (right association), amely szerint amikor el kell döntenünk, hogy az „in 2,2” PP-t hova helyezzük el az elemzési fában, akkor azt preferáljuk, hogy a jobb szélen elhelyezkedő összetevőhöz csatoljuk, ami jelen esetben az „a wumpus” NP. Természetesen ez csak egy heurisztika; az „I smelled a wumpus with my nose” mondatra a heurisztikát felülbírálná az a tény, hogy az „a wumpus with my nose” NP nem valószínű.
1. Mit jelent, hogy egy logika teljes? Egy logika teljes, ha minden vonzat reláció (magyarán minden igaz állítás) formálisan bebizonyítható (a bizonyítási lépések sorozatával). Ez a kérdés/válasz az egész feladat kulcsa. 2. Mit jelent, hogy Modus Ponens nem teljes, de pl. a rezolúció teljes? Azt, hogy egy teljes logikában (itt természetesen a teljes körű ítélet-, vagy predikátumkalkulusra gondolunk) a csupán MP lépésekből univerzális, minden igaz állítást bizonyító eljárás nem építhető ki. A rezolúciós lépésből viszont igen, és ez volt az az igény, ami a rezolúció felfedezéséhez vezetett. Vegyünk egy példát. Legyen a TudásBázis = {A -> B, ~A -> B}, B állítás vonzata a TB-nak, de a MP lépés nem végezhető el, a B igazát MP-szel belátni nem tudjuk. Mi a helyzet rezolúcióval? Ehhez a kérdéses állítás negáltját a TB-hoz kell hozzáadni: TB’ = {A -> B, ~A -> B, ~B}, az összes állítást klózzá alakítani: TB’ = {~A V B, A V B, ~B}, rezolválni üres rezolvensre törekedve: ~A V B és A V B –ből adódik a B, ami ~B –vel rezolválva eljutunk az üres rezolvensig. Lám, rezolúcióval sikerült a lehetetlen! Üres rezolvens azt jelenti, hogy a TB’ ellentmondásos, de az egyetlen gyanús állítás a hozzáadott ~B, tehát az hamis kell, hogy legyen, akkor viszont a ponált B igaz! 3. Mit jelent az, hogy egy logika eldönthető? Egy logika eldönthető, ha egy állításról (véges erőforrásokkal) ki lehet deríteni, hogy igaz, vagy hamis. Egy eldönthető logikában egy igaz állítást be tudunk látni, azaz egy eldönthető logika teljes is. Fordítva nem biztos! 4. Eldönthető-e a predikátum kalkulus? Nem. Ítéletkalkulus eldönthető (igazságtáblával minden véges lépésben be lehet látni), de a predikátum kalkulus egy olyan gazdag kiterjesztése, hogy a tökéletes eldönthetőséget elveszítettük. 5. Teljes-e? Igen. A teljesség megmaradt. Ha nem lenne teljes, akkor a világ leírásához sok hasznát nem lehetne venni. Gondoljunk, hogy milyen hasznos egy logikai állítás, ha sem az igazságát, sem a hamisságát belátni nem lehet.
Az így előállt {NSW=RED, WA=RED, NT=GREEN} behelyettesítés továbbra is konzisztens, így következhet az előretekintés. Ennek során az imént behelyettesített NT változóhoz kapcsolódó, eddig még behelyettesítetlen változók (SA és Q) értékkészletéből próbáljuk meg eltávolítani az előbbi, NT=GREEN értékadással inkompatibilis értékeket, azaz az SA és Q változók értékkészletéből kivesszük a GREEN értéket.
Leopold Löwenheim adta meg a modellelmélet rendszerezett leírását az elsőrendű logika számára (Löwenheim, 1915). Cikke az egyenlőségszimbólumot már a logika szerves részének tekinti. Löwenheim eredményeit Thoralf Skolem fejlesztette tovább (Skolem, 1920). Alfred Tarski a halmazelméletet felhasználva megadta az igazság és a modellelméleti kielégíthetőség explicit definícióját (Tarski, 1935, 1956).
M. Albert és R. Nowakowski adott leírást a játék „vesztő” és „nyerő” állásaira [6]. Bizonyítás nélkül a két tétel, mely a „vesztő” állításra vonatkozik: * Egy állás „vesztő” akkor, ha a két szélső kupac elemszáma sorban x és y, akkor |x-y|<=1. * Egy állást a következőképpen reprezentálunk: n darab x elemszámú kupac az elején, majd egy ettől eltérő elemszámú kupacból néhány, majd a végén m darab y elemszámú kupac, ahol n,m>=1. Ekkor vesztő helyzet: az előző pontban tárgyalt helyzetek, a csak x-ből álló állások, ahol n páros, illetve az üres helyzet.
A helymeghatározás (localization) jellemző példa a robotérzékelésre. A probléma lényege a dolgok pontos helyzetének meghatározása. A helymeghatározás az egyik legfontosabb érzékelési feladat a robotikában, mivel a fizikai környezettel való sikeres kölcsönhatáshoz feltétlenül szükséges. Például a robotkaroknak tudniuk kell, hol van az a tárgy, amivel dolgozni akarnak. A navigáló robotoknak pedig pontosan ismerniük kell saját helyzetüket, hogy eljuthassanak a céljukhoz.
Azt javasolta, hogy a korlátozott racionalitás elsődlegesen az elfogadhatóságban (satisficing) nyilvánul meg: olyan hosszú ideig kell foglalkozni a döntéssel, ami elegendő egy „elfogadhatóan jó” válasz megtalálásához. Ezért a munkájáért Simon közgazdasági Nobel-díjat kapott, és később részletesen is írt még róla (Simon, 1982). Ez az emberi viselkedésnek sok esetben hasznosnak tűnő modellje, azonban nem ad formális specifikációt az intelligens ágensek számára, mert az elmélet nem adja meg az „elfogadhatóan jó” definícióját. Továbbá az elfogadhatóság láthatóan csak egyike a korlátozott erőforrások leküzdését szolgáló technikáknak. 4. Korlátozott optimalitás, KO (bounded optimality, BO): Egy korlátozottan optimális ágens adott számítási erőforrások mellett a lehető legjobban cselekszik. Egy ilyen ágens esetén az ágensprogram várható hasznossága legalább olyan magas, mint az ugyanazon a gépen futó bármilyen más ágensprogramé.
Megmutattuk, hogy az eddig ismertetett következtetési szabályok helyesek, de nem tárgyaltuk az ezeket használó következtetési algoritmusok teljességének kérdését. A keresési algoritmusok, mint az iteratívan mélyülő keresés 3. szakasz - Iteratívan mélyülő mélységi keresés részben teljesek abban az értelemben, hogy meg fogják találni az elérendő célt. Ha azonban a rendelkezésre álló szabályok hiányosak, akkor a cél nem érhető el – nem létezik olyan bizonyítás, amely ezeket a szabályokat használja. Például ha kivennénk az ekvivalencia kiküszöbölés szabályt, az előző fejezetbeli bizonyítás nem futna végig. Ez a fejezet egyetlen következtetési szabályt mutat be, a rezolúciót (resolution), amelynek alkalmazása, párosítva bármelyik keresési módszerrel, egy teljes következtetési algoritmust eredményez.
A fejezet azért nem foglalkozott a párhuzamos keresési (parallel search) algoritmusokkal, mert ezek tárgyalásához a párhuzamos architektúrák terjedelmes ismertetésére lett volna szükség. A párhuzamos keresés egyre fontosabb témakörré válik mind az MI, mind pedig az elméleti számítógép-tudomány területén. Egy az MI idevágó irodalmába történő rövid bevezető Mahanti és Daniels cikkében (Mahanti és Daniels, 1993) található.
A Decisions with Multiple Objectives: Preferences and Value Tradeoffs (Keeney és Raiffa, 1976) c. könyv részletes bevezetést ad a többattribútumú hasznosságelméletbe. Ez olyan módszerek korai számítógépes megvalósítását írja le, amelyekkel a többattribútumú hasznosságfüggvényhez a szükséges paraméterek kikérdezhetők, és hosszasan tárgyalja az elmélet valós alkalmazásait. Az MI-n belül a legelső hivatkozás az MVH elvre Wellman cikke volt (Wellman, 1985), amely egy URP-nek (Utility Reasoning Package) nevezett rendszert tartalmaz, ami képes kezelni a preferenciák függetlenségéről és feltételes függetlenségéről szóló állításokat, hogy elemezze a döntési probléma struktúráját. A sztochasztikus dominanciát a kvalitatív valószínűségi modellel Wellman vizsgálta alaposan (Wellman, 1988; 1990a). Wellman és Doyle egy előzetes vázlatát adja annak, hogy hogyan lehet felhasználni hasznosság-függetlenség relációk komplex halmazát a hasznosságfüggvény strukturált modelljének megalkotásához, hasonlóan ahhoz, ahogyan a valószínűségi háló az együttes valószínűségeloszlás-függvény strukturált modelljét adja (Wellman és Doyle, 1992). Bacchus és Grove, valamint La Mura és Shoham további eredményeket közöl ezzel kapcsolatban (Bacchus és Grove, 1995; 1996; La Mura és Shoham, 1999).
Az interface ágensek felhasználó igényeihez messzemenően alkalmazkodnak, az egyre bonyolultabb programok kezelésének elsajátításában segítve, az ember és a számítógép között teremtenek partnerszerű kapcsolatot. Multimodálisak, a szöveg mellett képekkel, hangokkal és egyéb módszerekkel (például egyre gyakrabban gesztusokkal) kommunikálnak. pl.: beszédképtelen, vagy mozgásképtelen, halláskárosult emberek kommunikációjának segítésére.
Az elsőrendű logika fejlődésének fontos akadálya volt az egyváltozós predikátumok előtérbe helyezése, illetve a többváltozós predikátumok kizárása. Az egyváltozós predikátumokhoz való ragaszkodás általánosan jellemző volt a logikai rendszerekre Arisztotelésztől Boole-ig. A logikai relációk első rendszerezett leírását Augustus De Morgan adta (De Morgan, 1864). De Morgan a következő példával mutatta be az arisztotelészi logikával nem kezelhető következtetéseket: „Minden ló állat; ezért a ló feje egy állat feje”. Ez a következtetés nem valósítható meg az arisztotelészi rendszerrel, mert bármely, a következtetésben alkalmazható szabálynak először az „x a feje y-nak” két predikátumot tartalmazó mondatot kell elemeznie. A relációk logikáját alaposan tanulmányozta Charles Sanders Peirce (Peirce, 1870), aki Frege-től függetlenül, néhány évvel később szintén kifejlesztette az elsőrendű logikát (Peirce, 1883).
Beszéltünk már arról, hogyan tervezzük meg a mozgást, de arról nem, hogy hogyan mozogjunk. Az eddigi terveink – különösen azok, amelyeket determinisztikus pályatervezővel állítottunk elő – feltételezik, hogy a robot egyszerűen képes követni bármilyen megtervezett pályát. A valóságban természetesen nem ez a helyzet. A robotoknak van tehetetlenségük, ezért nem tudnak tetszőleges pályát követni, csak nagyon kis sebesség esetén. A legtöbbször erőt kell kifejtenie a robotnak, nem pedig meghatározott pozícióba eljutnia. Ez a fejezet azokról a módszerekről szól, amelyekkel ki lehet számítani ezeket az erőket.  ábra - Robotkarvezérlés (a) egységnyi erősítésű arányos szabályozóval; (b) 0,1-es erősítési tényezőjű arányos szabályozóval; (c) PD szabályozóval, 0,3-as erősítésű arányos taggal és 0,8-as differenciáló komponenssel. Mindhárom esetben a robotkar a szürkével jelölt pályát próbálta követni. Robotkarvezérlés (a) egységnyi erősítésű arányos szabályozóval; (b) 0,1-es erősítési tényezőjű arányos szabályozóval; (c) PD szabályozóval, 0,3-as erősítésű arányos taggal és 0,8-as differenciáló komponenssel. Mindhárom esetben a robotkar a szürkével jelölt pályát próbálta követni.
A da Vinci egy komplett távsebészeti eszköz, azaz teleoperációval vezérelt robot. A da Vinci-műtéti rendszer gyártója és fejlesztője az amerikai Intuitive Surgical Inc., amely a szektorban az egyedüli profitot termelő cég. A da Vinci széles körű elterjedésével vált világszerte ismertté a robotsebészet.
Az eddig leírt hegymászó algoritmusok nem teljesek – sokszor kudarcot vallanak a cél megkeresésében, mert egy lokális maximumba beragadnak. A véletlen újraindítású hegymászás (random-restart hill-climbing) az ismert közmondás szerint jár el: „Ha nem megy elsőre, csináld újra.” Véletlenül generált kiinduló állapotokból^[40] hegymászó keresést végez, amíg célba nem ér. Az algoritmus 1-hez tartó valószínűséggel teljes annál a triviális oknál fogva, hogy előbb-utóbb a célállapotot kezdőállapotként is fogja generálni. Ha minden hegymászó keresés p valószínűséggel sikeres, a véletlen újraindítások várható száma 1/p. A 8-királynő problémában, ha oldallépéseket nem engedünk meg, p ≈ 0,14, így a cél megtalálásához átlagosan 7 iterációra van szükség (6 kudarc és 1 siker). A várható lépésszám a sikeres iteráció költsége valamint (1 – p)/p-szerese a kudarc költségének, durván 22 lépés. Ha oldallépéseket is engedünk, átlagosan 1/0,94 ≈ 1,06 iterációra és (1 × 21) + (0,06/0,94) × 64 ≈ 25 lépésre van szükség. A 8-királynő probléma számára a véletlen újraindítású hegymászó keresés valóban hatékony. Még hárommillió királynő mellett is ez a megközelítés egy percen belül talál megoldást.^[41]
Ebből következően minden valószínűségi kijelentésnek hivatkoznia kell azokra a tényekre, amelyek alapján az adott valószínűség az állításhoz lett rendelve. Amint egy ágens új észlelések birtokába jut, ezek figyelembevételével módosítja a valószínűségek becslését. Mielőtt tények birtokába jutunk, előzetes, illetve a priori (prior) vagy feltétel nélküli (unconditional) valószínűségről beszélünk, a tények birtokában pedig utólagos, illetve a posteriori (posterior) vagy feltételes (conditional) valószínűségről. Az ágens a legtöbb esetben rendelkezni fog bizonyos tényekkel az érzékelései hatására, és érdekelt lesz az általa felügyelt kimenetelek a posteriori valószínűségeinek kiszámításában.
Az utasítások kiadásánál a konzol kiírja az aktuális parancsok eredményét, ha nem teszünk pontosvesszőt a végükre. Amennyiben ezt nem akarjuk, érdemes pontosvesszőt tenni az utasítások végére, hogy a konzol ne váljon átláthatatlanná.
Az az ágens, amelyik bármely körben 10 másodpercen belül nem cselekszik, lefagyottnak számít, ezért az adott játék véget ér. Ha egy csapat számítási ideje (a tagjainak összeadott számítási ideje) a 180 másodpercet meghaladja a játék szintén véget ér. Ezekben az esetekben a vétkes csapat pontszáma nullázódik, míg az ellenfél aktuális pontszáma megmarad. A fenti időkeretek alatt végezhető számítások mennyisége természetesen hardverfüggő – a korlátok tekintetében referencia rendszernek a hivatalos beadó rendszer számít.  ábra - Saját ágensek Saját ágensek
A Harris és Stephens féle detektor a Moravec kereső továbbfejlesztése. A lokálisan eltolt képeket elsőfokú Taylor polinomokkal közelíti, ezzel visszavezeti a különbség négyzetösszegeket a gradiens kép egy függvényére. Egy ún. Harris mátrixot számít ki, melynek sajátértékei a sajátvektorok irányába történő elmozdulás esetén jellemzik a kép megváltozását. A mátrix sajátvektorai szemléletesen a legnagyobb és legkisebb változás irányába mutatnak, a két változás mértéke pedig jellemezhető a sajátértékekkel. Ebből következik, hogy a Harris detektor rotáció invariáns, hiszen a sajátértékek nem függnek a kép orientációjától, csak a sajátvektorok. Sarokpont van tehát ott, ahol mindkét sajátérték nagy. Az algoritmus a sajátértékeket közelítő módszerrel számolja a sebesség érdekében.
A Jason nagy előnye, hogy rugalmasan bővíthető Java nyelven, így lehetőség van tisztán AgentSpeak nyelv használatára, az AgentSpeak részleges kibővítésére Java nyelven, vagy extrém esetben igen minimális AgentSpeak használatával szinte kizárólag Java implementáció létrehozására.
A kép egy képpontjának a fényessége arányos a jelenet képpontba vetített felületeleme által a kamera felé irányított fény mennyiségével. Ez viszont függ a felületelem fényvisszaverő tulajdonságaitól és a fényforrások elhelyezésétől, valamint eloszlásától a jelenetben. Egy képpont fényességébe a jelenet egyéb részeinek fényvisszaverő tulajdonságai is beleszólnak, hiszen a jelenet más felületei indirekt fényforrásként szolgálnak, mivel a rájuk eső fényt az adott felületelem felé verik vissza.
Összevetettük az állapottérben előre- és hátrafelé kereső tervkészítőket a részben rendezett tervkészítőkkel úgy, hogy az utóbbit egy tervtérkeresőnek vettük. Magyarázza meg, hogyan tekinthető az előre- és hátrafelé történő állapottér-keresés tervtérkeresőnek, és mondja meg, mik a tervfinomító operátorok.
A legegyszerűbb módszer, amivel ágensek egy csoportja biztosíthatja az egyetértést egy együttes tervben, hogy egy konvenciót (convention) fogadnak el az együttes cselekvés megkezdése előtt. A konvenció az együttes tervek közötti választásra vonatkozó bármely olyan megkötés, amely túlmutat az alapmegkötésen, amely szerint egy együttes tervnek működni kell, ha minden ágens alkalmazza. Például a „maradj a saját térfeleden” konvenció a páros partnereket a második terv kiválasztására sarkallja, míg az „egy játékos mindig a hálónál marad” konvenció az egyes tervhez vezetné őket. Néhány konvenció, mint az út megfelelő oldalán való vezetés, olyan széles körben elterjedt, hogy ezeket már társadalmi törvényszerűségekként (social laws) kezeljük. Az emberi nyelvek szintén konvencióknak tekinthetők.
P (Polinomial), NP (Non-deterministic Polinomial), NP-teljes (NP complete) * A globális és lokális optimum értelmezése * A problématér matematikai ábrázolás gráffal * A kereső (produkciós) rendszer jellemzői, általános algoritmusa * Klasszikus kereső algoritmusok és jellemzőik:
A HOG módszert sikerrel alkalmazták emberalakok felismerésre, illetve később általánosabb célokra is. Az algoritmus erőssége, hogy gyors és pontos, így némi módosítással mozgóképen is lehet alakzatokat követni vele.  ábra - A HoG eredménye egy jellemzően vízszintes, illetve egy függőleges éleket tartalmazó képrészletre. (forrás: 24.5. szakasz - Hivatkozások) A HoG eredménye egy jellemzően vízszintes, illetve egy függőleges éleket tartalmazó képrészletre. (forrás: 24.5. szakasz - Hivatkozások)
A  ábra mutatja be az IK-módszert használó, felfedező Q-tanuló ágens teljes programját. Vegyük észre, hogy ugyanazt az f felfedezési függvényt használja, mint amit a felfedező ADP-ágens is használt – ezért van szükség a végrehajtott cselekvések gyakorisági statisztikáira (az N táblára). Ha egy egyszerűbb felfedezési stratégiát használnánk – mondjuk a lépések egy részében véletlenszerű működést iktatva be, ahol ez részarány az idővel csökken –, akkor mellőzhetnénk ezt a statisztikát.
Nincs döntetlen: Egyik nagy előnye a játéknak, hogy sosem végződhet döntetlenként. Az egyik játékos biztosan létre tud hozni egy teljes hidat a két oldala között. Tehát ha piros csinál egy hidat, az meggátolja a kéket, hogy megtegye ugyanezt (és fordítva). Ha a piros nem csinál egy teljes hidat, akkor a kéknek van lehetősége erre. Ezt a teóriát többen kutatták, köztük Piet Hein, David Gale, de a bizonyítását John Nash nevéhez köthetjük.
Ésszerűnek tűnik tehát az összes, rendelkezésre álló memóriát használni. Az erre képes két algoritmus az MA^* (memóriakorlátozott A^*) és az EMA^* (egyszerűsített MA^*). Itt az EMA^*-t írjuk le, ami – mitagadás – az egyszerűbb. Az EMA^* az A^* módjára halad a legjobb levelet kifejtve, amíg a memória be nem telik. Ezen a ponton a keresési fához új csomópontot hozzáadni nem képes, hacsak egy régit nem töröl ki. Az EMA^* mindig a legrosszabb – a legmagasabb f-értékű – csomópontot hagyja ki. Majd, mint az RLEK, az elfelejtett csomópont értékét a szülőjéhez továbbítja. Ily módon egy elfelejtett részfa elődje tudja a részfa legjobb útjának az értékét. Ezzel az információval az EMA^* csak akkor fejti ki újra a fát, ha kimutatta, hogy minden más út rosszabbnak tűnik, mint az elfelejtett út. Más szóval, ha az n csomópont minden utódját elfelejtjük, nem tudjuk, hogy n-ből merrefelé lehetne menni, tudni fogjuk azonban, hogy mennyire érdemes egyáltalán bárhová is menni az n-ből kiindulva.  ábra - Az RLEK lépései, miközben a legrövidebb utat keresi Bukarest felé. A mindenkori rekurzív hívás f-korlát értéke a mindenkori aktuális csomópont felett látható. (a) A Rimnicu Vilceán át vezető utat az algoritmus addig követi, amíg az aktuális legjobb levél (Piteşti) értéke nem lesz rosszabb, mint a legjobb alternatív út értéke (Fogaras). (b) A rekurzió visszalép és az elfelejtett alfa legjobb levélértékét (417) Rimnicu Vilceánál feljegyezzük. Majd Fogaras kifejtése következik 450-nel, mint a legjobb levélértékkel. (c) A rekurzió visszalép. Az elfelejtett alfa legjobb levélértékét (450) Fogarasnál jegyezzük fel. Következik Rimnicu Vilcea kifejtése. Ezúttal, mivel a legjobb alternatív út (Temesváron keresztül) 447-be kerül, a kifejtés folytatódik Bukarest felé. Az RLEK lépései, miközben a legrövidebb utat keresi Bukarest felé. A mindenkori rekurzív hívás f-korlát értéke a mindenkori aktuális csomópont felett látható. (a) A Rimnicu Vilceán át vezető utat az algoritmus addig követi, amíg az aktuális legjobb levél (Piteşti) értéke nem lesz rosszabb, mint a legjobb alternatív út értéke (Fogaras). (b) A rekurzió visszalép és az elfelejtett alfa legjobb levélértékét (417) Rimnicu Vilceánál feljegyezzük. Majd Fogaras kifejtése következik 450-nel, mint a legjobb levélértékkel. (c) A rekurzió visszalép. Az elfelejtett alfa legjobb levélértékét (450) Fogarasnál jegyezzük fel. Következik Rimnicu Vilcea kifejtése. Ezúttal, mivel a legjobb alternatív út (Temesváron keresztül) 447-be kerül, a kifejtés folytatódik Bukarest felé.
Haladjunk sorban: az előbbi sikeres következtetés után a visszalépéses keresés újra meghívja önmagát az aktuális {SA=RED, NSW=GREEN} behelyettesítéssel, és a már-már egyértelműre egyszerűsített KKP-vel. Ezek után az első teendőnk szokás szerint egy még be nem helyettesített változó kiválasztása, aminek értéket adunk. Ezt a döntést a következő heurisztikák alapján hozzuk.
Meg kell említeni azt a tényt, hogy szemünk nem képes megkülönböztetni az egy helyről egyidejűleg beérkező, különböző hullámhosszúságú és energiájú fényt, csak ezek eredőjét, színérzetünk tehát nem objektív érzet.
 Az eredeti Strips programot a Shakey robot irányítására készítették. A  ábra a Shakey világának egy változatát mutatja, melyben négy szoba sorakozik egy folyosó mentén, melyek mindegyikének van egy ajtaja és van benne egy villanykapcsoló.
A mesterséges intelligencia célját illetően több megközelítés is ismert. Az egyik szerint a cél az emberi tudat mesterséges megalkotása, vagy azzal ekvivalens működés elérése (kognitív modellezés). A másik megközelítés gyakorlatias szempontból vizsgálja meg a kérdést, és azt tartja, hogy a végső cél „hasznos dolgok” létrehozása, és semmi több, vagyis az ennél elvontabb okfejtéseket feleslegesnek tartja. A harmadik hozzáállás azt mondja, hogy a cél valójában az „intelligencia teremtése”, azaz intelligens rendszerek, ágensek megalkotása, ahol is fontos kiemelni, hogy nem feltétlenül az emberi intelligencia utánzására törekszünk. Mindhárom megközelítés a mesterséges intelligencia egy-egy aspektusára világít rá, és mindhárom megközelítés a maga módján hasznos, de az ágenseken értelmezett racionalitást legjobban a harmadik megközelítésen keresztül, az intelligencia fogalmát taglalva közelíthetjük meg [1].
Kövessük végig a következő ügyes érvelést. Tegyük fel, hogy σ bizonyítható A-ból, de akkor σ hamis, mivel σ épp azt mondja ki, hogy nem igazolható. Ebben az esetben viszont létezik egy hamis mondat, ami bizonyítható A-ból, így A nem tartalmazhatna csak igaz mondatokat – ez az előzetes feltételezésünkkel ellentétes. Tehát σ nem bizonyítható A-ból. Ez viszont éppen az, amit σ állít saját magáról, tehát σ egy igaz mondat.
Mindössze egyetlen csomópontot tartani a memóriában eléggé extrém reagálásnak tűnik a memóriakorlát problémájára. A lokális nyaláb keresés (local beam search) algoritmus^[42] nem egy, hanem k állapotot követ nyomon. Az algoritmus k véletlen módon generált állapottal indul. Minden lépésben a k állapot mindegyikének összes követőit kifejti. Ha ezek valamelyike egy cél, az algoritmus leáll. Egyébként a teljes listából kiválasztja a legjobb k követőt, és ezt az eljárást ismétli. Fontos Első látásra a k állapotú lokális nyaláb keresés nem tűnik másnak, mint a k véletlen újraindítás parallel futtatása a szekvenciális futtatás helyett. Valójában a két algoritmus igen különböző. A véletlen újraindítású algoritmusban minden keresési folyamat a többitől függetlenül fut le. A lokális nyaláb keresési algoritmusban a k parallel keresési szál megosztja az információt. Például ha az egyik állapot számos jó követőt generál, és a többi k – 1 mind rosszabbat, akkor az eredmény az, mintha az első állapot megüzenné a többinek: „Gyertek át ide, itt zöldebb a fű!” Az algoritmus gyorsan abbahagyja az eredménytelen kereséseket, és az erőforrásait oda viszi, ahol a legnagyobb előrehaladás érzékelhető.
Az utolsó nehézség, ami gyakran felmerül a nemdeterminisztikus feladatkörökben, a következő: a dolgok nem mindig működnek elsőre, így újra kell próbálkozni. Vegyük például a „tripla-Murphy” porszívó példáját, mely (a korábban bemutatott szokások mellett) néha nem mozdul az utasítás ellenére. Például csakúgy, mint a   egyenletben, a Balra cselekvés tartalmazhatja a OttBal ∨ OttJobb diszjunktív hatást. Ekkor a [Balra, if TisztaBal then [] else Szív] terv már nem garantált, hogy működik. A  ábra a keresési gráf egy részletét mutatja. Tisztán látható, hogy a továbbiakban nincsenek ciklusmentes megoldások, és az És-Vagy-Gráf-Keresés hibával térne vissza. Létezik azonban egy ciklikus megoldás (cyclic solution), ami addig próbálgatja a Balra lépést, mígnem egyszer működik. Ez a megoldás könnyebben kifejezhető, ha a terv egy részét egy címkével (label) jelöljük meg, és a terv ismételgetése helyett erre a címkére hivatkozunk. Így a ciklikus megoldásunk
A tudástervezési projektek különbözők tárgyukat, tárgykörüket és nehézségüket tekintve, de minden ilyen projekt tartalmazza a következő lépéseket: 1. A feladat beazonosítása. A tudásmérnöknek fel kell vázolnia a kérdések sorát, amelyekkel a tudásbázis foglalkozni fog, és a tényeknek azokat a csoportjait, amelyek minden egyes problémaspecifikus példányban megtalálhatók lesznek. El kell döntenie például, hogy a wumpus tudásbázisnak képesnek kell-e lennie a cselekvések kiválasztására, vagy hogy csak az várható el, hogy a környezet elemeivel kapcsolatos kérdéseket válaszolja meg. Tudnia kell, hogy az érzékelőktől származó tények leírják-e a jelenlegi helyzetet. A feladat határozza meg, hogy mely tudást kell tárolni, hogy a problémapéldányokban a válaszokat megadhassuk az adott esetre vonatkozóan. Ez a lépés analóg az ágensek tervezésénél látott TKBÉ-folyamattal, amelyről a  fejezetben írtunk. 2. A releváns tudás összegyűjtése. A tudásmérnök vagy már szakértője a tárgyterületnek, vagy együtt kell működnie igazi szakértőkkel, hogy megismerje az ő tudásukat – ezt a folyamatot tudásmegszerzésnek (knowledge acquisition) nevezzük. Ezen a szinten a tudást formálisan nem reprezentáljuk. A cél az, hogy megértsük a tudásbázis tárgykörét, amit a feladat határol be, és meg kell érteni azt is, hogy a tárgyterület hogyan működik a gyakorlatban.
Most már csak értéket kellene adnunk WA-nak. A WA változó értékkészletének elemeihez az LCV heurisztika a következő heurisztikus értékeket rendeli: {BLUE(0)}. Ennek oka, hogy az előbbi következtetési lépés eredményeképp a WA változó értékkészlete már teljes egészében konzisztens a szomszédainak az értékkészletével, azaz amennyiben a BLUE értéket adnánk neki, nem korlátoznánk a szomszédos NT és SA változók későbbi értékadását. Ezek szerint tehát összesen 0 érték kivételét eredményezné a WA=BLUE értékadás a szomszédok értékkészletéből. Ebből következik, hogy a WA változónak az egyetlen lehetséges BLUE értéket adjuk (WA=BLUE).
Így egy elsőrendű Markov-folyamatban az állapotok időbeli változását leíró szabályokat teljes mértékben tartalmazza a P(X[t]|X[1–t]) feltételes eloszlás, amit az elsőrendű folyamat állapotátmenet-modelljének (transition model) nevezünk.^[157] A  ábra elsőrendű és másodrendű Markov-folyamatokhoz tartozó Bayes-hálóstruktúrákat mutat.  ábra - (a) Egy elsőrendű Markov-folyamathoz tartozó Bayes-hálóstruktúra. A reprezentált Markov-folyamatban az állapotot az X[t ]változók definiálják. (b) Egy másodrendű Markov-folyamat. (a) Egy elsőrendű Markov-folyamathoz tartozó Bayes-hálóstruktúra. A reprezentált Markov-folyamatban az állapotot az Xt változók definiálják. (b) Egy másodrendű Markov-folyamat.
írható fel, ahol H[f](x) a második deriváltak mátrixa (Hesse-mátrix), melynek H[ij]elemeit a ∂^2f /∂x[i]∂x[j]második deriváltak adják meg. Mivel a második deriváltak mátrixának n^2 eleme van, a Newton–Raphson-módszer sokdimenziós terekben drága lesz, ami számos közelítő módszer kifejlesztéséhez vezetett.
A második fejezet az anaesthesia eseményeinek bevitelét jelenti (intubáció, műtét kezdete stb.). Ennek megkönnyítésére többféle technikai eljárás létezik. Teljesség igénye nélkül felsorolva a számítógép billentyűzete. Vonalkód beolvasó. Direkt képernyőérintés (fényceruza, TOUCH SCREEN, TOUCH MOUSE), indirekt képernyőérintések (nyíl, billentyűk, egér, joystick, csavarógombok).
Ha a meghatározások tanuló algoritmusa már adott, a tanuló ágensnek módja van egy olyan minimális hipotézist megkonstruálni, amelyen belül kell a célpredikátumot megtanulnia. Összemásolhatjuk a Minimális-Konzisztens-Megh és a Döntési-Fa-Tanulás algoritmusait, és így az RADFT-t, a relevanciaalapú döntési fa tanuló algoritmust kapjuk, amely először a releváns attribútumok minimális halmazát azonosítja, majd ezen halmazt tanulás céljából döntési fa algoritmusának adja tovább. A Döntési-Fa-Tanulás-sal ellentétben az RADFT egyidejűleg tanulja és használja a releváns információt ahhoz, hogy a hipotézisteret minimalizálja. Elvárjuk, hogy az RADFT tanulási görbéje a Döntési-Fa-Tanulás tanulási görbéjénél jobb legyen, és tényleg ez a helyzet. A  ábrán a két algoritmus tanulási görbéje látható, véletlen módon generált adatokon és egy olyan tanulandó függvény esetén, amely a lehetséges 16 attribútumból csupán 5-öt használ fel. Természetesen azokban az esetekben, amikor az összes attribútum releváns, az RADFT nem mutat előnyöket.  ábra - Az RADFT és a Döntési-Fa-Tanulás hatékonyságának összehasonlítása véletlen módon generált adatok és egy olyan célfüggvény esetén, amely a lehetséges 16 attribútumból csak 5-öt használ Az RADFT és a Döntési-Fa-Tanulás hatékonyságának összehasonlítása véletlen módon generált adatok és egy olyan célfüggvény esetén, amely a lehetséges 16 attribútumból csak 5-öt használ
A G-RIF algoritmus egy jellemző alapú alakzatfelismerő, mely a SIFT-hez hasonló sémát követ. A kulcspontokat sarokpont detektálással és radiális szimmetria vizsgálatával választja ki. A felhasznált jellemző pont leíró vektor tárolja a lokális él irányát, sűrűségét és árnyalatát. A vektorok összehasonlításához és a felismert objektum megszavazásához egy az emberi látásból kölcsönzött módszert használ. A tapasztalatok szerint a SIFT-nél jobban teljesít zsúfolt környezetben.
Definíció 2. Adott Környezet osztályban adott Architektúrá-n futtatott Program futási időben (vagy felhasznált tárhely tekintetében) átlagosan aszimptotikusan korlátozott optimális, ha létezik olyan k, amely esetén minden egyéb Program’ programra V(Program, k*Architektúra, Környezet) legalább akkora, mint V(Program’, Architektúra, Környezet).
Ha a θ tetszőleges értéket felvehet 0 és 1 között, akkor a P(Θ)-nek egy folytonos eloszlásnak kell lennie, amely csak 0 és 1 között nem nulla értékű, és integrálja 1. Egy lehetséges jelölt az egyenletes eloszlás P(θ) = U[0, 1] (θ). (Lásd  fejezet.) Az egyenletes eloszlás a béta-eloszlások (beta distributions) családjának tagja. Minden egyes béta-eloszlás két hiperparaméterrel^[197] (hyperparameter) – a-val és b-vel – definiálható a következő egyenlet szerint:
A fentebbi fólián egy i játékos hasznát írtuk fel egy tetszőleges q kevert stratégia-kombináció esetén, feltéve hogy a játék 2-szereplős, és mindkét játékosnak 2-2 tiszta stratégiája van: p12 annak a valószínűsége, hogy az 1-es játékos a második tiszta stratégiáját játssza, míg p22 annak a valószínűsége, hogy a 2-es játékos játssza a második kevert stratégiáját. ui11 az i játékos hasznát jelöli akkor, ha mindkét játékos az első tiszta stratégiája szerint cselekszik, ui12 hasonlóképp az i játékosnak a haszna akkor, ha az 1-es játékos az első, a 2-es játékos pedig a második tiszta stratégiája szerint cselekszik, ... Magyarán az i játékos (akár az 1-es, akár a 2-es) hasznát egy q kevert stratégia-kombináció esetén az i játékos tiszta stratégia-kombinációkhoz tartozó ui hasznának várható értékeként kapjuk (nyilván a q-ban adott valószínűségeknek megfelelően). Ez tehát mindkét játékosra, az i=1-re, és i=2-re is egy-egy egyenletet jelent.
Ennek megfelelően nem egyértelmű, hogy melyik terület alá célszerű besorolni, esetleg inkább önálló tudományterületként tekinteni rá. Egyesek ez utóbbi mellett érvelnek, hiszen a számítógépes nyelvészet „saját elméleti háttérrel, tudományos rokonsággal, döntéskereső és tudásreprezentációs eljárásokkal” rendelkezik. [1] Mások különböző alterületek összességeként („számítógépes nyelvészet a nyelvészeten belül, természetes nyelv feldolgozás a számítástudományon belül, beszédfelismerés a villamosmérnöki tudományon belül, számítógépes pszicholingvisztika a pszichológián / kognitív tudományokon belül) tekintenek a területre. [2]  ábra -  ábra: a számítógépes nyelvészet, mint interdiszciplináris terület egy részletesebb megközelítésben  ábra: a számítógépes nyelvészet, mint interdiszciplináris terület egy részletesebb megközelítésben
Ezzel tehát még további 100 nem-egyenlő korlátot adtunk a modellhez. Így összességében 100+125+88=313 bináris korlátot definiáltunk a modellben lévő 56 változó felett (amiből 25 primér, 25 duál, és 3*2 segédváltozó), melyeknek átlagosan nagyjából 5-5 lehetséges értéke van. Ez így jóval komplexebb, mint az 5.1. szakasz - 1. Ausztrália térképének kiszínezése bemutatott és megoldott „Ausztrália térképének kiszínezése” feladat, melyben mindössze 7 változó szerepelt, melyeknek rendre csak 3 lehetséges értéke volt, továbbá e változók felett mindössze 8 korlát lebegett. A keresési tér akkor ennek megfelelően 3^7=2187 lehetséges behelyettesítésből állt, és ezen belül a megoldások viszonylag sűrűn és egyenletesen oszlottak el. Most viszont a keresési tér mérete ennél jóval nagyobb, konkrétan (5 érték/változóval számolva) 5^56, ami körülbelül 1,38*10^39 (azaz 39 nulla az egyes után). Ráadásul a lehetséges behelyettesítések immár nem 7-elemű vektorok, hanem 56-elemű vektorok, sőt mi több, a keresési térben a fentebb definiált 313 bináris korlát mellett mindössze egyetlen megoldás létezik. Ennek bemutatása következik most.
Szerencsére létezik egy használhatóbb általánosítás, az úgynevezett sztochasztikus dominancia (stochastic dominance), ami valós problémáknál is gyakran előfordul. A sztochasztikus dominanciát könnyebb megérteni egyetlen változó esetében. Tételezzük fel, hogy a reptér költsége az S[1] helyen egyenletes eloszlású 2,8 és 4,8 milliárd dollár között, és az S[2] helyszínen a költség egyenletes eloszlású 3,0 és 5,2 milliárd dollár között. A  (a) ábra ezen költségek eloszlásait mutatja, ahol a költségek negatív értékként szerepelnek. Ekkor csupán azt az információt ismerve, hogy a hasznosság csökken a költségekkel, azt mondhatjuk, hogy S[1] sztochasztikusan dominálja S[2]-t – azaz S[2] elhagyható. Fontos felismerni, hogy ez nem következik a várható költségek összehasonlításából. Például ha tudnánk, hogy S[1] költsége pontosan 3,8 milliárd dollár, akkor a pénz hasznosságára vonatkozó további információk nélkül nem tudnánk döntést hozni.^[167]  ábra - Sztochasztikus dominancia. (a) S[1] sztochasztikusan dominálja S[2]-t a költségek vonatkozásában. (b) S[1] és S[2] negatív költségeinek eloszlásfüggvényei. Sztochasztikus dominancia. (a) S1 sztochasztikusan dominálja S2-t a költségek vonatkozásában. (b) S1 és S2 negatív költségeinek eloszlásfüggvényei.
 Megjegyzés Valósítsa meg a diagramelemző algoritmus egy olyan változatát, amely a leghosszabb bal oldali él tömörített fáját adja vissza, és amennyiben ez az él nem fedi le a teljes fát, folytatja az elemzést annak az élnek a végén! Mutassa meg, hogy miért lesz szükség a Predict eljárás meghívására a folytatás előtt! A végeredmény tömörített fák egy olyan listája, ahol a teljes lista lefedi a bemenetet.
Az előző példákban azért fordulunk a háttértudáshoz, hogy a megválasztott általánosítást megpróbáljuk igazolni. Most azt nézzük meg, mik azok a vonzatkényszerek, amelyek ott rejtőznek. A kényszerek a Hipotézis-en, a megfigyelt Leírások-on és a Besorolások-on kívül a Háttértudás-ra is kiterjednek.
A beszédfelismerés őstörténete az 1920-as években kezdődött Radio Rexszel, a hangvezérlésű játék kutyával. Rex ugrált az 500 Hz körüli hangfrekvenciákra válaszul, ami az [eh] magánhangzóhoz tartozik a „Rex!”-ben. Kissé komolyabb munka a második világháború után kezdődött. Az AT&T Bell Labsnál egy rendszert építettek egyedülálló számjegyek felismerésére akusztikus jellemzők egyszerű mintaillesztésével (Davis és társai, 1952). A beszédhang átmenet-valószínűségeket először egy, a londoni University College-ban épített rendszerben használtak (Fry, 1959; Denes, 1959). 1971-ben az Egyesült Államok Védelmi Minisztériumának Kutatási Ügynöksége (Defense Advanced Research Project Agency, DARPA) négy ötéves kutatási tervet kezdett el finanszírozni, hogy nagy teljesítményű beszédfelismerő rendszereket fejlesszenek ki. A győztes, és egyben az egyetlen rendszer, ami az 1000 szavas szótáron a kitűzött 90%-os pontosságot elérte, a Harpy rendszer volt a CMU-ról (Lowerre, 1976; Lowerre és Reddy, 1980).^[164] A Harpy végső változatát egy Dragon nevű rendszerből származtatták, amit egy CMU-s diák, James Baker épített (Baker, 1975). A Dragon volt az első rendszer, ami RMM-eket használt beszédre. Majdnem egy időben Jelinek az IBM-nél kifejlesztett egy másik RMM-alapú rendszert (Jelinek, 1976). Ettől az időponttól kezdve, a valószínűségi módszerek általában is, de az RMM-ek különösen egyre inkább dominálták a beszédfelismerés kutatását és fejlesztéseit. Az utóbbi éveket a gyarapodó fejlődés, a nagyobb adathalmazok és modellek, és a szigorúbb verseny realisztikusabb beszédhelyzeteken jellemzi. Egyes kutatók megvizsgálták a DBH-k felhasználásának lehetőségét az RMM-ek helyett, azzal a céllal, hogy a DBH-k nagyobb kifejezőerejét kihasználva a beszédképző szervek komplex rejtett állapotából többet tudjanak megragadni (Zweig és Russell, 1998; Richardson és társai, 2000).
A leggyakoribb választás a lineáris Gauss-eloszlás (linear Gaussian), amelyben a gyermek Gauss-eloszlású, ahol a μ várható érték lineárisan változik a szülő értékével, és ahol a δ szórás rögzített. Két eloszlásra van szükségünk, a támogatás és a ¬támogatás esetére különböző paraméterekkel:
Ha az ágens preferenciái eleget tesznek a hasznosság axiómáinak, akkor létezik egy, az állapotokon értelmezett U valós értékű függvény, mellyel U(A) > U(B) akkor és csak akkor, ha A preferált B-vel szemben, és U(A) = U(B) akkor és csak akkor, ha az ágens számára A és B egyformán preferált.
Vizsgáljuk most a rejtett neuronokkal rendelkező hálókat. A legelterjedtebb esetben egy rejtett réteget^[205] szoktak használni, ezt mutatja  ábra. A rejtett réteg hozzáadásának az az előnye, hogy kiterjeszti a háló által reprezentálható hipotézisek terét. Gondoljunk minden egyes rejtett neuronra úgy, mint ami egy lágy küszöbfüggvényt reprezentál a bemeneti térben – lásd a  (b) ábrát. Ezek után gondoljunk úgy egy kimeneti neuronra, mint ami számos ilyen függvény lineáris kombinációjának lágy küszöbfüggvénye. Például összeadva két – egymással szemben álló – lágy küszöbfüggvényt, és küszöbözve az eredményt, a  (a) ábrán látható „hegygerinc” függvényt kapjuk. Egymásra derékszögben álló két ilyen hegygerinc kombinálásával (tehát 4 rejtett neuron kimenetének kombinálásával) a  (b) ábrán látható „dudort” kapjuk.
Természetesen a maximális várható hasznosságú döntés a hasznosság függvény létezésén alapult. Más esetekben, például ha bizonyos tipusú hibák nem összevethetőek (pl. 1. és 2. fajú hibák) akkor egyéb optimalitás is származtatható (pl. Neyman-Pearson lemma).
A „testvér” és a „fején” relációk bináris relációk – ez azt jelenti, hogy két objektum között állítanak fel kapcsolatot. A modell ezenkívül egyelemű, unáris relációkat vagy tulajdonságokat is tartalmaz: A „személy” tulajdonság egyaránt igaz Richárdra és Jánosra; a „király” tulajdonság csak Jánosra igaz (feltehetően azért, mert Richárd ekkor már halott volt); míg a „korona” tulajdonság csak a koronára igaz.  ábra - Egy öt objektumot – két bináris relációt, három egyelemű relációt (címkékkel jelezve az objektumokon) és egy egyargumentumú függvényt, bal láb – tartalmazó modell Egy öt objektumot – két bináris relációt, három egyelemű relációt (címkékkel jelezve az objektumokon) és egy egyargumentumú függvényt, bal láb – tartalmazó modell
De melyik kevert stratégiát? 1928-ban Neumann kifejlesztett egy módszert az optimális kevert stratégia megkeresésére kétszemélyes zérusösszegű játékokra. A zérusösszegű játék^[177] (zero-sum games) olyan játék, amiben a jutalmak a jutalommátrix elemeiben nullára összegződnek. Világos, hogy a snóblijáték ilyen játék. A kétszemélyes zérusösszegű játékoknál tudjuk, hogy a jutalmak egyenlők és ellentétesek, így csak az egyik játékos jutalmait kell figyelembe vennünk, aki a maximáló lesz (ahogy a  fejezetben is). A snóblijátéknál az E páros játékost választjuk maximálónak, így a jutalommátrixot az U[E](e, o) értékekkel definiálhatjuk – a jutalom E-nek, ha E e-t hajt végre és O o-t.
Egyelőre két fő fogalommal – a cselekvésekkel és az objektumokkal – foglalkoztunk. Ideje most utánanézni annak, hogyan illeszkednek ezek a fogalmak egy olyan befogadó ontológiába, ahol mind a cselekvések, mind az objektumok a fizikai univerzum aspektusának foghatók fel. Egy konkrét univerzumról feltesszük, hogy térbeli és időbeli dimenziója is van. A wumpus világ a kétdimenziós rács által definiált térbeli dimenzióval és diszkrét idővel rendelkezett. A világ térben háromdimenziós és időben egydimenziós,^[97] mely dimenziók mindegyike folytonos. Egy általánosított esemény (generalized event) a többdimenziós univerzum egy részének – a „tér-idő darabkának” – aspektusaiból áll össze. Ez az absztrakció az eddig látott fogalmak többségét általánosítja, a cselekvéseket, a lokációkat, az időket, a folyó eseményeket és a fizikai objektumokat beleértve. Az általános ötletet a  ábra szemlélteti. Mostantól kezdve az „esemény” egyszerű fogalommal az általánosított eseményeket fogjuk nevezni.
Az egyetlen elfogadható módszer a modellek tanulása valódi beszédadatból, amiből bizonyosan nincsen hiány. A következő kérdés a tanulás mikéntje. A teljes választ a  fejezetben adjuk meg, de a fő elképzeléseket itt is bemutathatjuk. Gondoljuk át a bigram nyelvi modellt; ennek megtanulását már elmagyaráztuk, ami a szópárok gyakoriságának a vizsgálatán alapult valódi szövegben. Megtehető ugyanez mondjuk a beszédhangok átmenet-valószínűségeire is a kiejtési modellben? A válasz igen, de csak akkor, ha valaki veszi a fáradságot, és végigjegyzeteli az összes előforduló szót a helyes beszédhangsorozattal. Ez egy bonyolult és számos hibalehetőséget magában rejtő feladat, de néhány standard adathalmazra elvégezték, amelyek több órás beszédet tartalmaznak. Ha ismerjük a beszédhang-szekvenciát, akkor a kiejtési modell átmenet-valószínűségeit megbecsülhetjük a beszédhangpárokból. Hasonlóan, ha minden keretre ismert a beszédhangállapot – egy még gyötrelmesebb kézi felcímkézés eredményeképpen – akkor megbecsülhetjük a beszédhangmodell átmenet-valószínűségeit is. Ha minden keretben ismerjük a beszédhangállapotot és az akusztikus jegyeket, akkor pedig megbecsülhetjük az akusztikai modellt, akár közvetlenül a gyakoriságokból (a VK modellek esetében) vagy statisztikai illesztési módszerekkel (a Gauss-modellek keveréke esetében; lásd  fejezet). Fontos A kézi címkézésű adatnak a költsége és a ritkasága, illetve az a tény, hogy az elérhető kézi címkézésű adathalmazok nem feltétlenül reprezentálják a beszélők és az akusztikus feltételek azon típusait, amelyek egy új beszédfelismerési feladatban fellépnek, ezt a megközelítést bukásra ítélik. Szerencsére a várhatóérték-maximalizálás vagy EM algoritmus (expectation-maximization, EM) anélkül is megtanulja az RMM átmenetvalószínűségeit és érzékelő modelljeit, hogy címkézett adatot használna. Kézi címkézésű adatokból származó becsléseket felhasználhatunk a modell kezdeti beállítására; ezután az EM következik, és betanítja a modellt az éppen aktuális feladatra. Az ötlet egyszerű: egy adott RMM és egy megfigyeléssorozat esetén, felhasználhatjuk a  és a  alfejezet simító algoritmusait az egyes állapot-valószínűségek kiszámítására minden időpontban, illetve egy egyszerű kiterjesztéssel az állapot-állapot párok valószínűségét is az egymást követő időpontokban. Ezeket a valószínűségeket felfoghatjuk bizonytalan címkéknek. A bizonytalan címkékből megbecsülhetünk új átmenet- és érzékelési valószínűségeket, majd az EM eljárás megismétlődik. A módszer minden iterációban bizonyítottan növeli a modell adathoz való illeszkedését, és általában a kezdetben beállított kézi címkézésű becslések értékeinél, a paraméterértékek egy sokkal jobb halmazához konvergál.
Mivel az elmefilozófiában a mesterséges intelligencia alapján a funkcionalizmus választása tűnik a legtermészetesebbnek, a funkcionalizmus kritikája tehát gyakran a mesterséges intelligencia kritikájának formáját ölti (mint Searle esetében). Block (Block, 1980) osztályozását követve többféle funkcionalizmust különböztethetünk meg. Az agyállapot-azonosság elmélet egyik variánsa, a funkcionális specifikáció elmélete (functional specification theory) (Lewis, 1966; 1980) a mentális állapotokkal identifikálandó agyi állapotokat a funkcionális szerepük választja ki. A gépi analógiához szorosabban kötődik a funkcionális állapotazonosság elmélet (functional state identify theory) (Putnam, 1960; 1967). E szerint a mentális állapotokat nem fizikai agyi állapotokkal, hanem a kifejezett számítóeszközként felfogott agy absztrakt számítási állapotaival kell azonosítani. Ezek az absztrakt állapotok feltételezés szerint függetlenek az agy specifikus felépítésétől, ami azonban néhányakat arra késztet, hogy a funkcionális állapotazonosság elméletet a dualizmus egy formájának tartsák!
Általában szükséges, hogy egy kórház képeit (jpeg formátumban) külső telephelyről elérhessék, ezt biztosítja a Web szerver megfelelő biztonság mellett. A képnéző állomások tehát a szerverrel állnak kapcsolatban és az archívumból lekért vizsgálatokat lehet rajtuk megtekinteni és feldolgozni.
Az induktív tanulási módszerek akkor működnek a legjobban, ha nem az állapot nyers leírását, hanem inkább az állapot kiértékeléséhez releváns jellemzőket (feature) kapják bemenetként. A „nem a helyén lévő lapkák száma” jellemző segítség lehet, ha az aktuális állapotnak a céltól vett távolságát szeretnénk megjósolni. Hívjuk ezt a jellemzőt x[1](n)-nek. Vehetnénk a 8-as kirakójáték 100 véletlenszerűen generált konfigurációját, és gyűjthetnénk statisztikákat a megoldás aktuális költségéről. Esetleg azt találnánk, hogy ha x[1](n) értéke 5, akkor a megoldás átlagos költsége 14 és így tovább. Ilyen adatok birtokában x[1](n) értékéből jósolni lehetne h(n) értékét. Persze több jellemzőt is lehetne használni. Egy másik jellemző, x[2](n) lehetne például „azon szomszédos lapkapárok száma, melyek a célállapotban is szomszédosak”. Hogyan lehetne x[1](n)-et és x[2](n)-et összekombinálni h(n) jóslása érdekében? Egy szokásos ötlet a lineáris kombináció használata:
A véletlen újraindítások helyett, a környezet feltárására a véletlen vándorlást (random walk) használhatjuk. A véletlen vándorlás egyszerűen véletlen módon választ egy lehetséges cselekvést az aktuális állapotban. Elsőbbséget élvezhetnek az eddig még nem kipróbált cselekvések. Könnyű bebizonyítani, hogy a véletlen vándorlás valamikor megtalálja a célt, vagy feltárja a környezetet, feltéve, hogy a tér véges.^[47] Másfelől a folyamat igen lassú lehet. A  ábra egy olyan környezetet mutat, ahol a véletlen vándorlásnak exponenciálisan sok lépésre van szüksége a cél megtalálásához, mert minden lépésnél a visszafelé haladás kétszer olyan valószínű, mint az előrehaladás. A példa persze kitalált, azonban sok olyan valós állapottér létezik, melynek topológiája ilyenfajta „csapdát” állít a véletlen vándorlás elé.  ábra - Egy környezet, amelyben a véletlen vándorlásnak exponenciálisan sok lépésre lesz szüksége, hogy a célt megtalálja Egy környezet, amelyben a véletlen vándorlásnak exponenciálisan sok lépésre lesz szüksége, hogy a célt megtalálja
Alapvetően ebbe a módszercsoportba sorolhatók az eddig bemutatott egyszerű eljárások, melyek tehát egy adott eljárás szerint pótolnak véletlen értékkel vagy valamilyen tárgyterülettől független heurisztika révén előállított fix értékkel.
Az Országos Kórház- és Orvostechnikai Intézet (ORKI) célul tűzte ki egy olyan információs rendszer kidolgozását, amely az egészségügyi intézmények számára megadja a szükséges információt, illetve fokozza a naprakészséget műszergazdálkodásuk folyamatában.
Mihelyt van egy összeadás-definíciónk, magától értetődik a szorzást ismételt hozzáadásokként definiálni, a hatványozást ismételt szorzásokként, meghatározni az egész számok osztását és maradékait, a prímszámokat és így tovább. Így tehát a teljes számelmélet (beleértve a titkosírást) felépíthető egy konstansból, egy függvényből, egy predikátumból és négy axiómából.
A cél: a hagyományos (crisp) halmazokon értelmezett alapműveletek, a komplementumképzés, a metszet és az egyesítés megfelelőinek értelmezése, bemutatása a fuzzy halmazokra. Ezek közül a leggyakrabban használt fuzzy halmazműveletek képi megjelentését mutatjuk be a következőkben, Matlab környezetben .
A dáma, és nem a sakk volt az első olyan klasszikus játék, amire egy valóban számítógépen futó program képes volt végigjátszani egy teljes játszmát. Christopher Strachey írta az első működőképes dámajátékprogramot (Strachey, 1952). Schaeffer (Schaeffer, 1997) nagyon olvasmányosan leírja a Chinook – a világbajnok dámajátékprogram – fejlesztéstörténetét az összes műhelytitkával együtt.
(Adottnak tételezzük fel a Gyereke(x,y), Házastársa(x,y) és Férfi(x) predikátum halmazt, továbbá az egyszerűbb jelölés érdekében felvesszük a Szülője(y,x), Testvére(x,y) és Nő(x) predikátumokat is, amelyeket a  fejezet tárgyalt. Az alábbi jelölés minden esetben úgy értendő, hogy x ilyen rokonságban van y-nal, azaz pl. Unokája(x,y) azt jelenti, hogy x unokája y-nak.)
Az első vizsgált él a (WA, SA). Ez nyilván nem konzisztens, hiszen a WA változó értékkészletében a RED értékhez nem tudunk megfelelő, konzisztens értéket találni az SA változó értékkészletében. Ezért tehát a WA változó értékkészletéből kivesszük a RED értéket, azaz WA értékkészletét {GREEN, BLUE}-ra redukáljuk, majd miután a (WA, SA) élet kivesszük az él-lista elejéről, a lista végére tesszük az összes WA-ra mutató élet. Ennek eredményeképpen a következő él-listát kapjuk:
Egy másik származtatás azon alapul, hogy az X i  ``független'' változók Y  feltétellel vett eloszlása p( X _ |Y)  egy normális eloszlás azonos kovariancia mátrix-xal N( μ _ , Σ _ _ )  . Ekkor a modelltanulás felfogható egy maximum likelihood becslésének az p ^ (y)  és μ _ ^ , Σ _ _ ^  eloszlásoknak, ami természetesen majd egy optimális lineáris diszkriminátort is meghatároz.
A második pontban már ismertetésre került a Wythoff-Nim, ezért az ott leírtakat nem ismételném meg. Wythoff 1907-ben mutatta meg, hogy a játéknak rendkívül elegáns formulája van. Legyen az aranymetszés arányszáma ϕ= 1+ 5 2  . Legyen a két halom elemszáma n és k, ekkor vegyünk fel egy n*k méretű táblát, legyen a példánkban ez 13x13-as, majd jelentse a táblázat i-edik sora és j-edik oszlopa azt az állást, amikor az egyik halom i, a másik j elemű. Ekkor Wythoff megmutatta, hogy a „vesztő” pozíciók pontosan a ([ϕn],[ ϕ 2 n])  , vagy a ([ ϕ 2 n],[ϕn])  koordinátáknál helyezkednek el, ahol [] alsó egészrészt jelent. Így a „vesztő” állapotok egy ϕ  és egy 1 ϕ  meredekségű egyenesen helyezkednek el a táblázatban.  ábra - Ábra [12] Ábra [12]
Az első csoportba tartozik a biológiai ágens (például az ember),a második csoportba az egyéb fizikai, de nem élő ágenseket soroljuk (például a robotok), és végül a harmadik csoport a számítógépes (például szoftver) ágensekből áll. Az ágensek, és a multi-ágensek rendszerei autonóm működési móddal rendelkező rendszerkomponensek, dinamikusabbá teszik a túl statikus ember-számítógép és számítógép-számítógép viszonyt, és egyre több alkalmazási feladat megoldásánál jutnak szerephez. Mindezekhez bizonyos mértékű intelligencia szükséges, amely a szoftverágensek esetében elsősorban az alkalmazkodó és a tanulási készségre vonatkozik. Az ágenskutatás az 1990-es években terjedt el.
Ebben a feladatban a   egyenlettel definiált béta-eloszlások tulajdonságait vizsgáljuk. a. A [0, 1] tartomány feletti integrálás alapján mutassa meg, hogy a béta[a,b] eloszlás normalizáló faktorát α = Γ(a + b)/ Γ(a) Γ(b) adja, ahol Γ(x) az úgynevezett gamma-függvény, amelynek definíciója: Γ(x + 1) = x × Γ(x) és Γ(1) = 1. (Egész x-ekre Γ(x + 1) = x!) b. Mutassa meg, hogy az átlagérték a/(a + b). c. Állítsa elő a modus(oka)t (a legvalószínűbb θ értéke(ke)t). d. Jellemezze a béta[ε, ε] eloszlást nagyon kis ε esetén. Mi történik, ha egy ilyen eloszlást frissítünk?
Legyen a kezdőállapot az üres tábla, egy lépés legyen egy teljes szó beírása úgy, hogy az minden ponton beleillik a már beírt szavak közé, célállapot az, ahol a teljes tábla ki van töltve. A költségek teljesen lényegtelenek, hisz a megoldási út nem lesz lényeges, így lehet egységnyinek választani egy szó beírásának költségét. Már csak azt kell eldöntenünk, milyen módon akarjuk bejárni a keresési fát! Kézenfekvő mélységi bejárást használni. (Kétirányút nem lehet, hisz nem ismertek a végállapotok (pont azok megtalálása a cél!), szélességit nem érdemes, mert a fának a levelein vannak a megoldások, így a szélességi bejárás a (majdnem) teljes fát átkutatná.)
A funkcionalizmus (functionalism) szerint a mentális állapotok köztes kauzális feltételek a bemenetek és a kimenetek között. A funkcionalista elmélet szerint ha két rendszer izomorf kauzális folyamatokkal rendelkezik, akkor ugyanazok a mentális állapotai is. Egy számítógépes programnak tehát ugyanolyan mentális állapotai lehetnek, mint egy embernek. Azt persze még nem határoztuk meg, hogy az „izomorf” pontosan mit jelent, de azt tételezzük fel, hogy létezik egy olyan absztrakciós szint, ami fölött nem számít a specifikus implementáció; és ha a folyamatok eddig a szintig izomorfak, akkor ugyanazok lesznek a mentális állapotok is.
A következő él, amit meg kell vizsgálnunk, a (Q, SA). Q értékkészlete momentán {GREEN, BLUE}. Ebből csak a BLUE értékhez nem tartozik SA értékkészletében konzisztens érték, így ezt törölnünk kell Q értékkészletéből, azaz Q értékkészlete {GREEN}-re szűkül. Az értékkészlet-szűkítés miatt, miután a listáról töröljük a (Q, SA) élet, a lista végére oda kell tennünk az összes Q-ra mutató élet. Ennek következtében a következő él-lista adódik:
A  fejezetben meghatároztuk a következtetés (inference) fogalmát, és bemutattuk, hogy hogyan érhető el helyes és teljes következtetés az ítéletlogikában. Ebben a fejezetben kiterjesztjük ezeket az eredményeket, hogy olyan algoritmusokat kapjunk, amelyek bármely, az elsőrendű logikában feltehető kérdésre válaszolni tudnak. Ez jelentős eredmény, mivel az elsőrendű logikával többé-kevésbé mindent kifejezhetünk, ha elég alaposan végezzük a feladatot.
Sok probléma esetén a legjobb algoritmus a jó öreg Newton–Raphson-módszer (Newton, 1671; Raphson, 1690). Ez a gyökhelykeresés, azaz a g(x) = 0 alakú egyenletek megoldásának általános módszere. Működésének alapja az x gyök új becslésének számítása a Newton-formula szerint:
Célkitűzés: A radiológiai rendszerek összekapcsolásával biztosítani a különböző gyártmányú és különböző fizikai alapokon működő eszközök által előállított digitális képi anyag egységes és intelligens kezelését, helyi és központi tárolását, visszakereshetőségét.
20. lépés: ebben az utolsó lépésben is ugyanaz történik LCV-vel, mint anélkül: a T=RED értékadás. A heurisztikus értékek ugyanis a következőképp alakulnak: T{RED(0), GREEN(0), BLUE(0)}. A heurisztika értéke azért zérus a T változó mindhárom lehetséges értékére, mivel a T változó nem áll kapcsolatban a KKP semelyik másik változójával sem. E szerint tehát nem meglepő, hogy a T=RED értékadásra kerül sor, hiszen mindhárom érték egyenrangú, így közülük az első kerül kiválasztásra. ...és ezzel gyakorlatilag meg is találtuk a megoldást, éppen ugyanúgy, mint  szakasz -  Visszalépéses keresés előretekintéssel.
Minden egyes szóra gondolhatunk úgy, mint ami meghatároz egy külön P(X[1:t]|szó) valószínűségi eloszlást, ahol X[i] megadja az i-edik keretben a beszédhang állapotát. Tipikusan ezt az eloszlást két részre osztjuk. A kiejtési modell (pronunication model) egy eloszlást ad meg a beszédhangsorok felett (figyelmen kívül hagyva az ütemet és a kereteket), majd a beszédhangmodell (phone model) leírja, ahogyan a beszédhangok leképződnek a keretek szekvenciájára.
Egy népszerű találós kérdés gyerekeknek a következő: „Nincs se bátyám, se nővérem, mégis annak az embernek az apja az én apám fia.” Használja fel a család tárgykör szabályait (lásd  fejezet), és mutassa meg, hogy ki is az említett ember. Használhatja bármelyik, a fejezetben bemutatott következtetési módszert.
Az Alpha rendszerben megfigyelt fiziológiai paraméterek méréséhez kiválasztandó szenzoroknak, mérési metodikának több elvárásnak kell megfelelnie. Ezek garantálják, hogy az otthoni monitorozáshoz szükséges mérési jeleket és paramétereket megfelelő pontossággal, rendszerességgel tudja-e mérni, illetve hogy a kitűzött prevenciós és rehabilitációs feladatokat maradéktalanul el tudják látni. A mérési elvárások legfontosabb szempontjai: * vezeték nélküli működési mód (Bluetooth, Zigbee) * megfelelő érzékenység és megfelelő gyakoriságú mintavételezés * kellően magas működési időtartam elemről, akkumulátorról * testre rögzíthetőség és viselhetőség
[7] Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple plays-Part II: Markovian Rewards, Venkatachalam Anantharam, Pravin Varaiya, Jean Warland, IEEE Transactions on Automatic control, Vol. AC-32, NO. 11, 1987 November
A várható számértékek a példák összegzésével nyerhetők, kiszámítva a P(X[i ]= x[ij], Pa[i ]= pa[ik]) valószínűségeket egy tetszőleges Bayes-háló következtetési algoritmussal. A pontos algoritmusok esetén – beleértve a változó eliminációt is – ezek a valószínűségek mind a következtetés melléktermékeként közvetlenül nyerhetők, a tanuláshoz nincs szükség semmilyen speciális extra számításra. Sőt minden egyes paraméter tanulásához szükséges információ lokálisan rendelkezésre áll.
Az egyes protonok viselkedését nem csupán a külső mágneses tér, hanem a szomszédos molekulák fizikai tulajdonságai is befolyásolják. Ha a környezet a relaxáció során könnyen vesz át energiát a protontól, a csillapodás gyorsabb lesz és fordítva. Közvetlenül a rádiófrekvenciás impulzust követően a testből érkező jel erősségét a rezonanciába kerülő protonok száma, valamivel később azonban már molekuláris környezetük határozza meg. Az egyes szövetek különböző megjelenése annak köszönhető, hogy a bennük lévő protonok mennyisége eltérő, és azok a mágneses térerő, a nagyfrekvenciás elektromágneses impulzus behatásaira is eltérően válaszolnak.
A leírt vásárló ágens egy egyszerű rendszer, és számos finomítása lehetséges. Mégis elegendő a képessége arra, hogy a megfelelő területspecifikus tudással a vásárlót ténylegesen tudja segíteni. Deklaratív konstrukciója miatt könnyen felskálázható bonyolultabb alkalmazásokhoz. Ennek az alfejezetnek a fő célja az volt, hogy kimutassuk, hogy bizonyos tudásreprezentáció – különösképpen a termék hierarchiája – szükséges az ilyen típusú ágensek számára, és ha már az ilyen formájú tudás rendelkezésre áll, a többi már nem is olyan nehéz egy tudásalapú ágens számára.
A Deep Blue sikere megerősítette azt a széles körű hiedelmet, hogy számítógépes játékokban az előrehaladás főleg az egyre hatékonyabb hardver eredménye – ezt a véleményt az IBM is bátorította. Másfelől a Deep Blue megalkotói azt nyilatkozták, hogy a keresés kiterjesztése és a kiértékelő függvény szintén kritikusak voltak (Campbell és társai, 2002). Azonban tudjuk azt is, hogy néhány közelmúltbeli algoritmikus javítás lehetővé tette, hogy standard PC-n futó programok 1992 óta minden Számítógépes Sakkvilágbajnokságot megnyerjenek, sokszor 1000-szer több csomópont keresésére is képes masszívan parallel ellenségeket is legyőzve. A nyesési heurisztikák egész választékának használatával az effektív elágazási tényezőt kevesebb mint 3-ra lehet csökkenteni (a tényleges 35-ös elágazási tényezőhöz viszonyítva). Ezek közül legfontosabb a nulla lépés (null move) heurisztika, amely sekély keresést alkalmazva – ahol az ellenség az indulásnál kétszer is léphet – az állás értékének jó alsó becslését adja. Ez az alsó korlát sokszor lehetővé teszi az alfa-béta nyesést a teljes mélységű keresés költsége nélkül. Fontos még a hatástalanság nyesése (futility prunnig), amely segíti eldönteni, hogy mely lépések vezetnek béta nyeséshez a követő csomópontok szintjén.
Mutassuk be a szűrési folyamatot két lépésen keresztül az alap esernyős példában (lásd  ábra). Feltesszük, hogy a biztonsági őrünknek van valamilyen a priori hite, hogy a 0. napon esett-e, éppen azelőtt, hogy a megfigyelések sorozata elkezdődik. Tegyük fel, hogy ez P(R[0]) = 〈0,5, 0,5 〉. Most a két megfigyelést a következőképpen dolgozzuk fel: * Az 1. napon az esernyő feltűnik, így U[1] = igaz. Az előrejelzés t = 0-ról t = 1-re
Írja meg azt az általános, naiv Bayes-eloszlást használó algoritmust, amely alkalmas P(Ok∣e) alakú keresések megválaszolására. Fel kell tételeznie azt is, hogy az e bizonyíték az érintett változók tetszőleges részhalmazához rendel értékeket.
Ebben a fejezetben a következő kérdésekkel foglalkoztunk: * A filozófusok szóhasználata szerint a gyenge MI kifejezés azt a feltevést jelenti, hogy a gépek valószínűleg képesek intelligensen viselkedni, az erős MI pedig azt a feltevést, hogy az ilyen gépek elmével rendelkezőknek számítanának (és nem elmét szimulálóknak). * Alan Turing visszautasította a „Tudnak-e gondolkodni a gépek?” kérdést, és egy viselkedési tesztet állított a helyébe. Sok, a gondolkodó gépek lehetőségével szemben felhozható ellenvetést megelőlegezett. A mesterséges intelligencia kutatói közül kevesen szentelnek figyelmet a Turing-tesztre, előnyben részesítik, ha a rendszerek gyakorlati problémamegoldó képességére összpontosítunk, nem pedig az emberek utánzására való képességükre. * A modern korban általánosan egyetértenek abban, hogy a mentális állapotok agyállapotok. * Sem az erős MI ellen, sem a mellette felhozható érvek nem meggyőzők. Kevés első vonalbeli MI-kutató gondolja azt, hogy bármi fontos is függ ennek a vitának a kimenetelétől. * A tudatosság rejtély maradt. * A mesterséges intelligencia és a kapcsolódó technológiák által a társadalomra jelentett veszélyek hat fajtáját azonosítottuk. Arra a következtetésre jutottunk, hogy néhány veszély valószínűtlen, kettő azonban további megfontolást érdemel. Az első az, hogy az ultraintelligens gépek, a maitól nagyon különböző, ámde ránk nézvést korántsem kedvező jövőhöz vezethetnek. A második pedig az, hogy a robottechnológia a pszichopatáknak is a kezébe adhatja a tömegpusztító fegyverek kulcsát. Arra jutottunk azonban, hogy ez a fenyegetés inkább származik a biotechnológiából és a nanotechnológiából, mintsem a robotikából.
A kapott funkcionalitás hasonlít a szituációkalkulusra, azonban az időpontokról és az időintervallumokról is nyilatkozhatunk. Így képesek vagyunk a Történik(Kikapcs(LámpaKapcs[1]), 1:00)-val azt mondani, hogy a lámpakapcsolót pontosan 1:00-kor kapcsolták ki.
Laplace közömbösségi elve (principle of indifference) (Laplace, 1816) szerint a tény tekintetében szintaktikailag „szimmetrikus” állításokhoz azonos valószínűséget kell rendelni. Erre vonatkozóan történtek különböző finomítási javaslatok, amelyek Carnap és más filozófusok azon törekvéseiben csúcsosodtak ki, hogy egy olyan precíz induktív logikát (inductive logic) fejlesszenek ki, amely képes bármely állítás helyes valószínűségét tetszőleges megfigyelések alapján meghatározni. Jelenlegi felfogásunk szerint nem létezik különálló induktív logika; inkább úgy gondoljuk, hogy minden ilyen logika egy szubjektív a priori valószínűségi eloszláson alapszik, amelynek hatása a megfigyelések számának növekedésével csökken.
Általában, ha egy feladatnál lehetséges cselekvések egy sorozatával ugyanabba az állapotba visszajutni, és a cselekvések megfordíthatóak, egy adott szituációban általában a cselekvéssorozat végrehajtásának egyik iránya célravezetőbb, mint a másik. Ilyenkor persze ezt érdemes választani, de ha valamiért a heurisztika nem tudja megmondani, hogy melyik irányban kell több lépést elvégezni a cselekvéssorozatból, az vezethet szuboptimális megoldáshoz.
Említettük az előző részben, hogy a kernelgépek kiemelkedően teljesítenek a kézírásos számjegyek felismerésében, de gyorsan felhasználják őket más alkalmazásokhoz is, különösen olyanokhoz, ahol sok bemeneti tulajdonság van. Ennek a folyamatnak részeként számos új kernelt dolgoztak ki, amelyek karakterfüzérekre, fákra és más nemnumerikus adattípusokra alkalmazhatók. Az is megfigyelhető, hogy a kernelmódszer nemcsak optimális szeparátorok keresésére alkalmas, hanem bármely olyan algoritmusra, amely úgy átalakítható, hogy csak adatpontpárok skalárszorzatát használja, mint a  , illetve   egyenlet. Amint ezt sikerült megtennünk, a skalárszorzat kicserélhető egy kernelfüggvényre, és elkészítettük az algoritmus kernelesített (kernelized) változatát. Ez az átalakítás többek közt a k-legközelebbi-szomszéd algoritmusra és a perceptron tanulásra is könnyen elvégezhető.
Example 1.2 Ha potenciális zavaró tényezők nincsenek a priori kizárva, akkor még legalább egy változó megfigyelése szükséges, hogy kizárjuk, hogy egy direkt függést tisztán egy zavaró tényező okoz. Folytatva az előző példát tegyük fel, hogy megfigyelünk egy negyedik W  változót Y,W  direkt függéssel és feltételes függetlenséggel (W  ⊥⊥  {X,Z}|Y)  (stabil eloszlást feltételezve W  függ X  -től és Z  -től is). Mivel Y  függetlenséget okoz, a d-szeparációs szemantika egy Y→W  élet követel meg (figyelembe véve a korábbi v-structúrát), hiszen egy tiszta zavaró tényező hatását *  Y←*→W  mint okozat nem tudná blokkolni.
Az informatikai szolgáltatások sokfélék lehetnek. Fontosak a különböző adatbázisok és esettárak, melyekben a klinikusok megtalálhatják a felmerülő problémák megoldásához szükséges adatokat és ismereteket. A tárolt adatok hatékony lekérdezéséhez standard ontológiákra, fogalomtárakra és keresőprogramokra van szükség, melyek a heterogén és elosztott információtárakból is képesek előkeresni a kért információt.
Vizsgáljuk meg az egzakt következtetés komplexitását általános Bayes-hálókban. a. Bizonyítsa be, hogy bármely 3-SAT probléma redukálható egy egzakt következtetésre egy olyan Bayes-hálóban, amely a konkrét problémát reprezentálja, és így az egzakt következtetés NP-nehéz. (Segítség: gondoljon egy olyan hálóra, amiben minden kijelentésszimbólumhoz, minden klózhoz és a klózok minden konjunkciójához rendre egy-egy csomópont tartozik.) b. A 3-SAT problémában a kielégítő érték-hozzárendelések számának meghatározása #P-teljes. Bizonyítsa be, hogy az egzakt következtetés legalább ennyire nehéz.
A legjobb hivatkozott dolog kiválasztása egy olyan többértelműség feloldási folyamat, amely többféle szintaktikai, szemantikai és pragmatikus értelmezési információ kombinálásán alapszik. Bizonyos nyomravezetők kényszerek formájában adottak. Például a névmásoknak nemben és számosságban egyezniük kell a hivatkozottakkal: a „he” például utalhat Johnra, de nem Marshára; a „they” utalhat egy csoportra, de nem egyetlen személyre. A névmásoknak a reflexivitás szintaktikai kényszereinek is meg kell felelniük. Például a „he saw him in the mirror” mondatban a két névmásnak két különböző emberre kell vonatkoznia, míg a „he saw himself” kifejezésben ugyanarra az emberre kell utalniuk. Vannak a szemantikai konzisztenciára vonatkozó kényszerek is. A „he ate it” kifejezésben a „he” névmásnak olyan dologra kell utalnia, ami eszik, míg az „it” olyanra, amit megehetnek.
Elég elkeserítőnek tűnhet, hogy az előrefelé láncolás tartalmaz egy NP-nehéz illesztési problémát a belső hurokban. Három módja van annak, hogy felvidítsuk magunkat: * Emlékezhetünk arra, hogy a legtöbb szabály a valódi tudásbázisokban kisméretű és egyszerű (mint a bűntény példa szabályai), és nem nagy és komplex (mint a  ábrán látható kényszerproblémánál). Az adatbázisok területén fel szokták tételezni, hogy mind a szabályok mérete, mind a predikátumok argumantumszáma egy konstanssal megadható korlát alatt marad, és így csak az adatkomplexitás (data complexity) miatt kell aggódni – vagyis a következtetés komplexitása miatt, ami az adatbázisban lévő alaptények számának függvénye. Könnyű megmutatni, hogy az előrefelé láncolás adatkomplexitása polinomiális. * Tekinthetjük a szabályok azon csoportját, amelyekre az illesztés hatékony tud lenni. Alapjában véve minden Datalog klózt tekinthetünk úgy, mint ami egy kényszerkielégítési problémát határoz meg, így az illesztés kivitelezhető akkor, ha a megfelelő kényszerkielégítési probléma is nyomon követhető. Az  fejezet leírja a kényszerkielégítési problémák néhány praktikusan megoldható családját. Például ha a kényszergráf (egy olyan gráf, amelynek a csomópontjai változók és az élei kényszerek) fát formáz, akkor a kényszerkielégítési probléma lineáris időben megoldható. Pontosan ugyanez a szabály áll fenn a szabályillesztésre. Például ha eltávolítjuk Dél-Ausztráliát a  ábráról, akkor az új klóz a következő lesz:
Ilyen szintaktikával tehát a játékosok tiszta stratégiáit extenzív játékokban úgy kódolhatjuk, hogy a kód hossza mindig az adott játékos információs halmazainak számával lesz azonos (pl. most az 1-lapos póker esetén 2-hosszúak a kódok az 1-es játékosnál, hiszen az 1-es játékosnak az 1-lapos pókerben csak 2 információs halmaza van), a kódban szereplő számok pedig rendre a megfelelő információs halmazban teendő elemi cselekvést azonosítják. Tehát például 21 a fenti példában azt jelenti, hogy az 1-es játékos az 1-es (Red-es) információs halmazában a 2-es elemi cselekvést, azaz Fold-ot teszi, míg a 2-es (Black-es) információs halmazában az 1-est, azaz a Raise-t.
Egy Gauss-szűrő alkalmazása azt jelenti, hogy az I(x[0], y[0]) intenzitást lecseréljük a minden (x, y)-ra kiszámolt I(x, y)G[σ](d) intenzitások összegével, ahol d az (x[0], y[0]) és az (x, y) pontok távolsága. Az ilyen fajta súlyozott összeg olyan gyakori, hogy van számára egy speciális elnevezés és jelölés. Azt mondjuk, hogy a h függvény az f és a g függvények konvolúciója (convolution) (amit h = f ∗ g alakban írunk), ha
Ennek megfelelően az egészségügyben elsősorban az alábbi főbb területeken terjedt el a számítástechnika: * adminisztráció, - gazdasági ellátás,- műszergazdálkodás, - diagnózis/terápia, - orvosi kutatás/értékelése, - társadalom-orvostan.
Az Otter úgy működik, hogy folyamatosan rezolválja a támogatóhalmaz egy elemét az egyik használható axiómával szemben. A Prologgal ellentétben, ez egy „a legjobbat-először” keresést használ. A heurisztikus funkciója megméri minden egyes klóz „súlyát”, ahol a könnyebb klózokat részesíti előnyben. A heurisztika egzakt kiválasztása a felhasználón múlik, de általában a klóz súlyának korrelálnia kell a méretével vagy a nehézségével.  ábra - Az Otter tételbizonyítás vázlata. A heurisztikus kontrollt alkalmazzák a „legkönnyebb” klóz kiválasztására és a Filter funkcióban, amely kitörli a jelentéktelen klózokat a további vizsgálatból. Az Otter tételbizonyítás vázlata. A heurisztikus kontrollt alkalmazzák a „legkönnyebb” klóz kiválasztására és a Filter funkcióban, amely kitörli a jelentéktelen klózokat a további vizsgálatból.
A predikátum kalkulus egy érdekes, számítógépes nyelvészethez kötődő kiegészítése az úgynevezett „szemantikus hálók” használata. A szemantikus háló egy nem pontosan definiált grafikus ábrázolása egy szöveg szemantikájának (jelentésének). Bizonyos szabályok szerint felrajzolva egy ilyen háló egyenértékűvé tehető egy adott predikátum kalkulusbeli reprezentációval. Vagy másképpen, egy adott predikátum kalkulusbeli reprezentációhoz felrajzolhatunk egy neki megfelelő szemantikus hálót, hogy vizuálisan is megjelenítsük például a szavak közti logikai, jelentésbeli kapcsolatokat. [5]  ábra -  ábra: példa szemantikus hálóra  ábra: példa szemantikus hálóra
Ebben az esetben a nullhipotézis az, hogy az attribútum irreleváns, ennek megfelelően egy végtelen nagy mintahalmazra vett információnyereség nulla lenne. Azt kell kiszámítanunk, hogy a nullhipotézist feltéve egy ν méretű mintahalmazban a várt pozitív és negatív eseteloszlástól a megfigyelt eloszlás eltérése milyen valószínűséggel léphet fel. Az eltérés mértékét megadhatjuk a részhalmazok tényleges pozitív és negatív esetszámának (p[i], n[i]) a nullhipotézis fennállása esetén várt esetszámokkal  való összehasonlításával:
A   megadja θ-ra a [0, 1] tartományban a sűrűségfüggvény értékét. Az α normáló konstans a-tól és b-től függ. (Lásd  feladat.) A  ábrán bemutatjuk, hogy hogyan néz ki az eloszlás különböző a-k és b-k esetén. Az eloszlás átlaga a/(a + b), tehát nagyobb a értékek arra utalnak, hogy Θ-t 1-hez közelebb hisszük, mint 0-hoz. Az a + b nagyobb értékei az eloszlást csúcsosabbá teszik, ami a Θ értéke felőli nagyobb bizonyosságunkat jelenti. Látható, hogy a béta-család a hipotézis prior lehetőségek hasznos választékát nyújtja.
Az adatbázisok gyakorlati haszna attól függ, hogy milyen könnyen és pontosan tudjuk megtalálni bennük a szükséges információt. A különböző adatbázisok között sok az átfedés. A lekérdezéshez tehát olyan keresőprogramokra van szükség, melyek ezekből a heterogén és elosztott információtárakból is képesek kikeresni a kért információt. Lehetővé kell tenni, hogy kulcsszavakkal, szövegesen és grafikusan egyaránt megfogalmazhassa igényeit.
A Scilab egy nyílt forráskódú matematikai programcsomag, amit 1990-ben fejlesztettek ki az INRIA és az ENPC kutatói. Szintaxisa, felépítése és működése nagyon hasonló a MATLAB-éhoz, vagyis mátrixalapú és főként folyamatok szimulálására, jelfeldolgozásra, statisztikai analízisre, függvényábrázolásra és optimalizálásra használják.
Sok kérdés, és egyelőre nem sok válasz. Pedig a helyzet egyszerűbb, mint gondolnánk. Viszont mielőtt még rátérnénk a válaszokra lássuk, hogy mégsem volt értelmetlen a fenti 25 primér változó bevezetése - szükség van rájuk, még ha csak (egyelőre/számunkra) részben is. Modellezzük tehát le korlátokkal azt az említett két állítást (3-asat és 14-eset), amelyek állítólag megragadhatók a fentebb definiált 25 változóval. Az említett két állítás a következő:
A  ábra mutatja az ítéletkalkulus formális nyelvtanát; ha nem ismeri a BNF jelölésrendszert, akkor nézze meg az B.1. szakasz - B1. Nyelvek definiálása Backus–Naur-Formában (BNF) részt.  ábra - Ítéletkalkulus-beli mondatok BNF (Backus–Naur-forma) nyelvtana Ítéletkalkulus-beli mondatok BNF (Backus–Naur-forma) nyelvtana
A kartagok kapcsolódási szerkezetétől függően a kinematikai láncok lehetnek egyszerűek, összetettek, nyíltak, vagy zártak. Egyszerűnek nevezünk egy kinematikai láncot, ha nincsen benne egyetlen olyan kartag sem, amely kettőnél több kinematikai párhoz kapcsolódna. Összetett a kinematikai lánc, ha legalább egy kartag kettőnél több kinematikai párhoz kapcsolódik. A nyílt kinematikai lánc tartalmaz legalább egy olyan kartagot, amely csak egyetlen kinematikai párhoz tartozik. Ha minden egyes kartag legalább két kinematikai párhoz tartozik, akkor zárt kinematikai láncról beszélünk. A manipulátorok modellezésekor ebben a könyvben egyszerű, nyílt kinematikai láncokat tételezünk fel.
Tekintsen egy végtelen hosszú, r sugarú hengert, amelynek tengelye az y tengellyel párhuzamos. A henger Lambert-féle felülettel rendelkezik, és a pozitív z tengely felől irányítjuk rá a kamerát. Mit fog látni a képen, ha a hengert egy pontszerű fényforrás világítja meg, amely a pozitív x tengelyen a végtelenben helyezkedik el. Magyarázza meg a válaszát úgy, hogy az azonos fényességű kontúrokat rajzolja fel a vetített képre. Az azonos fényességű kontúrok azonos távolságban helyezkednek-e el?
Vegyük a P(Betörés∣JánosTelefonál = igaz, MáriaTelefonál = igaz) lekérdezést. A rejtett változók ennél a kérdésnél a Földrengés és a Riasztás. A   egyenletből, a kifejezések rövidítése miatt a kezdőbetűket használva, azt kapjuk, hogy^[149]
A harmadik lépésben gondolunk arra, hogy a kiegészítések mellett az igei (és egyéb) kifejezések segédszavakat (adjuncts) is kaphatnak, melyek nem egy igéhez kötődő kifejezések, hanem mindenféle igei kifejezésben feltűnhetnek. Az időt és helyet jelképező kifejezések segédszavak, mivel szinte minden cselekvés vagy esemény rendelkezhet időponttal és helyszínnel. Például a „now” határozószó az „I smell a wumpus now” mondatban és az „on Tuesday” PP a „give me the gold on Tuesday”-ban segédszavak. Íme, két szabály a segédszavak megengedéséhez:
Ahogyan az  fejezetben említettük, a filozófusok sokkal régebb óta vannak jelen, mint a számítógépek, és folyamatosan próbálnak válaszokat találni néhány olyan kérdésre, amely a mesterséges intelligenciához kötődik: Hogyan működik az elme? Képesek-e gépek intelligensen cselekedni; és ha igen, lenne-e elméjük? Milyen etikai következményei lehetnek az intelligens gépeknek? A könyv eddigi huszonöt fejezetében a mesterséges intelligencia saját kérdéseiről volt szó, most egy fejezet erejéig a filozófusok témáival foglalkozunk.
És így tovább. A beteg helyett szinte mindent az ágensek intéznek. Elrendezik, hogy Y úr felesége dél Olaszországba repüljön, módosítják a férj visszaútjának időpontját, informálják munkáltatóját arról, hogy dolgozójuk pár nap késéssel fog visszatérni.
„A FSzR-en belül a problémák megoldását szimbolikus struktúrákként reprezentáljuk és a FSzR keresés révén mutatja fel a problémamegoldó intelligenciáját – folyamatos generál és módosít szimbolikus struktúrákat, amíg rá nem fut a megoldásra”
A tényezők csoportosított kezelése lehetővé teszi a számunkra, hogy bizonyos alcsoportokat a bemenetek függvényében különbözőképpen súlyozzunk. Például egy egészségügyi kockázat tekintetében egyes kockázati tényezők más-más súllyal jelennek meg a női és a férfi pácienseknél, vagy egy környezeti katasztrófa esetében egy-egy csoport hatása évszakonként más súlyozású lehet.
Ezek szerint tehát meg kellene tudnunk mondani, ki kellene tudnunk fejezni formálisan (a duál reprezentációban bevezetendő változókra kimondott majdani korlátok segítségével), hogy pl. az angol személy melyik színű házban lakik, a spanyolnak milyen háziállata van, adott cigaretta márkát vagy pedig italt milyen színű házban fogyasztanak, stb. Ennek megfelelően vezessük még be (a primér reprezentáción túl) a következő változókat:
Előbbi esetben a szöveg elemzése során abból egy logikusan szervezett, összefüggéseket tartalmazó tudásbázist alakítunk ki, amit aztán megadott szabályok és módszerek segítségével használhatunk fel a szöveg értelmezése vagy új szöveg előállítása során. Ez a modell a természetes nyelv feldolgozás racionalista irányzatával köthető össze, melynek előfeltételezése szerint az emberi agy már születéskor kiterjedt általános tudással rendelkezik a nyelvek megtanulásához. [4] Ebből következően egy feldolgozó programba is be kell építenünk ezt a tudást (vagy egy ehhez feltételezhetően hasonlót), a tudásbázis felépítéséhez és felhasználásához kapcsolódó szabályok formájában. Egy tipikus ehhez a megközelítéshez kapcsolódó mesterséges intelligenciai eszköz lehet a logika (predikátum kalkulus). Ide sorolhatóak még az irányzat nagy úttörője, Noam Chomsky által bevezetett generatív és környezetfüggetlen nyelvtanok. [1][4]  ábra -  ábra: a tudás alapú szövegfeldolgozás folyamatának egyszerű szemléltetése  ábra: a tudás alapú szövegfeldolgozás folyamatának egyszerű szemléltetése
Ebben a fejezetben a megerősítéses tanulással foglalkoztunk: azzal, hogy egy ismeretlen környezetben működő ágens hogyan válhat gyakorlottá pusztán megfigyelései, valamint az esetenként kapott jutalmak felhasználásával. A megerősítéses tanulás a teljes MI-témakör mikrovilágának is tekinthető, amelyet azonban az előrehaladás érdekében egy sor egyszerűsítő feltétel mellett tárgyaltunk. A következő fontosabb megállapításokat tettük: * Az ágens teljes felépítése határozza meg azt, hogy milyen információt kell megtanulni. A tárgyalt három fő felépítési lehetőség: egyrészt a modellalapú, amely a T modellt és az U hasznosságfüggvényt használja; másrészt a modell nélküli felépítés, amely a Q cselekvésérték-függvényt használja, és a reflexszerű, amely a π stratégiát használja. * A hasznosságértékek háromféle megközelítés alapján tanulhatók: 1. A közvetlen hasznosságbecslés (direct utility estimation), amely a megfigyelt hátralévő jutalmat használja egy-egy állapot esetén, mint a hasznosság tanulásának közvetlen jellemzőjét. 2. Az adaptív dinamikus programozás (ADP) (adaptive dynamic programming) megfigyelései alapján egy modellt és egy jutalomfüggvényt tanul, ezek után érték- vagy stratégiaiterációt használ ahhoz, hogy az állapotok hasznosságbecsléséhez vagy az optimális stratégiához eljusson. Az ADP optimálisan használja fel a környezetnek a szomszédos állapotokra vonatkozó struktúrájából adódó lokális kényszereket. 3. Az időbeli különbség (IK) (temporal-difference, TD) megközelítés úgy módosítja a hasznosságbecsléseket, hogy megfeleljenek az adott állapotot követő állapotokra vonatkozó becsléseknek. Tekinthetjük egyszerűen az ADP megközelítés olyan approximációjának is, amely a tanuláshoz nem igényel modellt. Mindamellett, ha a megtanult modellt pszeudotapasztalatok generálására használjuk, gyorsabb tanuláshoz juthatunk. * A cselekvésérték-függvények vagy Q-függvények ADP- vagy IK-megközelítéssel tanulhatók. IK esetén a Q-tanulás sem a tanulás, sem a cselekvés kiválasztásának fázisában nem igényli modell használatát. Ez egyszerűsíti a tanulási feladatot, de potenciálisan korlátozhatja a bonyolult környezetben való tanulási képességet, mivel az ágens nem képes szimulálni a lehetséges cselekvéssorozatok eredményét. * Ha a tanuló ágens a tanulás során a cselekvések kiválasztásáért is felelős, akkor kompromisszumot kell kötnie a választott cselekvés jelenlegi hasznossága és hasznos új információ tanulásának lehetősége között. A felfedezési probléma egzakt megoldása kezelhetetlen, de néhány egyszerű heurisztika elég jó megoldásra vezet. * Nagy állapotterek esetén a megerősítéses tanuló algoritmusnak közelítőfüggvényreprezentációt kell használnia az állapotok feletti általánosítóképesség érdekében. Az időbeli különbség jel közvetlenül felhasználható a neurális háló jellegű reprezentációk paramétereinek frissítésére. * A stratégiakeresési módszerek közvetlenül a stratégia reprezentációját használják, a megfigyelt teljesítmény alapján próbálva meg javulást elérni. Súlyos problémát okoz sztochasztikus területeken a teljesítményben mutatkozó nagy variancia. Szimulációval vizsgálható területeken ez megoldható úgy, hogy előre rögzítjük a véletlen hatásokat.
Az Ipem (Intergrated Planning, Execution, and Monitoring) (Ambros-Ingerson és Steel, 1988) volt az első rendszer, ami integrálta a részben rendezett tervkészítést és a végrehajtást, hogy folytonos tervkészítő ágensre jusson. Az általunk tárgyalt folytonos, részben rendezett tervkészítő ágens (Continuous-POP-Agent) az Ipem a Puccini (Golden, 1998) és a Cypress (Wilkins és társai, 1995) tervkészítők ötleteit kombinálja.
Az előbbi megjegyzés egyenes következménye, hogy egy élő ágens igazából soha nem képes (de nem is törekszik erre) ideálisan optimális megoldást megtalálni. A ténylegesen legjobb megoldás megtalálásához a keresési teret rendszeresen, kimerítően végig kell nézni, aminek ódiuma össze-mérhető a nyers erő módszer nehézségeivel. Azt is szokás mondani, hogy az ember (és más élő ágens) majdnem (közel) optimális megoldásra törekszik, mert a valós életben ez az igazán kifizetődő.
∀x, s Szomszédos([x, y], [x + 1, y]) ∧ Szomszédos([x, y], [x, y + 1])  ábra - Egy tipikus családfa. Az „=” szimbólum a házastársakat köti össze, a nyilak a gyerekekre mutatnak. Egy tipikus családfa. Az „=” szimbólum a házastársakat köti össze, a nyilak a gyerekekre mutatnak.
Ugyebár a célunk az lenne, hogy minél kisebb büntetéssel megússzuk, avagy maximalizáljuk az egyéni hasznunkat. A fenti 2x2-es táblázat (más néven bimátrix) mind a 4 lehetséges kimenetelhez (mindkettő tagad; mindkettő vall; egyik vall, másik tagad; és fordítva) hozzárendeli az előálló kimenetelnek megfelelő hasznokat, avagy kifizetéseket mindkét játékos számára. Ha tehát például az 1-es játékos vall, és a 2-es tagad (azaz nem ismeri el, hogy ők követték el a súlyos bűntényt, pl. bankrablást (még ha esetleg valóban nem is így volt)), akkor az 1-es játékos szabadon elmehet (0 évet kap), és a teljes büntetést a 2-es játékosra verik rá, azaz 10 évet kap (-10 lesz a haszna).
Most úgy gondolhatunk az RMMDF-ekre, mint amelyek keresést igényelnek a hiedelmi állapotok terében, pontosan úgy, ahogy az érzékelő nélküli és az eshetőségi problémákra vonatkozó módszerek a  fejezetben. A fő különbség az, hogy az RMMDF hiedelmi állapot tere folytonos, mivel egy RMMDF hiedelmi állapot egy valószínűségi eloszlás. Például a hiedelmi állapot a 4 × 3-as világ esetében egy pont a 11 dimenziós folytonos térben. Egy cselekvés megváltoztatja a hiedelmi állapotot is, nemcsak a fizikai állapotot, így kiértékelése aszerint az információ szerint történik, amit a cselekvés eredményeként az ágens megszerez. Az RMMDF-ekbe ezért beletartozik az információ értéke is ( alfejezet) mint a döntési probléma egy komponense.
A legtöbb angolul beszélő ember^[277]^ egyetért abban, hogy az első kérdésre igennel kell felelni: a repülőgépek tudnak repülni; de a második kérdésre az angolban a válasz nemleges: a hajók és a tengeralattjárók haladnak ugyan a vízben, de az angolban ezt nem nevezik úszásnak. Sem ezeknek a kérdéseknek persze, sem a válaszoknak nincsen semmilyen hatása a tengerészek, a tengerészeti mérnökök vagy az ezekkel a dolgokkal bármilyen kapcsolatba kerülők mindennapi munkájára. A válaszoknak alig van közük a repülőgépek és tengeralattjárók tervezéséhez és képességeihez; inkább arról szólnak, hogy a szavak használatának milyen módjait választottuk ki. Úgy alakult, hogy az „úszni” szó az angolban azt jelenti, hogy ’testrészeinek mozgásával előrehalad a vízben’, míg a „repülni” szó jelentése az angolban nem határozza meg a helyváltoztatás módját.^[278] A „gondolkodó gépek” gyakorlati lehetősége még csak mintegy ötven éve jelent meg, és ez az idő nem volt elég az angolul beszélőknek, hogy a „gondolkodni” szó az angolban új jelentést kapjon.
P(Csomagoló[i] = piros|Íz[i] = meggy, Θ[1 ]= θ[1]) = θ[1] Fontos Ezek után az egész Bayes-tanulási folyamat formalizálható egy megfelelően konstruált Bayes-háló következtetési problémájaként, amint a  ábrán látható. Egy új példány predikciója egyszerűen azt jelenti, hogy új példányváltozókat adunk a hálóhoz, amelyből egyesekre rákérdezünk. A tanulás és a predikció ezen formalizmusa nyilvánvalóvá teszi, hogy a Bayes-tanuláshoz nem kell semmilyen extra „tanulási elv”. Megállapíthatjuk továbbá, hogy lényegében csak egyetlen tanulási algoritmus van, ami a Bayes-háló következtetési algoritmusa.
(Tart(g, Eredmény(a, s)) ⇔ (a = Megfog(g)) ∨ (Tart(g, s) ∧ (a ≠ Elenged(g))) Fontos A követő állapot axiómák a reprezentációs keretproblémát megoldják, mert az axiómák össz-száma O(AE) literál. Az E hatások és az A cselekvések mindegyike pontosan egyszer kerül említésre. A literálok F különböző axióma között vannak szétosztva, így egy axióma átlagos nagysága AE/F.
A neuronok szinapszisokon keresztül kapcsolódnak egymáshoz. A századfordulón kezdődött, és több hullámban, több mint ötven évig tartott a nagy vita arról, hogy vajon a neuronok végződéseiken keresztül fizikailag összeérnek-e, vagy önálló, diszkrét entitások. A neuron doktrína igazságát az elektronmikroszkóp felfedezése után lehetett véglegesen igazolni.
A világot lehetne úgy szemlélni, hogy primitív objektumokból (részecskékből) és az azokból felépülő összetett objektumokból áll. Az olyan nagy objektumok, mint például az almák vagy az autók szintjén végzett következtetéssel meg tudunk birkózni a primitív objektumok óriási számából eredő bonyolultsággal. A valóság tekintélyes része azonban az egyedesítés (individuation) – az elkülönülő objektumokra való felbontásnak – látszólag ellenáll. A valóságnak ezt a részét az anyag (stuff) általános névvel fogjuk illetni. Példaképpen tételezzük fel, hogy van előttem egy malac és egy kevés vaj.^[94]^ Azt mondhatom, hogy malacból egy van, de a „vaj objektum” nem megszámlálható, hiszen akármilyen része a vaj objektumnak szintén vaj objektum, legalábbis amíg az igazán kicsi részekhez el nem jutunk. Ez az anyag és a dolgok közötti fő különbség. Sajnos nem lesz két malacunk, ha a malacot kettészeljük.
A biológiai indíttatású informatika (lágy számítási modellek) egyik fő ága a természetes idegrendszer tulajdonságaiból kiinduló mesterséges neurális hálózatok. Az élőlények legfontosabb információ feldolgozó rendszerének felépítése és működése alapvetően eltér a digitális elvű számítógépekétől, és teljesítménye számos területen (pl. tanulás, információ-tárolás, asszociatív emlékezés, hibatűrő működés, ….) felülmúlja azt. Az előadás első része bemutatja a természetes idegi hálózatok általános működésének alapjellemzőit és az azokból levonható tanulságokat. Ezt követően ismerteti a mesterséges neurális hálózatok létrehozásánál használt kiinduló megfontolásokat, első modelleket és jellemzőiket. Egy neurális hálózatot alapvetően három tényező (az alkalmazott processzorok jellemzői, a processzorok összekötetési módja, valamint a tanítás mikéntje) határozza meg. Az előadás második része részletesen kitér ezen meghatározó elemek bemutatására és elemzésére, végül elemzi, hogy milyen körülmények esetén, és hogyan célszerű neurális hálózatot alkalmazni.
Nézzük meg tüzetesebben a cselekvések kimenetelét. Nevezetesen, számítsuk ki annak valószínűségét, hogy az ágens a b hiedelmi állapotból a b′ hiedelmi állapotba jut az a cselekvés végrehajtása után. Ekkor, ha ismernénk a cselekvést és a bekövetkező megfigyelést, akkor a   egyenlet a hiedelmi állapot egy determinisztikus frissítését adná: b′ = Előre(b, a, o). Természetesen a bekövetkező megfigyelés még nem ismert, így az ágens a számos lehetséges hiedelmi állapot egyikébe, b′-be kerülhet az előforduló megfigyelés függvényében. Az o érzékelésének valószínűsége, feltéve hogy a b′ hiedelmi állapotban az a cselekvés került végrehajtásra, az összes olyan s′ aktuális állapot feletti összegzéssel adódik, amelyeket az ágens elérhet:
A teljesség (completeness) tulajdonság szintén kívánatos: egy következtetési eljárás teljes, ha képes levezetni minden vonzatmondatot. Valódi szénakazlak esetében, amelyek véges méretűek, nyilvánvalónak tűnik, hogy szisztematikus kutatással mindig eldönthető, hogy a tű a kazalban van-e. Sok tudásbázis esetében azonban a konzekvenciák szénakazlának mérete végtelen, és így a teljesség egy fontos kérdéssé válik.^[65] Szerencsére léteznek teljes következtetési eljárások a logikához, amelyek megfelelően kifejezők ahhoz, hogy számos tudásbázist kezeljenek. Fontos Egy olyan következtetési folyamatot írtunk le, amelynek következményei bármely világban garantáltan igazak, ahol a premisszák is igazak. Nevezetesen, ha a TB igaz a valódi világban, akkor bármely, a TB-ből helyes következtetési eljárással levezetett α mondat szintén igaz a valódi világban. Így, míg a következtetési folyamat a „szintaxison” működik – belső fizikai konfigurációkon, mint például regiszterek bitjein vagy az agy elektromos jelzéseinek mintáin –, addig a folyamat megfelel valódi világ viszonyainak. Ennek megfelelően a valódi világ néhány aspektusa lesz az eset,^[66] mivel a valódi világ bizonyos más aspektusai jelenleg képezik az esetet. Ezt a megfeleltetést a világ és a reprezentáció között mutatja a  ábra.  ábra - A mondatok az ágens fizikai konfigurációi, és a következtetés az a folyamat, amely új fizikai konfigurációkat hoz létre régiekből. A logikai következtetésnek biztosítania kell, hogy az új konfigurációk olyan aspektusait reprezentálják a világnak, amelyek ténylegesen is következnek azokból az aspektusokból, amelyeket a régi konfigurációk reprezentálnak. A mondatok az ágens fizikai konfigurációi, és a következtetés az a folyamat, amely új fizikai konfigurációkat hoz létre régiekből. A logikai következtetésnek biztosítania kell, hogy az új konfigurációk olyan aspektusait reprezentálják a világnak, amelyek ténylegesen is következnek azokból az aspektusokból, amelyeket a régi konfigurációk reprezentálnak. Fontos Az utolsó kérdés, amivel foglalkoznunk kell a logikai ágensek tárgyalásánál, a megalapozottság (grounding) kérdése, ami nem más, mint a kapcsolat, ha egyáltalán létezik ilyen, a logikai következtetési folyamat és a valódi környezet között, amelyben az ágens létezik. Nevezetesen hogyan tudhatjuk meg, hogy a TB igaz-e a valódi világban? (Ezután a TB már csak „szintaxis” az ágens fejében.) Ez egy filozófiai kérdés, amelyről sok-sok könyvet írtak (lásd  fejezet). Egy egyszerű válasz az, hogy az ágens érzékelői létesítik a kapcsolatot. Például a mi wumpus világbeli ágensünknek van egy szagló érzékelője. Az ágensprogram létrehoz egy megfelelő mondatot mindig, ha van illat. Így bármikor, ha ez a mondat a tudásbázisban van, ez igaz a valódi világban is. Ezáltal az érzetmondatok jelentését és igazságát az őket létrehozó érzékelő és mondatkonstruáló folyamatok határozzák meg. És mi a teendő az ágens tudásának egyéb részeivel, mint az a meggyőződése, hogy a wumpus rossz illatot terjeszt a szomszédos négyzetekben? Ez nem egy közvetlen reprezentációja egy egyedi érzetnek, hanem egy általános szabály, például érzékelési tapasztalatokból levezetve, de nem azonos magával a tapasztalatnak a kijelentésével. Az ilyen általános szabályokat a tanulásnak (learning) nevezett mondatkonstruáló folyamat hozza létre, ami a VI. résznek a tárgya. A tanulás nem tévedhetetlen. Lehet, hogy az eset az, hogy a wumpusok mindig rossz illatot árasztanak, kivéve szökőévekben február 29-én, amikor egyébként megfürödnek. Így lehet, hogy a TB nem igaz a valódi világban, de jó tanuló eljárásokkal van ok az optimizmusra.
A Jason lehetővé teszi a különböző forrásfájlok szerkesztését, illetve szükség esetén elvégzi azok fordítását is. A .mas2j fájl betöltését követően a normál futtatást az ablak jobb alsó régiójában található (az Eclipse-ből esetleg már ismert) zöld háromszög gombbal lehet elindítani (lásd a lenti ábrán a piros keret bal oldalán).  ábra - Futtatás és hibakeresés Futtatás és hibakeresés
* ha Red-et kap, akkor Raise, és ha Black-et kap, akkor is. * ha Red-et kap, akkor Raise, de ha Black-et kap, akkor Fold. * ha Red-et kap, akkor Fold, de ha Black-et kap, akkor Raise. * ha Red-et kap, akkor Fold, és ha Black-et kap, akkor is.
Ebben az alfejezetben azt mutattuk meg, hogy még a látszólag bonyolult problémák is pontosan megfogalmazhatók valószínűség-elméleti alapokon, és meg is oldhatók egyszerű algoritmusok segítségével. Hogy hatékony megoldásokat kapjunk, a szükséges összegzéseket a függetlenségekre és feltételes függetlenségekre alapozva egyszerűsítjük. Ezek az összefüggések gyakran egybeesnek a probléma részproblémákra való felbontására vonatkozó természetes felfogásunkkal. A következő fejezetben ilyen összefüggések formális megjelenítésére alkalmazunk módszereket, valamint olyan algoritmusokat fejlesztünk ki, amelyek hatékonyan használhatók e reprezentációk alapján elvégzett valószínűségi következtetések végrehajtására.
A nem ellenőrzött osztályozás vagy klaszterezés (unsupervised clustering) az a probléma, amikor objektumok valamilyen gyűjteményében több kategóriát különböztetünk meg. A probléma nem ellenőrzött, mivel a kategóriacímkék nincsenek megadva. Tegyük fel például, hogy felvesszük százezer csillag spektrumát. Az a kérdés, hogy a spektrumok alapján vannak-e különböző típusú csillagok, és ha igen, akkor hányféle, és mik a jellemzőik? Mindannyian jól ismerünk olyan neveket, mint „vörös óriás” és „fehér törpe”, de a csillagoknak nincs ilyen címke a kalapjukon – a csillagászoknak nem ellenőrzött osztályozást kell végezniük ahhoz, hogy ezeket a kategóriákat felállítsák. Más példák lehetnek: a fajok, nemek, rendek stb. azonosítása a Linné-féle taxonómiában vagy hétköznapi tárgyak természetes kategóriáinak megalkotása (lásd  fejezet).
Egy mesterséges ágens több észlelési modalitással is rendelkezhet. Amin az ágensek és az emberek is osztoznak, az többek között a látás, a hallás és a tapintás. A hallással a beszédfelismerés témán belül, a  alfejezetben foglalkozunk. A tapintás, másképpen a taktilis érzékelés (tactile sensing) a  fejezet témája, ahol a kézi műveleteket végző ágenseknél vizsgáljuk meg ennek a használatát, a fejezet hátralevő része pedig a látással foglalkozik. Egyes robotok aktív érzékelést (active sensing) végeznek, azaz kiküldenek egy jelet, mint például radar- vagy ultrahangjelet, és ennek a jelnek a környezetből érkező visszaverődését vizsgálják.
Két egymástól kissé eltérő - képet érzékelünk, melyek agyunkban a térbeliség érzetét keltik. A szemek legfontosabb optikai része a szemlencse. Szemünk automatikusan követi a képalkotás igényeit, ezt a tudatalatti szabályozást autófókusznak nevezzük. A leképzés eredményeként a retina belső felületén létrejön egy fordított állású kép, melyet az agyunk feldolgoz, és mi egyenes állású képnek érzékeljük.
A debug.DebugAction a saját fejlesztésű belső utasítások mintapéldája. Forráskódja a debug package-ben, a DebugAction.java fájlban található. A debug package név saját választás, tetszőlegesen más is lehetne. A fájlokat az src/java/[package_név] könyvtárba kell elhelyezni, vagyis jelen esetben a DebugAction.java fájl a src/java/debug könyvtárba került. Kicsit zavaró lehet, hogy a környezet forráskódja is az src/java mappában található, de ettől függetlenül tetszőleges package-t ezáltal almappát lehet ebben a könyvtárban tetszés szerint létrehozni.
Sajnos ez a becslés nagyon nehezen alkalmazható közvetlenül a fentiekben felsorolt vagy azokhoz hasonló való életbeli hipotézisekre. Ugyanis pl. egy szennyezés is annyiféle forrásból származhat, annyiféle mechanizmuson keresztül hathat, hogy még a hipotézisek megfogalmazása is legtöbbször rendkívül nehéz, másrészt sokszor nem egy ok van, hanem számos ok összejátszása okozta a bajt. (Pl. a macska elpusztulásához a péntek 13.-án kívül, lehet, hogy kellett a szomszéd által vásárolt pitbull is, az anyóst se hiába kínáltuk a mérgezett süteménnyel…)
Az izületi változó (q) minden egyes értékére az A[0,n](q) manipulátor-mátrix kiszámítható. Az R(q) 3x3-as bal felső sarokmátrix a szerszám orientációját határozza meg, míg a v(q) 3x1-es vektor a szerszámközéppont helyzetét adja. Az orientációs mátrix három oszlopvektora a szerszámhoz rendelt (n) normál-, (o) orientációs és (a) közelítés-vektort a bázis koordináta-rendszerhez viszonyítva jelenti.
ahol a w-k a súlyok és az f-ek az adott táblaállás jellemzői. Sakk esetében az f[i]-k a táblán levő egyes bábufajták számát jelölhetik és a w[i]-k a bábukhoz rendelt pontértékek (gyalog 1, futó 3 stb.) lehetnek.
Az Alpha prototípus alapvető célja, hogy könnyen áttekinthető módon nyújtson információt a monitorozott személy állapotáról és a felügyelő orvos a magas szintű jelek felől mindig eljuthasson a kiváltó okok legalacsonyabb fizikai szintű jeleit mutató képernyőig is. A jelek feldolgozása, lényegkiemelés a beépített intelligencia egyik formája, ami által az Alpha rendszer az orvosi munkaidőből is jelentős szeletet takaríthat meg.
A vásárló vásárlásleírására vonatkozó pontatlan illeszkedés teljes megoldása nagyon nehéz, és szükségessé teszi a természetes nyelvfeldolgozási és információ-kinyerési technikák egész sorát (lásd 22. és  fejezet). Egy kis lépés megengedni a felhasználónak, hogy a különböző attribútumok minimális és maximális értékeit határozhassa meg. Megköveteljük, hogy a vásárló a termékek leírásához az alábbi nyelvtant vegye igénybe:
Egy értelmes ágensnek a felhasználóhoz ésszerű sorrendben kell kérdéseket intéznie, nem szabad lényegtelen kérdéseket feltennie, az információ fontosságát a költségéhez kell viszonyítania, és a megfelelő ponton abba kell hagynia a kérdezősködést. Mindezen képességek elérhetők az információérték használatának a segítségével.
A játékok pontos megoldásához a minimax algoritmust használjuk (lásd  ábra). Ehhez a feltételes tervkészítésben tipikusan két módosítás tartozik. Először is, a max és a min csomópontok vagy és és csomópontokká válhatnak. Nyilvánvalóan a tervnek minden elért állapotban választania kell valamilyen cselekvést, de kezelnie kell ezen cselekvés minden kimenetelét. Másodszor, az algoritmusnak nemcsak egy lépést kell megadnia, hanem egy feltételes tervet kell készítenie. Egy vagy csomópontban a terv egyszerűen a választott cselekvés, amelyet bármi követhet. Egy és csomópontban a terv if-then-else lépések egymásba ágyazott sorozata, melyek minden lehetséges kimenetelhez egy résztervet adnak meg. Ezen lépésekben a feltétel vizsgálatokban teljes állapotleírások szerepelnek.^[124]  ábra - Egy algoritmus a nemdeterminisztikus környezetek által generált és-vagy gráfok keresésére. Feltételezzük, hogy az Állapotátmenet függvény a cselekvések egy listáját adja vissza, melyek mindegyike egy lehetséges kimenetel halmazhoz tartozik. A cél egy feltételes terv megtalálása, ami bármilyen körülmények között elér egy célállapotot. Egy algoritmus a nemdeterminisztikus környezetek által generált és-vagy gráfok keresésére. Feltételezzük, hogy az Állapotátmenet függvény a cselekvések egy listáját adja vissza, melyek mindegyike egy lehetséges kimenetel halmazhoz tartozik. A cél egy feltételes terv megtalálása, ami bármilyen körülmények között elér egy célállapotot.
Bár a rablók problémáját rendkívül nehéz pontosan megoldani úgy, hogy egy optimális felfedezési stratégiát nyerjünk, azonban készíthető egy ésszerű terv, ami végül az ágens optimális viselkedéséhez vezet. Minden ilyen eljárás a végtelen felfedezés határán lehet mohó (VFHM) (greedy in the limit of infinite exploration, GLIE). Egy VFHM-séma alapján működő ágensnek korlátlan számban ki kell próbálnia minden cselekvést az összes állapotban. Ezáltal kerüli el annak veszélyét, hogy egy szokatlanul rossz kimenetelű sorozat miatt véges (nem nulla) valószínűséggel nem talál meg egy optimális cselekvést. Egy ilyen terv alapján működő ADP-ágens végül megtanulja az igazi környezeti modellt. Egy VFHM-tervnek végül mohóvá kell válnia, így az ágens cselekvései optimálissá válnak a megtanult (azaz itt az igazi) modellre nézve. Fontos Rablók és felfedezés
A robotmanipulátorok, eredeti nevükön kéz–szem gépek (hand-eye machine) tanulmányozása meglehetősen más vonalon fejlődött. Egy ilyen kéz–szem gép megalkotására az első igazi kísérlet Heinrich Ernst MH-1-ese volt, amelyet PhD-téziseiben írt le az MIT-n (Ernst, 1961). Az edinburghi Gépi Intelligencia (Machine Intelligence) program már igen korán jelentős eredményeket mutatott fel látásalapú összeszerelő rendszerükkel, amit Freddy-nek hívtak (Michie, 1972). Ezen úttörő kísérletek után rengetegen foglalkoztak geometriai algoritmusokkal determinisztikus és teljesen megfigyelhető mozgástervezési problémákhoz. A robotmozgás-tervezés P-TÁR nehézségét Reif egy termékenyítő cikke mutatta be (Reif, 1979). A konfigurációstér-reprezentációt Lozano-Perez-nek (Lozano-Perez, 1983) köszönhetjük. Nagy befolyást gyakoroltak Schwartz és Sharir publikációi az általuk zongoraszállítóknak (piano movers) nevezett problémáról (Schwartz és társai, 1987).
A CI szűrők a háttértől némileg elkülönülő fényességű sima határvonallal rendelkező kerek területeket keresnek. A keresés alapja a feltételezett kör határvonala mentén elhelyezkedő gradiens vektorok iránya. Amennyiben ezek konvergálnak a középpont felé, a kerek folt megléte valószínűbb. Az eljárás inkább alapoz a terület körszerűségére, mint eltérő fényességére, továbbá jól kezeli az elmosott határokat. Így a nagyon halvány, környezetébe mosódó foltok megtalálására is alkalmas. Végeredményben előáll egy eredménykép, ahol az intenzívebb pixelértékek az előzetes foltmodellnek jobban megfelelő területeket jelölik. Az algoritmusnak nagyon sok módosulata van a vizsgált tartományok szerint. Néhány ezek közül az Iris Filter, a Sliding Band Filter, stb..
Olvasnivaló.  Minden évben Loebner-díjjal jutalmazzák azt a programot, amely a Turing-teszt egy bizonyos verzióját legjobban képes teljesíteni. Kutassa fel a Loebner-díj legújabb győztesét és számoljon be róla. Milyen technikákra támaszkodik ez a megoldás? Milyen hatással lehet ez az MI jelenlegi helyzetére?
Most fordítsuk figyelmünket a tervkészítő algoritmusok felé. A legkézenfekvőbb megközelítés az állapottér-keresés. Mivel a tervkészítési problémák cselekvéseinek leírása tartalmazza az előfeltételeket és a következményeket, a keresés mindkét irányban végrehajtható: vagy előrefelé egy kiindulási állapotból, vagy visszafelé a céltól, amint azt a  ábra is mutatja. Mindezeken túl felhasználhatjuk az explicit cselekvés- és célreprezentációkat, hogy automatikusan hatékony heurisztikákat származtassunk.
Frans Morsh és Ed Schröder a 80-as évek elejétől már dolgoztak egy sakkprogramon. A 90-es években egy német cég - ChessBase – felkérte Morschot, hogy írja meg a Fritz programot. Mathias Feist programozóként dolgozott a ChessBasenek, az ő nevéhez fűződik az Atari ST-ről DOS-ra való átállás.
A tervkészítőnek két cselekvéssorozattal kell előállnia, a JobbZokni-t a JobbCipő követi, hogy a cél első konjunktját elérjük és a BalZokni-t a BalCipő követi a második konjunkthoz. A két cselekvéssorozat kombinálható, hogy teljes tervet nyerjünk. Így a tervkészítő a két részsorozatot egymástól függetlenül kezelheti anélkül, hogy felállítana bármi megkötést arra, hogy az egyik sorozat egy cselekvése a másikéhoz képest előbb vagy később következik. Bármely olyan tervkészítő algoritmust, ami két cselekvést be tud illeszteni egy tervbe anélkül, hogy meghatározná azok sorrendjét, részben rendezett tervkészítőnek nevezünk (partial-order planner). A  ábra a cipő és zokni felvételére készített részben rendezett tervet szemlélteti. Vegyük észre, hogy a megoldást a cselekvések gráfja ábrázolja, nem egy szekvencia. Érdemes megfigyelni az Indít és a Befejez „technikai” cselekvéseket, melyek a terv kezdetét, illetve végét jelölik. Ezeket cselekvésnek nevezni egyszerűsíti a dolgokat, mert így a terv minden lépése egy cselekvés. A részben rendezett megoldás hat teljesen rendezett megoldásnak felel meg, melyek mindegyike a részben rendezett megoldásnak egy sorba rendezése (linearization).  ábra - Egy részben rendezett terv a zokni és a cipő felvételéhez, és a hat lehetséges sorba rendezés a teljesen rendezett tervhez Egy részben rendezett terv a zokni és a cipő felvételéhez, és a hat lehetséges sorba rendezés a teljesen rendezett tervhez
Akkor most vegyünk egy egyszerűbb megközelítést, ami egy nem biztosan nyerő, de jó stratégiát ad. Ez lényegében arról szól, hogyha a célt nézzük, tehát hogy két oldalt összekössünk egy híddal, ne pontokban gondolkozzunk, hanem területekben. Tehát ha lerakunk egy figurát, ne arra törekedjünk, hogy azok a figurák csatlakozzanak egymáshoz, és hogy láncot építsünk belőlük, hanem arra, hogy elfoglaljunk vele egy számunkra fontos területét a pályának, így későbbi előnyökhöz jutva. Ez a játék elején a legfontosabb, hogy stratégiailag fontos helyekre tegyük első figuráinkat, ahelyett, hogy építenénk, vagy az ellenfelet blokkolnánk a játék kezdetén. Az esetek elég nagy százalékában ugyanis az mondható, hogy jobb az ellenfelünk figuráitól távol tenni a sajátjainkat, mintsem közvetlenül az övéi mellé építkezni.
Az egyszerű porszívóágens racionalitásának vizsgálatakor specifikálnunk kellett egy teljesítménymértéket, a környezetet és az ágens beavatkozóit és érzékelőit. Mindezeket a feladatkörnyezet (task environment) címszó alatt fogjuk egyesíteni. A betűszavakban gondolkodók számára ezt TKBÉ – Teljesítmény, Környezet, Beavatkozók, Érzékelők (Performance, Environment, Actuators, Sensors) leírásnak hívjuk. Egy ágens tervezése során az első lépésnek mindig a feladatkörnyezet lehető legteljesebb meghatározásának kell lennie.  ábra - Egy automatizált taxi feladatkörnyezetének TKBÉ-leírása Egy automatizált taxi feladatkörnyezetének TKBÉ-leírása
Tegyük fel, hogy a következő változó az eddigi sorrendnek megfelelően az SA lesz. Mivel értékkészlete már csak egy-elemű, ezért nyilván csak azt az egy értéket tudja kapni a FOR-ciklus megkezdésekor, a BLUE-t (SA=BLUE).
Nathoo ezeket a sebész-robot interakció alapján három osztályba sorolja. Az első egy olyan, sebész által felügyelt rendszer, amelyben a robot által végzett műveleteket előre megtervezik, beprogramozzák, majd a sebész felügyeli elvégzésüket (1. sz ábra). A második egy tele-robotikai rendszer, amelyben a robotot a sebész valós időben irányítja, erő alapú visszacsatolással (kooperatív sebészet). A harmadik rendszerben a sebész egy konzol segítségével irányítja a robotot, amely révén fokozódik a pontosság, csökken a kézremegés okozta trauma (valódi távsebészet).  ábra - A jelenlegi sebészrobotok felosztása 3 csoportba az alkalmazott irányítási koncepció alapján A jelenlegi sebészrobotok felosztása 3 csoportba az alkalmazott irányítási koncepció alapján
A magyar lakosok tájékozottsági szintje az egészségügyi témákat illetően alacsony, nem ismertek a legfontosabb rizikófaktorok, hiányzik a tudatos prevenciós magatartás. Ez a probléma az érdeklődés hiányára vezethető vissza, hiszen a felnőtt lakosság 34%-a egyáltalán nem keres egészségügyi vonatkozású információkat, további 7%-uk pedig teljesen érdektelen ilyen tekintetben.(2.sz ábra)  ábra - Egészségtudatosság, compliance, tájékozottság-orvosi vélemények Egészségtudatosság, compliance, tájékozottság-orvosi vélemények
 ábra - 8. lépés: az előbbi, NT=RED értékadás következményeként a RED érték eltávolítása a WA változó értékkészletéből az előretekintés miatt 8. lépés: az előbbi, NT=RED értékadás következményeként a RED érték eltávolítása a WA változó értékkészletéből az előretekintés miatt
Mivel a jelenlegi behelyettesítés még továbbra sem adja a probléma megoldását, ezért egy behelyettesítetlen változót kell választanunk. Tegyük fel, hogy a Q-ra esik a választás, majd az algoritmus az egyetlen lehetséges BLUE értéket rendeli hozzá első körben (Q=BLUE).
Hasonló modelleket hozhatunk létre minden egyes beszédhangra, akár a hármashangzó környezet hatását is figyelembe véve. Minden szómodell, amikor a beszédhangmodellekkel kombináljuk, egy RMM teljes specifikációját adja. A modell megadja a beszédhang állapotok közötti, keretről keretre történő átmeneteknek a valószínűségeit csakúgy, mint az akusztikus jegyek valószínűségeit minden egyes beszédhang állapothoz.
Ha Aliz okos és racionális, folytatni fogja az érvelést a következőképpen: Bendegúz domináns stratégiája szintén a tanúskodás. Így ő is tanúskodni fog, és mindketten 5 évet kapunk. Amikor mindegyik játékosnak van domináns stratégiája, ezek kombinációját domináns stratégiai egyensúlynak (dominant strategy equilibrium) nevezzük. Általában egy stratégiaprofil akkor van egyensúlyi helyzetben (equilibrium), ha egyik játékos sem nyer stratégiája váltásával, az összes többi játékos stratégiájának változatlansága mellett. Egy egyensúlyi helyzet lényegében egy lokális optimum (local optimum) az eljárásmódok terében: egy csúcs teteje lejtőkkel minden dimenzióban, ahol a dimenziók egy játékos stratégiai választásaihoz tartoznak.
A Chinook dámaprogram kiterjedten használja a végjáték adatbázisokat, amelyek pontos értékeket adnak a játék utolsó hat lépésében előálló összes állásához. Hogyan lehet egy ilyen adatbázist hatékonyan legenerálni?
Arisztotelész a logika kifejezést nem használta, nem is említi. Ő maga mindig „első filozófia”, bölcsesség néven emlegette. A művet magát Rhodosi Andronikos állította össze Arisztotelész több különböző művéből. Címének („fizika utáni”) kétféle értelmezése lehetséges: mivel vagy Arisztotelész Fizika című munkája után jelent meg, vagy pedig mélyebb okok miatt. Arisztotelész mindig azt vallotta, hogy a megismerés kétféle módon lehetséges. Egyik út a puszta fizikai érzékelés. A másik a közvetlen érzékvilágon túlra mutat: az érzékelést követően gondolkodással felfogni, megérteni egy tárgy, egy létező lényegét. Ezt megfelelő következtetések, régebbi tapasztalatok segítségével lehet elérni.
Az első góprogramokat valamivel a dáma- és a sakkprogramok után fejlesztették ki (Lefkovitz, 1960, Remus, 1962), és lassabban fejlődtek a dáma- és a sakkprogramoknál. Ryder tiszta keresésalapú megközelítést használt (Ryder, 1971) a szelektív nyesési módszerek egész palettájával, hogy le tudja győzni a hatalmas elágazási tényezőből adódó problémát. Zobrist feltétel-cselekvés szabályokat használt elfogadható lépések generálására, ha a játékban ismert alakzatok jelentek meg (Zobrist, 1970). Reitman és Wilcox jó eredménnyel kombinálta a szabályokat és a keresést (Reitman és Wilcox, 1979), a legtöbb modern program követte ezt a hibrid megközelítést. A számítógépes gó jelenlegi állását Müller (Müller, 2002) foglalja össze, és tömérdek hivatkozást is közöl. Anshelevich (Anshelevich, 2000) hasonló módszereket a Hex játékra alkalmaz. A friss fejleményekről a Számítógépes Go Szövetség által kiadott Computer Go Newsletter számol be.
* 5.1. szakasz - 1. Ausztrália térképének kiszínezése +  szakasz -  Visszalépéses keresés +  szakasz -  Visszalépéses keresés fokszám (DEG) heurisztikával +  szakasz -  Visszalépéses keresés előretekintéssel +  szakasz -  Visszalépéses keresés előretekintéssel, és legkevesebb megmaradó érték (MRV) heurisztikával +  szakasz -  Visszalépéses keresés előretekintéssel, és legkevésbé korlátozó érték (LCV) heurisztikával +  szakasz -  Visszalépéses keresés élkonzisztencia ellenőrzéssel (AC3/MAC) +  szakasz -  Visszalépéses keresés élkonzisztencia ellenőrzéssel (AC3/MAC), és legkevesebb megmaradó érték (MRV), fokszám (DEG), illetve legkevésbé korlátozó érték (LCV) heurisztikával +  szakasz -  Lokális keresés (Min-Conflicts) +  szakasz -  Összefoglalás * 5.1. szakasz - 2. Zebra feladvány +  szakasz -  A probléma modellje +  szakasz -  A probléma megoldása +  szakasz -  Futási eredmények * Összefoglalás
Mivel az értékadás konzisztens a korlátokkal, ezért a következő behelyettesítéssel dolgozunk tovább: {NSW=RED, Q=GREEN}. Következhet az előretekintés: ennek során a Q-val szomszédos, eddig még behelyettesítetlen változók értékkészletéből vesszük ki a GREEN értéket. Magyarán az SA változó értékkészlete {GREEN, BLUE}-ról {BLUE}-ra szűkül, míg az NT változó értékkészlete {RED, GREEN, BLUE}-ról {RED, BLUE}-ra szűkül.
Tegyük fel, hogy egy döntési fa segítségével generálunk egy tanító mintahalmazt, majd döntési fa tanulást alkalmazunk erre a halmazra. A tanulási algoritmus végül is a helyes döntési fát fogja visszaadni, ha a tanító halmaz mérete a végtelenhez tart? Miért, vagy miért nem?
c) Van egy olyan programja, amely "illegális input rekord" üzenetet ad, ha az input rekordok egy bizonyos állományát dolgozza fel. Ön tudja, hogy egy-egy rekord feldolgozása a többitől független. Szeretné az illegális rekordot megtalálni.
Térjünk vissza az asztal és a szék azonos színűre festésének problémájához, ez alkalommal újratervezéssel. Feltételezzük a teljesen megfigyelhető környezetet. A kiinduló állapotban a szék kék, az asztal zöld, valamint 1 doboz kék és 1 doboz zöld festék áll rendelkezésünkre. Ez a következő probléma definícióhoz vezet:  ábra - A végrehajtás előtt a tervkészítő egy teljes_terv-nek nevezett tervet készít, mely S-ből G-be vezet. Az ágens az E-vel jelölt pontig végrehajtja a tervet. A terv hátralevő részének végrehajtása előtt a szokásos módon ellenőrzi az előfeltételeket, és azt találja, hogy valójában az O állapotban van, és nem az E-ben. Ezután meghívja a tervkészítő algoritmust, hogy készítsen egy javítás-t, mely egy terv az O-ból az eredeti teljes_terv egy P pontjához. Az új terv ezután a javítás és a folytatás (az eredeti teljes_terv maradék része) összefűzéséből adódik. A végrehajtás előtt a tervkészítő egy teljes_terv-nek nevezett tervet készít, mely S-ből G-be vezet. Az ágens az E-vel jelölt pontig végrehajtja a tervet. A terv hátralevő részének végrehajtása előtt a szokásos módon ellenőrzi az előfeltételeket, és azt találja, hogy valójában az O állapotban van, és nem az E-ben. Ezután meghívja a tervkészítő algoritmust, hogy készítsen egy javítás-t, mely egy terv az O-ból az eredeti teljes_terv egy P pontjához. Az új terv ezután a javítás és a folytatás (az eredeti teljes_terv maradék része) összefűzéséből adódik.
Vizsgáljuk meg most a következtetés kérdését. Világos, hogy a következtetés elvégezhető az ekvivalens Bayes-hálóban, feltéve, hogy az RVM nyelvet úgy korlátozzuk, hogy az ekvivalens háló véges és rögzített struktúrájú. Ez analóg azzal a módszerrel, ahogy elsőrendű logikai következtetést végezhetünk el ítéletlogikai következtetéssel, az ekvivalens ítéletlogikai tudásbázisban (lásd  alfejezet). Ahogy a logikai esetben is, az ekvivalens háló túlságosan nagy ahhoz, hogy megkonstruáljuk, különösen ahhoz, hogy kiértékeljük. A sűrű összekötöttség szintén probléma (lásd  feladat). A közelítő algoritmusok, mint az MCMC (lásd  alfejezet), ezért igen hasznosak RVM-kben való következtetésre.
A keresés lényege a következő: egy lehetőséget kiválasztani, és a többit későbbre halasztani arra az eshetőségre, ha az első választás nem vezetne megoldásra. Tételezzük fel, hogy elsőnek Nagyszebent választjuk. Ellenőrizzük, hogy ez célállapot-e (nem az), majd kifejtjük, aminek hatására a Benn(Arad), Benn(Fogaras), Benn(Nagyvárad) és Benn(RimnicuVilcea) állapotokat kapjuk. Ezek után e négy közül bármelyiket választhatjuk, vagy akár vissza is mehetünk és választhatjuk Nagyzeréndet vagy Temesvárt. Folytatjuk a kiválasztást, a célállapot-ellenőrzést és a kifejtést, míg egy megoldást nem találunk, vagy amíg el nem fogynak a kifejtendő állapotok. A kifejtendő állapot kiválasztását a keresési stratégia (search strategy) határozza meg. Az általános fakereső algoritmus informális megfogalmazását a  ábra mutatja.  ábra - Az általános fakeresési algoritmus informális leírása Az általános fakeresési algoritmus informális leírása
A fuzzy rendszer fontos jellemzője, univerzális jellege, ami egyenes következménye annak, hogy a rendszer tanulással szerez ismereteket. Elméletileg bármilyen probléma típusra alkalmazható és ennek megfelelően az alkalmazási köre igen széleskörű. Használják porszívókban, fényképezőgépekben, villanyborotvákban, videó kamerákban, képfeldolgozás során, sőt, az egyik első alkalmazása egy cementmű irányítása volt.
A kép (a) egyes elemeinek értékét soros letapogatással határozzuk meg (b) majd helykódok segítségével mátrix-képpé rendezzük össze (c). A felbontást a képpontok száma határozza meg.(1. sz ábra)  ábra - Számított kép Számított kép
A nemdeterminisztikusság kezelésére négy tervkészítő módszer létezik. Az első kettő a korlátos nemdeterminisztikusság, a második kettő pedig a nem korlátos nemdeterminisztikusság esetén használható: * Érzékelőmentes tervkészítés (sensorless planning): ez a más néven alkalmazkodó tervkészítésnek (conformant planning) nevezett módszer normál, szekvenciális terveket készít, melyeket érzékelés nélkül kell végrehajtani. Az érzékelésmentes tervkészítő algoritmusnak biztosítania kell, hogy a terv a célt minden lehetséges körülmény között elérje függetlenül a valós induló állapottól és a cselekvések aktuális kimeneteleitől. Az érzékelésmentes tervkészítés a kényszerítésen (coercion) alapul, alapgondolata, hogy a világ erőszakkal egy megadott állapotba vihető még akkor is, ha az ágensnek csak részleges információi vannak az aktuális állapotról. A kényszerítés nem mindig lehetséges, így az érzékelésmentes tervkészítés gyakran alkalmazhatatlan. Az érzékelőmentes feladatmegoldást, amely magában foglalja a hiedelem állapottérben történő keresést, a  fejezetben mutattuk be. * Feltételes tervkészítés (conditional planning): a más néven eshetőségi tervkészítés (contingency planning) módszere a korlátos nemdeterminizmust kezeli úgy, hogy egy feltételes tervet készít, amely a különböző eshetőségekhez különböző ágakat tartalmaz. Csakúgy, mint a klasszikus tervkészítésben, az ágens először tervez, majd végrehajtja az elkészített tervet. Az ágens érzékelő cselekvéseket épít a tervbe, hogy ellenőrizze a megfelelő feltételeket, így meghatározhatja, hogy a terv mely részét hajtsa végre. A légi szállítási feladatkörben például olyan terveink lehetnek, amelyek a következőket mondják: „ellenőrizd, hogy az SFO repülőtér használható-e. Ha igen, repülj oda; egyébként repülj Oaklandba”. A feltételes tervkészítéssel a  alfejezet foglalkozik. * Végrehajtás monitorozás és újratervezés (execution monitoring and replanning): ebben a megközelítésben az ágens bármely ezt megelőző tervkészítő technikát felhasználhat (klasszikus, érzékelőmentes vagy feltételes), de használ egy végrehajtás monitorozást (execution monitoring), amely eldönti, hogy a terv használható-e az aktuális állapotban, vagy újra kell gondolni. Újratervezés (replanning) akkor történik, amikor valami hiba lép fel. Ily módon az ágens a nem korlátos nemdeterminisztikusságot is kezelni képes. Például ha az újratervező ágens nem látta előre az SFO lezárásának lehetőségét, észreveheti ezt a szituációt, amikor fellép, és a tervkészítő által új útvonalat találhat a célhoz. Az újratervező ágenseket a  alfejezetben tárgyaljuk. * Folytonos tervkészítés (continuous planning): minden eddig látott tervkészítő arra készült, hogy elérje a célt, és megálljon. A folytonos tervkészítő egy életen keresztüli működésre van tervezve. Kezelni képes a környezetben fellépő nem várt körülményeket még akkor is, ha azok az ágenst egy tervkészítés közepén érik. A célújraformálás (goal formulation) által képes kezelni a célok elhagyását, illetve új célok keletkezését is. A folytonos tervkészítést a  alfejezetben tárgyaljuk.
Láttuk, hogy a tervkészítési gráfok, csak ítéletlogikai cselekvéseket képesek kezelni. Mi van, hogyha a tervkészítő gráfokat a célban változókat is tartalmazó problémákra kívánjuk alkalmazni, mint az Ott(P[1], x) ∧ Ott(P[2], x), ahol x egy véges terület pozícióiból kerül ki? Hogyan tudna egy ilyen problémát átkódolni, hogy tervkészítő gráfokkal dolgozhasson? (Segítség: emlékezzen a részben rendezett tervkészítés Vége cselekvésére. Ehhez milyen előfeltételeknek kellene tartozni?)
Egyes kategóriáknak szigorú definíciói vannak. Egy objektum akkor és csak akkor háromszög, ha egy háromoldalú sokszög. A valódi világban viszont a kategóriák többsége természetes fajtájú (natural kind) anélkül, hogy bármilyen letisztult definíciója lenne. Tudjuk például, hogy a paradicsomok mélyvörös színűek és nagyjából gömbszerűek szoktak lenni, a tetejükön, ahol a száruk csatlakozott, egy kis bemélyedéssel rendelkeznek, nagyjából 5–8 cm átmérőjűek, és vékony, de erős bőrrel, belül hússal, magvakkal és lével rendelkeznek. Azt is tudjuk azonban, hogy eltérések is vannak. Egyes paradicsomok narancsszínűek, az éretlen paradicsom zöld, egyesek kisebbek, illetve nagyobbak, mint az átlag, a miniparadicsomok egységesen kisméretűek. Ahelyett hogy a paradicsomok tökéletes definíciójára törekednénk, inkább egy tulajdonsághalmazzal rendelkezünk, amely arra szolgál, hogy azonosítsunk bizonyos objektumokat, amelyek nyilvánvalóan tipikus paradicsomok, más objektumok esetén azonban lehet, hogy a felismerés csődöt mond. (Lehet egy paradicsom szőrös, mint egy őszibarack?)
A játék egyben szórakoztató és elgondolkodtató is. Első ránézésre egyszerűnek tűnik, mégis meglepetéseket okozhat a nehézsége egy rövid idő eltöltése után. Szabályai egyszerűek, viszont játék közben észre lehet venni a háttérben megbújó matematikát.
Bár az ILP jelenleg a konstruktív indukció problémájának domináló megközelítése, nem csak ezzel próbálkoztak. Az ún. felfedező rendszerek (discovery systems) célja az új koncepciók tudományos felfedezésének általában a koncepció definícióterében történő közvetlen keresés útján történő modellezése. Doug Lenat AM (Automated Mathematician) programja (Davis és Lenat, 1982) szakértő szabályok formájában megfogalmazott felfedező heurisztikákat használt arra, hogy elemi számelméleti koncepciók és sejtések keresését irányítsa. Éles ellentétben a matematikai következtetésre kifejlesztett rendszerekkel, az AM nélkülözte a bizonyítás fogalmát, és csak sejtés szinten tudott működni. A program újra felfedezte a Goldbach-sejtést és az egyértelmű prímtényezőkre bontást. Az AM architektúráját később az Eurisko rendszerben (Lenat, 1983) általánosították, a rendszert a saját felfedező heurisztikákat átírni képes szabályokkal bővítve. Az Eurisko-t más, a matematikai felfedezéstől eltérő tartományokban is alkalmazták, de az AM-nél kisebb sikerrel. Az AM és az Eurisko módszertana vitatott, lásd (Ritchie és Hanna, 1984; Lenat és Brown, 1984).
Ha a célhoz vezető út nem számít, más algoritmusokra is gondolhatunk, amelyek az utakkal egyáltalán nem foglalkoznak. A lokális keresési algoritmusok (local search) csak egy aktuális állapotot (current state) vesznek figyelembe (a többszörös utak helyett) és általában csak ennek az állapotnak a szomszédjaira lépnek tovább. A keresés által követett utat tipikusan nem is tárolják el. Bár a lokális keresési algoritmusok nem szisztematikusak, két kulcsfontosságú előnyük van: (1) igen kevés – általában konstans mennyiségű – memóriát használnak, és (2) sokszor nagy vagy végtelen (folytonos) keresési térben elfogadható megoldást produkálnak ott, ahol a szisztematikus algoritmusok alkalmatlanok lennének.
Miután a képet “deformáltuk” megfigyelhető, hogy a mintázatok elsimultak és a kontúrvonalak viszonylag sértetlenül megmaradtak (lásd 1 és  ábrák). A deformáció mértéke az iterációk számával szabályozható. A deformált képet ún. “maszkként” alkalmazzuk a fontos élek kinyeréséhez. Mindezt az alábbi módon tesszük:
Az irányelvek nem törvényi kötelezettségeket, inkább erkölcsi védelmet jelentenek, így ernyőként védik az alkalmazókat. Az irányelvekben foglaltaktól el lehet térni,de akkor az esetlegesen adódó hibás orvosi alkalmazások („malpractice”) következményeiért az adott egyén, azaz a konkrét személy a felelős.
A mélységi keresés nagyon szerény tárigényű. Csak egyetlen, a gyökércsomóponttól egy levélcsomópontig vezető utat kell tárolnia, kiegészítve az út minden egyes csomópontja melletti kifejtetlen csomópontokkal. Egy kifejtett csomópont el is hagyható a memóriából, feltéve, hogy az összes leszármazottja meg lett vizsgálva. Egy b elágazási tényezőjű és m maximális mélységű állapottér esetén a mélységi keresés tárigénye bm + 1. A  ábra feltételezéseivel élve és feltételezve, hogy a célcsomópont mélységű csomópontoknak nincsenek követőik, azt találjuk, hogy például d = 12 mélység esetén a mélységi keresés 118 kbájtot igényelne a 10 Pbájttal szemben, ami tízmilliárdos redukciót jelent a tárigényben.
A mondatokat a Kijelent használatával adjuk hozzá a tudásbázishoz, pontosan úgy, mint az ítéletlogikában. Ezeket a mondatokat kijelentéseknek (assertions) nevezzük. Például, azt jelenthetjük ki, hogy János egy király, és hogy a királyok személyek:
Vegyünk egy nagyon egyszerű példát. Kedvenc „meglepetés” cukorkánk kétféle, meggy- (nyam-nyam) és citrom- (brrrr) ízben kapható. A cukorkagyártónak sajátos humora van, és mindegyik cukorkát – ízétől függetlenül – ugyanolyan átlátszatlan papírba csomagolja. Az édességet nagyon nagy zsákokban árulják, amelyekből ötféle van – kívülről megint csak megkülönböztethetetlenek egymástól:
Ha egy példa hamis negatív vagy hamis pozitív egy hipotézis szempontjából, akkor a példa és a hipotézis logikailag ellentmondásban van egymással. Feltéve, hogy a példa a tények korrekt megfigyelésén alapul, a hipotézist kizárhatjuk. Logikailag ez a következtetés a rezolúciós szabállyal pontos analógiában van (lásd  fejezet). Hipotézisek diszjunkciója felel meg egy klóznak, a példa pedig megfelel annak a literálisnak, amely a klózban levő egyik literálissal rezolvál. Egy szokásos logikai következtető rendszer tehát elméletileg tud példákból tanulni oly módon, hogy egy vagy több hipotézist kizár. Tegyük fel például, hogy a példát az I[1] állítással jelöljük, és a hipotézistér H[1] ∨ H[2] ∨ H[3] ∨ H[4]. Ha I[1] ellentmondásban van H[2]-vel és H[3]-mal, akkor a logikai következtető rendszer származtatni tudja az új H[1] ∨ H[4] hipotézisteret.
Fontos, hogy kellő megbízhatósággal előre meg tudja becsülni a betegségek kialakulását és a tervezett beavatkozások várható hatásait. A jelenleg rendelkezésre álló nagyszámú morbiditási és túlélési összefüggések is ezt a célt szolgálják.
Ebben a részben leírjuk az Otter (Organized Techniques for Theoremproving and Effective Research) nevű tételbizonyítót (McCune, 1992), különös figyelmet fordítva a vezérlési stratégiájára. Ahhoz, hogy egy problémát készítsen elő az Otter számára, a felhasználónak négy részre kell osztania a tudásbázist: * Egy klózhalmazra, melyet támogató halmaznak (set of support) (vagy sos-nek) nevezünk, és amely definiálja a problémáról szóló legfontosabb tényeket. Minden rezolúciós lépés a támogató halmaz egy tagját rezolválja egy másik axiómához képest, tehát a keresést a támogató halmazra koncentráljuk. * A használható axiómákra (usable axioms), amely egy halmaz a támogató halmazon kívül. Ez háttértudást biztosít a problématerületről. A határ megválasztása a probléma részei (tehát ami az sos-en belül van) és a háttér (tehát a használható axiómák) között a felhasználó megítélésén múlik. * Az egyenletek egy halmazára, amelyeket átírásoknak (rewrites) vagy demodulátoroknak (demodulators) nevezünk. Habár a demodulátorok egyenletek, mindig balról jobbra irányban alkalmazzák őket. Így tehát egy kanonikus formát határoznak meg, amelyben az összes term leegyszerűsödik. Például az x + 0 = x demodulátor kimondja, hogy minden termet az x + 0 formában az x termmel kell helyettesíteni. * Paraméterek és klózok egy halmazára, amelyek meghatározzák a vezérlési stratégiát. A felhasználónak egy heurisztikus funkciót szükséges specifikálni, hogy kontrollálja a keresést, és egy szűrő funkciót, hogy kitöröljön néhány lényegtelen részcélt.
Az elutasító mintavétel legnagyobb hibája, hogy nagyon sok mintát utasít el! Az e bizonyítékkal konzisztens minták aránya exponenciálisan egyre kevesebb, ahogy a bizonyítékváltozók száma nő, így az eljárás egyszerűen használhatatlan komplex problémákban.
Az előbb vázolt két (illetve három) stratégia igen hasznos, hiszen egy csomó következtetést levonhatunk gondolkodás nélkül. Ráadásul a második stratégia segítségével nagy mennyiségű új mezőt felfedhetünk, amelyekre újra alkalmazható az első stratégia... Ám nagyon hamar belefutunk egy olyan helyzetbe, amikor már nem segítenek ezek a módszerek. Ilyenkor jönnek jól a különböző ismert minták.
Ezzel a rövid írással próbáltam bemutatni a sejtautomaták fontosabb jellemzőit, és ebből a rövidségből következik, hogy közel se alaposan. 6 évvel ezelőtt mikor először találkoztam a Conway-féle játékkal, még nem is gondoltam, hogy mennyire komoly tudomány van a sejtautomaták mögött.
Bemeneti változó: A fuzzy rendszer azon változói, aminek ismeretében a hiányzó változók értékeit becsüli a rendszer. Jelen esetben a vízhozam, víz hőmérséklet és lebegőanyag koncentráció képezik a bemeneti változókat.
A SOL is megszűnt illetve átalakult. Mai jogutódnak az ML Tanácsadó és Informatikai Kft. fogható fel. 1999-ig a többprocesszoros illetve elosztott Prolog rendszerek fejlesztésén dolgoztak. A projekt a CS-Prolog (Constraint Solver Prolog) nevet kapta. A rendszer az ML Kft. weblapjáról letölthető.
Egy másik és talán jobb ok arra, hogy a HFH-tervkészítő hatékonyságában higgyünk, az, hogy a gyakorlatban működik. A nagyméretű alkalmazásokhoz használt tervkészítők majdnem mindegyike HFH-tervkészítő, mert a HFH-tervkészítés lehetőséget ad az emberi szakértő számára, hogy a komplex feladatok végrehajtásához szükséges elengedhetetlen tudást átadhassák, hogy a nagy tervek kicsi számítási ráfordítással elkészíthetők legyenek. Például a HFH-tervkészítést az ütemezéssel ötvöző O-Plan-t (Bell és Tate, 1985) arra használták, hogy a Hitachi számára készítsen gyártási terveket. Egy tipikus gyártási sor probléma 350 különböző terméket, 35 összeszerelő gépet és több mint 2000 különböző műveletet tartalmaz. A tervkészítő egy 30 napos ütemezést készít, naponta három 8 órás műszakkal és több millió lépéssel.
Ha bepillantunk a dartmouthi munkatalálkozót javasló anyagba (McCarthy és társai, 1955), látjuk, miért volt szükségszerű, hogy az MI egy külön területté váljon. De miért nem lehetett az MI-kutatást az irányításelmélet, az operációkutatás vagy a döntéselmélet keretein belül tartani, amikor ráadásul ezek célkitűzései nagyon hasonlók az MI célkitűzéseihez? Vagy az MI miért nem lett a matematika egyik ága? Az első válasz az, hogy az MI a kezdetek óta sajátjának tekintette az olyan emberi képességek duplikálását, mint a kreativitás, az önfejlesztés és a nyelv használata. Ezekkel a kérdésekkel semmilyen más terület nem foglalkozott. A másik válasz a módszertanban rejlik. Az említett területek közül tisztán csak az MI tekinthető a számítógépes tudományok egy ágának (bár az operációkutatás szintén súlyt helyez a számítógépes szimulációkra). Az MI az egyetlen olyan terület, ahol bonyolult, változó környezetben autonóm módon működő gépek építése a cél.
Többféle sejtautomata létezik, amelyeket alapvetően a dimenziójuk, és az előre meghatározott generációs szabályaik különböztetnek meg. Ilyenek például a Rule sejtautomaták (Rule 30, Rule 90 stb.), a Wireworld, a Neumann sejtautomata, és a Conway-féle életjáték, amelyek közül néhányat majd részletesebben bemutatok.
Megközelítésünkben a többtestű tervkészítést a  alfejezetben bemutatott részben rendezett tervkészítésre alapozzuk. Az egyszerűség kedvéért teljes megfigyelhetőséget feltételezünk. Van még egy kérdés, ami nem merül fel az egyedülálló ágens esetén: a környezet a továbbiakban nem igazán statikus (static), mert a többi ágens cselekedhet, amíg egy bizonyos ágens gondolkozik. Ezért foglalkoznunk kell a szinkronizációval (synchronization). Az egyszerűség kedvéért feltételezzük, hogy minden cselekvés végrehajtási ideje azonos, illetve hogy az összetett terv minden pontján a cselekvések végrehajtása szimultán történik.
kiinduló állapot ∧ követő állapot axiómák ∧ p ⊨ cél c. Egy szituációkalkulusban elsőrendű követő állapot axiómák esetén (mint a  fejezetben) lehetséges bizonyítani, hogy egy illegális cselekvéseket tartalmazó terv elérje a célt?
Az előző bekezdés megfigyelése ellenére a visszaugrás mögötti ötlet továbbra is használható marad: lépjünk vissza a hiba helyére. A visszaugrás akkor veszi észre a kudarcot, amikor egy változó értéktartománya üressé válik, de sok esetben egy ág már sokkal korábban levágásra lett ítélve. Tekintsük ismét az {NyA = vörös, ÚDW = vörös} részleges hozzárendelést (amelyik, korábbi megjegyzéseink értelmében, inkonzisztens). Tegyük fel, hogy a T = vörös-t próbáljuk, majd az ÚT, Q, V, DA-kra alkalmazzunk hozzárendeléseket. Tudjuk, hogy az utóbbi négy változó esetén semmilyen hozzárendelés sem lesz jó, így végül ÚT próbálgatásánál kifogyunk az értékekből. A kérdés most az, hogy hova lépjünk vissza? A visszaugrás nem fog működni, mert ÚT-nek igenis van az előző hozzárendelésekkel konzisztens értéke: ÚT esetén nincsen teljes konfliktushalmaz a megelőző változók között, amely felelős lenne a kudarcért. Tudjuk azonban, hogy a négy változó, ÚT, Q, V, és DA együttvéve a megelőző változók egy halmaza miatt hiúsul meg, és ezek azok a változók, amelyek közvetlenül konfliktusban állnak a kérdéses néggyel. Ennek alapján ÚT konfliktushalmazának egy mélyebb megértéséhez jutottunk: a konfliktushalmaz a megelőző változók azon halmaza, ami bármely következő változóval együtt felelős ÚT konzisztens megoldásának kudarcáért. Esetünkben ez a halmaz az NyA és az ÚDW, tehát az algoritmusnak ÚDW-hez kell visszalépnie átugorva Tasmaniát. A konfliktushalmaz ilyen definíciójára támaszkodó visszaugrási algoritmust nevezzük konfliktusvezérelt visszaugrásnak (conflict-directed backjumping).
Az n-konzisztencia és az élkonzisztencia között egy széles köztes mező húzódik: az erősebb konzisztencia-ellenőrzések futtatása tovább fog tartani, de több eredménnyel jár az elágazási tényező csökkentésében és az inkonzisztens részleges hozzárendelések felderítésében. Ki lehet számolni a legkisebb olyan k értéket, melyre a k-konzisztencia ellenőrzésének futtatása visszalépés nélküli problémamegoldást tesz lehetővé (lásd  alfejezet), de ez gyakran nem célszerű. A legmegfelelőbb konzisztencia-ellenőrzési szint kiválasztása valójában leginkább empirikus tudomány.
Tanítás során a tanuló adatpárokból egyesével szabályokat generálunk, ami a következő három műveletből áll: * Változó értékei alapján tagsági érték számítása a tagsági függvénnyel. * Szabályok létrehozása. * Szabály elhelyezése a szabálybázisban, hitelességi érték segítségével.
Az előbbihez hasonlóan a következő, (V, NSW) él is inkonzisztens, hiszen a V változó GREEN értékéhez sem tudunk az NSW változó {GREEN} értékkészletéből konzisztens értéket választani. Ezért tehát a GREEN értéket kivesszük a V változó értékkészletéből, azaz V értékkészletét is {BLUE}-ra redukáljuk, majd a (V, NSW) élet töröljük a lista elejéről, és a lista végére odatesszük az összes V-be mutató élet. Így a következő él-listát kapjuk:
Vegyük észre, hogy a szélességi keresés d + 1 mélységben is generál csomópontokat, az iteratívan mélyülő kereséssel ellentétben. Ennek eredménye, hogy az iteratívan mélyülő keresés a szélességi keresésnél gyorsabb, annak ellenére, hogy a csomópontokat többször fejti ki. Konkretizálva, például b = 10 és d = 5 esetén ezek a számok:
A konvenciók ahelyett, hogy minden egyes új problémához kifejlesztenénk őket, tipikusan egyedi multiágens tervkészítő problémák egy egész csoportjához készülnek. Ez rugalmatlansághoz és hibához vezethet, amit néha láthatunk, ha a páros tenisz játszmában a labda nagyjából azonos távolságra van a két partner között. Egy alkalmazható konvenció hiányában az ágensek kommunikálhatnak (communication), hogy egy közös tudást érjenek el az alkalmazható összetett tervből. Például a teniszpáros egy játékosa felkiálthat „Enyém!” vagy „Tiéd!”, hogy jelezze a preferált tervet. A kommunikációs mechanizmusokat nagyobb mélységben a  fejezetben tárgyaljuk, ahol megfigyelhetjük, hogy a kommunikáció nem szükségszerűen szóbeli információcserét jelent. Például egy játékos úgy is közölheti az általa előnyben részesített összetett tervét a másikkal, hogy végrehajtja annak kezdő részletét. A teniszjátszma problémánkban, ha az A ágens elindul a hálóhoz, akkor a B ágens rákényszerül, hogy visszamenjen az alapvonalhoz megütni a labdát, mivel a második terv az egyetlen összetett terv, ami A hálóhoz mozgásával kezdődik. Ez az irányítási megközelítés, amelyet gyakran a terv felismerésének (plan recognition) nevezünk, akkor működik, ha egyetlen cselekvés (vagy cselekvések rövid sorozata) elegendő, hogy egyértelműen azonosítsuk az összetett tervet.
A temporális operátorok szemléletes intuitív jelentését mutatja be a következő ábra. Az ábrán az egyes körök állapotokat jelölnek. Egy állapotból a lehetséges következő állapotokba nyilak mutatnak. A sötétre színezett állapotokban p feltétel igaz. A fehér színű állapotokban p igazságtartalma nem lényeges. A pepita színezésű állapotokban q feltétel igaz.  ábra - Az LTL-beli temporális operátorok jelentése Az LTL-beli temporális operátorok jelentése
A kétszemélyes játékokkal fogunk foglalkozni, ahol a két játékost – a hamarosan nyilvánvalóvá váló okból – max-nak és min-nek fogjuk hívni. max lép először, majd a játékosok felváltva lépnek, amíg a játék véget nem ér. A játék végén a győztes játékos pontokat kap (vagy néha a vesztes kap büntetőpontokat). A játékot formálisan egyfajta keresési problémaként lehet definiálni az alábbi komponensekkel: * A kiinduló állapot (initial state), ami magában foglalja a táblaállást, valamint azt, hogy ki fog lépni. * Egy állapotátmenet-függvény (successor function), amely (lépés, állapot) párok listájával tér vissza, megadva a legális lépéseket és az azokból következő állapotokat. * Egy végteszt (terminal test), ami meghatározza, hogy a játéknak mikor van vége. Azok az állapotok, ahol a játék befejeződött, a végállapotok (terminal states). * Egy hasznosságfüggvény (utility function, amit nyereségfüggvénynek – payoff function – is neveznek) a játék végeredményéhez egy számértéket rendel. A sakkban a végeredmény győzelem, vereség vagy döntetlen lehet, amit a +1, –1 és 0 értékekkel ábrázolhatunk. Néhány játék ennél több végeredményre vezethet. Például az ostáblában a nyereség +192 és –192 között változhat. Ebben a fejezetben főleg a zérusösszegű játékokkal foglalkozunk, bár a nem zérusösszegű játékokat is megemlítjük.
Erre építve született meg Maynard-Smith jóvoltából az Evolúciós Játékelmélet, majd Harsányiék algoritmikai vonatkozásokat taglaló összefoglaló munkáján túl, amely a játékok megoldásával kapcsolatos egzisztenciális tételeken túl immár algoritmusokat is adott a megoldások kiszámítására, a Mechanizmus-tervezés elmélete kezdett el virágozni, és virágzik egészen napjainkig. Ennek segítségével szavazási, aukciós, és egyéb gazdasági mechanizmusokat konstruálhatunk, akár a közjó elérésének érdekében is.
Ez a feladat a döntési listák kifejező erejével foglalkozik (lásd  alfejezet). a. Mutassa meg, hogy egy döntési lista képes bármely Boole-függvény reprezentálására, ha nem korlátozzuk a tesztek méretét! b. Mutassa meg, hogy ha egy teszt legfeljebb k literálist tartalmazhat, akkor a döntési lista minden olyan függvény reprezentálására képes, amely egy k mélységű döntési fával reprezentálható!
Ennek során az első dolgunk egy újabb változó kiválasztása következik. Szerencsére az előbbi esethez képest a konfliktusban lévő változók halmaza egyel csökkent, mivel a V változó már nem ütközik színben egyetlen szomszédjával sem. A konfliktus-halmaz most a következő: NSW, WA, NT, Q, és SA. Ezek közül a változók közül kell most tehát véletlenszerűen választanunk egyet. Tegyük fel, hogy a választás a Q-ra esik.
A döntési problémák hatásdiagram (influence diagram) reprezentációját – ami a valószínűségi változók reprezentációjához egy irányított körmentes gráfot tartalmazott – döntéselemzések során használták az 1970-es évek vége felé (lásd  fejezet), de a kiértékelésre csak felsorolást használtak. Judea Pearl fejlesztette ki az üzenetváltó eljárást, a fa hálókban (Pearl, 1982a) és az egyszeresen összekötött (polifa) hálókban (Kim és Pearl, 1983) történő következtetésre, és szemben az akkor népszerű bizonyossági tényezős rendszerekkel, a diagnosztikai helyett a kazuális valószínűségi modellek létrehozásának fontossága mellett érvelt. Az első szakértői rendszer, ami Bayes-hálókat használt, a Convince volt (Kim, 1983; Kim és Pearl, 1987). A későbbi rendszerek között találjuk a mozgatóidegekkel kapcsolatos rendellenességek diagnosztizálására szolgáló Munin (Andersen és társai, 1989) és a patológiás helyzetek felderítésére szolgáló Pathfinder rendszert (Heckerman, 1991). Messze a legtöbbet használt Bayes-hálós rendszerek a Microsoft Windows diagnóziskijavítás moduljai (például a Nyomtató varázsló) (Breese és Heckerman, 1996) és az Office segéd a Microsoft Office-ban (Horvitz és társai, 1998).
Adott tartománnyal kapcsolatos „statisztikai” kijelentések. Bár a fenti – alapvetően propozicionális jellegű – kijelentésekhez a fenti módon könnyen lehet valószínűségi igazságértéket rendelni, könnyen belátható, hogy az olyan kijelentések esetében, amelyek a területről magáról adnak valamilyen áttekintést (pl. forall X: pred(X)) a fenti módszer már nem feltétlenül a várt eredményt hozza.
1972 novemberében a fejlesztők úgy érezték megérett a rendszer a publikusabb tesztelésre. Elmentek többek között Edinburgh-ba, az Egyesült Államokba, az MIT-re, Bostonba, a Stanfordra. Mindenhol ott hagytak 1-1 példányt az elkészült programjukból. Az innen visszaérkező jelzések és saját tapasztalataik hatására az eddigi Prolog verziójuk továbbfejlesztésén dolgoztak. Céljuk volt, hogy ne csak egy automatikus tételbizonyítót készítsenek, hanem egy komplex programozási nyelvet. Már ekkorra, 1973 tavaszára két alkalmazást is lefejlesztettek a Prolog kezdeti változatával.
Most folytassuk a  ábrán mutatott feltételes eloszlásokkal. Az ábrán minden eloszlás mint feltételes valószínűségi táblázat – FVT (conditional probability table, CPT) van feltüntetve. (A táblázatos formát diszkrét változók esetén lehet használni; más reprezentációkat, amelyek már folytonos változóknál is használhatók, a  alfejezetben írunk le.) Az FVT-táblázatban minden sor az egyes csomóponti értékek feltételes valószínűségét tartalmazza az adott sorhoz tartozó szülői feltétel (conditioning case) esetén. A szülői feltétel a szülő csomópontok értékeinek egy lehetséges kombinációja (egyfajta elemi esemény, ha úgy tetszik). Az egyes sorokban szereplő számok összegének 1-et kell adnia, mivel az adott változó összes lehetséges értéke szerepel a bejegyzésekben. Bináris változók esetén, ha ismerjük, hogy az igaz érték valószínűsége p, a hamis érték valószínűségének 1 – p-nek kell lennie, így gyakran a második számot elhagyjuk, ahogyan ezt a  ábrán is tettük. Általánosabban, egy k bináris szülővel rendelkező bináris változó esetén 2^k[ ]valószínűség adható meg tetszés szerint. Szülő nélküli csomópontok esetén a táblázat csak egyetlen sort tartalmaz, a változó egyes értékeinek a priori valószínűségeit.
Az ötlet az „indifferencia” és a „legjobb válasz” fogalmaira épít. Nézzük csak meg, hogy mi lenne a 2-es játékos legjobb válasza a p12 adott értékeire! Figyeljük meg, hogy a második egyenletben a 2-es játékos valójában nem befolyásolhatja az ax+b alakból a b tagot, csak az a taggal van dolga. Ha a pozitív, akkor p22-őt minél nagyobbra, azaz 1-re kell vennie (ésszerű, ha maximalizálni akarja a hasznát), míg amennyiben a (pirossal körbekarikázott rész a második egyenletben) negatív, úgy p22 értékét zérusnak célszerű választania. Amennyiben a zérus, úgy a 2-es játékos indifferens. Ekkor teljesen mindegy, hogy mi p22 értéke, hiszen ez úgysem változtat a hasznán.
Az ismétlődő játékok jelenleg ismert megoldási módszerei a  fejezetből ismert többfordulós játékok megoldási módszereihez hasonlítanak abban, hogy egy játékfa építhető a gyökértől lefelé, és megoldható a levelektől felfelé. A fő különbség az, hogy egy gyerekcsomópont értékei feletti egyszerű maximum- vagy minimumképzés helyett az algoritmusnak minden szinten meg kell oldania egy kevert stratégiájú játékot, feltételezve, hogy a gyerekcsomópontok már megoldottak, és jól definiált értékük felhasználható.
Következésképpen ahhoz, hogy ezt a megközelítést használni tudjuk, P(s) helyett P(s∣¬m)-et kell tudnunk becsülni. Nincs ingyenebéd – van, hogy ez könnyebb és van, hogy nehezebb. A normalizált Bayes-tétel általános alakja
A  ábra bemutatja a generált bizonyítási fát. Vegyük észre, hogy új következtetések már nem lehetségesek ezen a ponton, mivel minden mondatot, amelyet kikövetkeztethetnénk az előrefelé láncolással, már explicit módon tartalmaz a TB. Egy ilyen tudásbázist a következtetési folyamat fix pontjának (fixed point) nevezzük. Az elsőrendű határozott klózok előrefelé láncolásával létrehozott fix pontok hasonlítanak az ítéletlogikai előrefelé láncolással (7. szakasz - Előre- és hátrafelé láncolásrészben) generáltakhoz. A leglényegesebb különbség az, hogy egy elsőrendű fix pont tartalmazhat univerzális kvantorral ellátott atomi mondatokat.
(Pietro Perona nyomán.) A  ábra X és Y pontban két kamerát mutat, amelyek megfigyelnek egy jelenetet. Rajzolja le azt a képet, amit az egyes kamerák látnak (felteheti, hogy minden jelölt pont ugyanabban a vízszintes síkban helyezkedik el). Mi tudható meg a két kép alapján az A, B, C, D és E pontok a kamera alapvonalától mért relatív távolságáról, és milyen alapon?  ábra - Felülnézeti kép egy kétkamerás látórendszerről, amely egy üveget figyel meg, amely mögött egy fal van Felülnézeti kép egy kétkamerás látórendszerről, amely egy üveget figyel meg, amely mögött egy fal van
A fentebbi fólia már az előbb bevezetett normál formában definiált játékok tulajdonságairól beszél. Ahhoz, hogy egy játék szimmetrikus legyen, először is minden játékos stratégia-halmazának meg kell egyeznie (alapesetben ezek végesek), másrészt bárhogy is permutáljuk a játékosokat, a játékosok szemszögéből nézve ne változzon a kimenetelek haszna. Két játékos esetén ezt azt jelenti, hogy amennyiben a két játékos helyet cserélne (az egyik a másik szerepébe bújna), akkor végső soron ugyanazzal a szituációval szembesülne (ugyanazok a lehetőségek, kimenetelek, hasznosságok). A zéró-összegű játék pedig olyan, hogy minden egyes kimenetel esetén (minden stratégia-kombinációra) igaz, hogy a játékosok kifizetéseinek összege zérus.
Cselekvések definiálására alkalmazhatók az állapotátmenet-függvények, amelyek egy adott állapothoz (cselekvés, állapot) párok egy halmazát rendelik, aszerint, hogy az adott állapotból mely cselekvések hajthatók végre, és azok mely állapotokba vezetnek.
Az S és G halmazok S[i] és G[i] elemeivel kell foglalkoznunk. Az új példa bármelyik esetén lehet hamis pozitív vagy hamis negatív. 1. S[i]-re hamis pozitív. Azt jelenti, hogy S[i] túl általános, de mivel S[i]-nek (a definíciója értelmében) nincs konzisztens szűkítése, így S[i]-t eltávolítjuk az S halmazból. 2. S[i]-re hamis negatív. Azt jelenti, hogy S[i] túl szűk, így S[i]-t az összes közvetlen általánosításával helyettesítjük, feltéve, hogy ezek szűkebbek, mint G egyes elemei. 3. G[i]-re hamis pozitív. Azt jelenti, hogy G[i] túl általános, így az összes közvetlen szűkítésével helyettesítjük, feltéve, hogy ezek általánosabbak, mint S egyes elemei. 4. G[i]-re hamis negatív. Azt jelenti, hogy G[i] túl szűk, de mivel nincs konzisztens általánosítása (definíciója értelmében), így G[i]-t eltávolítjuk a G halmazból.
Egy új objektum felismerésénél két esetet kell megkülönböztetni. Ha csak egy objektum feltételezhető a vizsgált képen, akkor közvetlenül használhatók a tárolt hisztogramok, hiszen azok egy adott objektumot jellemeznek. Szimpla Bayes szabállyal kikereshető az objektum, aminek a legnagyobb a valószínűsége feltéve a megfigyelt hisztogramot. Ha az egy objektumra vonatkozó feltételezés nem biztosítható, tehát több objektum lehet a képen, akkor a Probabilistic latent semantic analysis (pLSA) vagy a Latent Dirichlet Allocation (LDA) módszerek vethetők be.
Vegyen m darab (x[j], y[j]) adatpontot, az x[j]-ből a   egyenlet alapján generálja y[j]-t. Keresse meg azokat a θ[1], θ[2] és σ értékeket, amelyek maximálják az adatok feltételes log likelihood értékét.
Először a problémamegfogalmazás folyamatát írjuk le, majd a fejezet további részeit a Keresés függvény különböző változatainak ismertetésére fordítjuk. Ebben a fejezetben nem tárgyaljuk részletesen az Állapot-Frissítés és a Cél-Megfogalmazás függvényeket.
Az egyik legegyszeűbb útkereső algoritmus akadályokkal teli terepen a célt az akadályok sarokpontjai éríntésével haladó pálya számítása. Az alábbi animáción a robot ezt az eljárást alkalmazza, mohó keresési algoritmusra alapozva.
Van néhány bonyolultabb rész az előbbi gondolatmenetben. A fő kérdés a tanító és a tesztpéldák viszonya: végül is mi azt szeretnénk, ha a hipotézis nem csupán a tanító halmazon lenne közelítőleg helyes, hanem elsősorban a teszthalmazon. Az alapvető feltevés az, hogy a tanító és a teszthalmazt ugyanolyan valószínűség-eloszlást használva, véletlenszerűen és függetlenül választottuk ugyanabból a példapopulációból. Ezt stacionaritási (stationarity) feltevésnek nevezzük. A stacionaritási feltevés nélkül az elmélet semmilyen, a jövőre vonatkozó igénnyel nem léphet fel, mivel nem lesz szükségszerű kapcsolat a múlt és a jövő között. A stacionaritási feltevés támasztja alá azt a gondolatot, hogy a példákat kiválasztó eljárás nem rosszindulatú. Persze ha a tanító halmaz kizárólag furcsa példákat tartalmaz – kétfejű kutyákat például –, akkor a tanuló algoritmus nyilvánvalóan nem tehet mást, mint sikertelen általánosításra jut a felől, hogy mi módon lehet a kutyákat felismerni.
Írjon egy általános tény- és axiómahalmazt, hogy reprezentálja a következő állítást: „Wellington hallott Napóleon haláláról”; és hogy helyesen megválaszolhassuk a kérdést: „Hallott Napóleon Wellington haláláról?”.
Az maradt hátra, hogy megmutassuk, hogy ez a P[1], …, P[k] hozzárendelés modellje S-nek, feltéve, hogy RC(S) zárt a rezolúcióra, és nem tartalmazza az üres klózt. Ennek bizonyítását meghagyjuk feladatnak.
 ábra - 6. lépés: az előbbi, NT=GREEN értékadás következményeként ellentmondásra jutottunk, mivel Q értékkészlete üresre redukálódott az AC3/MAC következtetés során 6. lépés: az előbbi, NT=GREEN értékadás következményeként ellentmondásra jutottunk, mivel Q értékkészlete üresre redukálódott az AC3/MAC következtetés során
A hegymászás sikere nagyban függ az állapottér „felszínének” alakjától: ha azon csak néhány lokális maximum és fennsík található, akkor a véletlen újraindítású hegymászó algoritmus gyorsan meg fog találni egy jó megoldást. Egy valódi problémához egy olyan felszín tartozik, ami leginkább egy sündisznócsaládhoz hasonlít egy sima padlón, ahol minden sündisznótüske csúcsát egy további miniatűr sündisznó lakja és így a végtelenségig. Ha a probléma NP-teljes, akkor minden bizonnyal exponenciálisan sok lokális maximummal rendelkezik, melyekben beragadhatunk. Ennek ellenére általában kisszámú újraindítás után már elfogadhatóan jó megoldást lehet találni.
A rete hálók, és más, hatékonyságot növelő fejlesztések mindig is jelentős szerepet játszottak az úgynevezett produkciós rendszerekben (production systems), amelyek az első széles körben használt előrefelé láncolási rendszerek voltak.^[90] Az Xcon szakértői rendszer (eredetileg R1-nek hívták, McDermott, 1982) egy produkciós rendszer felépítését felhasználva készült. Az Xcon néhány ezer szabályt tartalmazott számítógép-tartozékok konfigurációinak megtervezésére a DEC cég vásárlóinak számára. Ez volt az egyik első igazi kereskedelmi siker a szakértő rendszerek feltörekvő piacán. Sok más hasonló rendszer épült ugyanezt a technológiát felhasználva, amelyet be is építettek egy általános célú programozási nyelvbe, az Ops-5-be.
A térképezéshez használatos KKSZ algoritmus hasonlít a korábban a lokalizációhoz használt KKSZ-re. A legfontosabb különbséget a referenciapont-változók hozzáadása jelenti a posteriorban. A referenciapontok mozgásmodellje triviális: nem mozognak. Így az f függvény azokra a változókra az identitásfüggvény. A mérési függvény alapvetően megegyezik a korábban használttal. Az egyetlen különbség a KKSZ frissítési egyenletben az, hogy a H[t] Jacobi-mátrixnak nemcsak a robot pozícióját kell figyelembe vennie, hanem a referenciapontok t-ben megfigyelt helyzetét is. Az így kapott KKSZ-egyenletek még ijesztőbbek, mint a korábbiak, ezért itt nem is tárgyaljuk őket.
Az alapvető megközelítés, amit követünk, hasonló a  fejezetben leírt szituációkalkulus alapjául szolgáló ötlethez: a változás folyamatát pillanatfelvételek sorozatának tekinthetjük, amelyek mindegyike egy adott pillanatban írja le a világ állapotát. Minden pillanatfelvétel vagy időpont (time slice) valószínűségi változók egy halmazát tartalmazza, amelyek némelyike megfigyelhető, némelyike pedig nem.
Logisztikus regresszió a megszokott eszköz egy kimeneti Y  változó függésének a vizsgálata az X _ _  bemeneti változóktól, mind megfigyelési, mind eset-kontroll adatok esetén. Az egyszerűség kedvéért bináris kimeneti értékeket y, y ¯  és bináris bemeneti értékeket x i , x ¯ i  tételezve fel, az interakciós tagok nélküli modell az x i  , i=1,…,n  bemenetekhez tartozó esélyhányadosokat és a bias tagot tartalmazza Ψ 0  ( x 0 = Δ 1  ):
A második terület, ahol a mátrixjelölés javítást jelez, a menet közben történő (online) simítás állandó időkülönbözettel. Az a tény, hogy a simítás elvégezhető állandó tárigénnyel azt jelzi, hogy léteznie kell egy hatékony rekurzív algoritmusnak menet közbeni simításra – azaz olyan algoritmusnak amelynek az időkomplexitása független az időkülönbözet hosszától. Tegyük fel, hogy az időkülönbözet d; azaz a simítást t – d időpontban végezzük, ahol t a jelenlegi időpont. A   egyenlet szerint ki kell számolnunk a t – d időpontra az
A Sobel operátor egyszerű, de hatékony gradiens közelítést tesz lehetővé. Az iránymenti deriváltakat közelíti véges differenciával, melyekből a gradiens vektor normája és szöge számítható. Az iránymenti deriváltak számítási algoritmusa irányonként egy 3x3-mas mátrixszal való konvolúcióból áll. A végeredmény ekvivalens egy közelítő Gauss simítás utáni véges differencia számítással. A konvolúciók szeparálhatók két darab három hosszú vektorral való konvolúcióra, melynek köszönhetően az algoritmus igen gyors lesz. A módszer hátránya, hogy nem invariáns a forgatásra. Ebből a szempontból jobb megoldást jelenhet például a Scharr operátor, mely a Gauss simítás pontosabb közelítésével ér el jobb forgatás invarianciát.
A (c)-ben két párt véletlenszerűen választunk ki reprodukcióra, a (b)-beli valószínűségeknek megfelelően. Figyeljük meg, hogy egy egyedet kétszer, egy másikat viszont egyszer sem választottunk ki.^[43] Minden keresztezendő párnál egy keresztezési (crossover) pontot választunk a füzérbeli pozíciók közül. A  ábrán a kereszteződési pontok az első párnál a harmadik számjegy után, a második párnál az ötödik számjegy után vannak.^[44]  ábra - A  (c) ábrán látható első két szülőnek és a  (d) ábrán látható két utódnak megfelelő 8-királynő állapotok. Az árnyalt oszlopok a kereszteződés során elvesznek, a nem árnyalt oszlopok megmaradnak. A  (c) ábrán látható első két szülőnek és a  (d) ábrán látható két utódnak megfelelő 8-királynő állapotok. Az árnyalt oszlopok a kereszteződés során elvesznek, a nem árnyalt oszlopok megmaradnak.
Sajnos számos olyan logikai függvény van, amelyet a küszöbfüggvényt használó perceptron nem tud reprezentálni. A   egyenletet vizsgálva látjuk, hogy a küszöbérték-perceptron akkor és csak akkor ad 1-et, ha bemeneteinek (beleértve az eltolás- bemenetet is) súlyozott összege pozitív:
A tételbizonyítók (amelyeket automatizált következtetőknek is szoktunk nevezni) két szempontból különböznek a logikai programozási nyelvektől. Először is a legtöbb logikai programozási nyelv csak Horn klózokkal dolgozik, ezzel szemben a tételbizonyítások elfogadják a teljes elsőrendű logikát. Másodszor, a Prolog programok egymásba fűzik a logikát és a kontrollt. Ha a programozó választása az A:-B, C-re esik az A:-C, B helyett, ez a program végrehajtására van hatással. A legtöbb tételbizonyításban a mondatok választott szintaktikai formája nincs hatással a kapott eredményekre. A tételbizonyításoknak is szükségük van az információ kontrolljára, hogy hatékonyan működhessenek, de ezt az információt általában külön tárolják a tudásbázistól, ahelyett, hogy magának a tudásreprezentációnak a részeként jelenne meg. A legtöbb kutatás a tételbizonyítások tárgyában magában foglalja az általában hasznos kontrollstratégiákat, amelyek a sebességet is növelhetik.
Szórakozás. A robotok megkezdték a játék- és szórakoztatóipar meghódítását is. A  (b) ábrán már találkoztunk a Sony AIBO-val, ami bár kutyaszerű játék, világszerte használják MI-kutatólaborokban különböző projektek platformjaként. Az egyik, nagy kihívást jelentő feladat, ahol az AIBO-kat fizikai eszközként használták, a robotfoci (robotic soccer): versengés két csapat között, az emberi focihoz hasonló szabályokkal, de autonóm mobil robotokkal. A robotfoci nagyon jó lehetőségeket kínál az MI-kutatásokhoz, mert sok olyan problémát foglal magában, ami máshol, komolyabb alkalmazásokban is megjelenik. Az évente megrendezésre kerülő robotfoci-bajnokságok sok MI-kutatót vonzzanak, és érdekesebbé, izgalmasabbá tették a robotikának ezt az ágát.
A  ábrán láthatóhoz hasonló sűrűségbecslések a bemeneti tér feletti együttes eloszlásokat határoznak meg. A Bayes-hálóktól eltérően a példányalapú reprezentációk nem tartalmaznak rejtett változókat, ami azt jelenti, hogy nem alkalmazhatunk nem ellenőrzött osztályozást, mint ahogy a kevert Gauss-modellnél tettük. Továbbra is alkalmazhatjuk a kívánt y értéknek az x bemeneti tulajdonságok alapján történő jóslására a sűrűségbecslést, ha kiszámítjuk P(y|x) = P(y, x)/P(x)-et, feltéve, hogy a tanító adatok a kívánt értékre nézve is tartalmaznak értékeket.
Például: * MGED (Microarray Experiments DataBase) adatbázis például a génexpresszálódási vizsgálatok adatainak leírására szolgál. * TAMBIS (Transparent Access to Multiple Bioinformatics Information Sources) szótár a bioinformatika és molekuláris biológia területén használt fogalmakat fedi le.
A behelyettesítetlen változók (WA, Q, V, és T) DEG-heurisztika szerint számított fokszáma most már mindenhol zérus, így a következő lépésben bármelyiket választhatjuk. Tegyük fel, hogy az algoritmus a WA változót választja, és ennek adja első körben a RED értéket (WA=RED).
Ez eddig egész könnyen ment (már meg is van a 25-ből 2 változónak az értéke (a probléma 8%-ban meg van oldva, mondhatnánk), de a többi korláttal és változóval sajnos még mindig nem boldogulunk. A nehézséget általában, vegyük észre, az okozza, hogy az előbbi primér reprezentációban értékként feltüntetett elemek (pl. az angol személy) kapcsán jelentjük, hogy melyik sorszámú vagy színű házhoz tartoznak, vagy éppenséggel melyik állatot tartják, mit isznak, illetve milyen dohányt szívnak. Tehát olyan, mintha ezek a kijelentések a jelenlegi formális reprezentációnk esetében értékekhez társítanának változókat.
A korszerű irányításelmélet, és különösen ennek a sztochasztikus optimális szabályozás nevű ága olyan rendszerek kifejlesztését tűzte ki célul, amelyek egy célfüggvényt (objective function) maximalizálnak az időben. Ez nagyjából megegyezik az MI-ről alkotott képünkkel: optimálisan viselkedő rendszereket fejleszteni. Akkor miért lett az MI és az irányításelmélet két különböző kutatási terület, hiszen megalkotói még szoros kapcsolatban is álltak egymással? A válasz forrása az a szoros kapcsolat, amely a résztvevők által ismert matematikai technikák és az ezeknek megfelelő, mindegyik világképben megjelenő problémák halmaza között alakult ki. Az irányításelmélet eszközei, a mátrixalgebra és az analízis folytonos változók rögzített halmazai által leírható rendszerekhez vezettek. Az egzakt analízis ráadásul tipikusan csak lineáris rendszerek esetében lehetséges. Az MI-t részben azért alkották meg, hogy az irányításelmélet 1950-es évekbeli matematikájától meg tudjanak szabadulni. A logikai következtetés és a számítás eszközei lehetővé tették az MI-kutatók számára, hogy olyan problémákat is figyelembe vegyenek, mint a nyelv, a látás és a tervkészítés, amelyek az irányításelméleti célkitűzéseken teljesen kívül estek.
Ez az alfejezet öt keresési stratégiát tárgyal, amelyek a nem informált (vaknak is nevezett) keresés (noninformed (blind) search) cím alá sorolhatók. A kifejezés azt jelenti, hogy ezen stratégiáknak semmilyen információjuk nincs az állapotokról a probléma definíciójában megadott információn kívül. Működésük során mást nem tehetnek, mint a következő állapotok generálása és a célállapot megkülönböztetése a nem célállapottól. Azokat a stratégiákat, amelyek tudják, hogy az egyik közbülső állapot „ígéretesebb”, mint egy másik közbülső állapot, informált keresési (informed search) vagy heurisztikus keresési (heuristic search) stratégiának nevezzük. Ezeket majd a  fejezet tárgyalja. A keresési stratégiákat a csomópontok kifejtési sorrendje különbözteti meg egymástól.
A kiterjesztés ezen formalizmusát definit klóz nyelvtannak (definite clause grammar) vagy DCG-nek hívjuk, mivel minden nyelvtani szabály értelmezhető a Horn-logikában szereplő definit klózként.^[229] Elsőként azt mutatjuk meg, hogy egy normális, kiterjesztés nélküli szabály hogyan értelmezhető definit klózként. Minden kategóriajelet karaktersorozatra vonatkozó predikátumnak tekintünk, így az NP(s) igaz, ha az s karaktersorozat egy NP-t formál. Az
A problémáról másféleképp is gondolkodhatunk: az alfa-béta algoritmus előnye, hogy figyelmen kívül hagyja azokat a későbbi fejleményeket, amelyek a legjobb játék során egyszerűen nem következnek be. Így a valószínű előfordulásokra koncentrál. A kockát használó játékok esetén nem léteznek valószínű lépéssorozatok, mivel adott lépések megtételéhez először a kockának kell a megfelelő oldalára esnie, hogy ezek a lépések egyáltalán megengedhetők legyenek. Ez egy általános probléma minden olyan esetben, amikor a bizonytalanság is képbe kerül: a lehetőségek óriási mértékben megsokszorozódnak, és értelmetlenné válik részletes cselekvési terveket kidolgozni, mert a világ valószínűleg nem ezek szerint fog viselkedni.
Az igazság-karbantartó rendszerek mechanizmust biztosítanak magyarázatok (explanations) generálásához is. Technikailag, P mondat magyarázatát olyan E mondatok halmazával definiáljuk, melyekre E maga után vonja P-t. Ha E mondatai igazak, akkor E egyszerűen P igazolásához egy szükségszerű alap. Azonban a magyarázat részei lehetnek feltételezések (assumptions) is – olyan állítások, amelyek nem igazak, de ha igazak lennének, elegendők lennének P igazolásához. Lehetséges például, hogy nincs elég tény bebizonyítani, hogy az autó nem fog indulni, de egy jó magyarázat része lehet az elromlott akkumulátor. Ez, összekapcsolva a gépkocsik működésére vonatkozó tudással, elegendő, hogy a megfigyelt jelenséget megmagyarázza. Az esetek többségében egy minimális E magyarázatot fogunk preferálni, azaz olyan magyarázatot, amelynek nincs olyan megfelelő részhalmaza, amelyik szintén egy magyarázat lenne. Egy ATMS a „gépkocsi nem indul” problémához magyarázatokat generál azáltal, hogy tetszőleges sorrendben feltételezéseket tesz (mint például „benzin a tankban” vagy „hibás az akkumulátor”), akkor is, ha azok egy része ellentmondásos. Elég majd a „gépkocsi nem indul” állítás címkéjét megnézni, hogy az állítást igazoló feltételezések halmazát megismerjük.
A fenti megfogalmazás egyértelművé teszi, hogy az intelligenciát a racionalitással érdemes definiálni, az intelligencia alapvető vonása a racionalitás. A racionalitás akkor vizsgálható, ha az ágensen és a sikeresség fogalmán kívül a környezet és a hozzáférhető információk halmaza is adott. A továbbiak során látni fogjuk, hogy a környezetre azt a megkötést kell tegyük, hogy az ágens cselekvéseivel sikert tudjon elérni benne. (Pl. egy képfelismerő szoftver, mint ágens, nem nagyon tudna „intelligenciát” tanúsítani egy pénzügyi trendeket analizáló környezetben.) A rendelkezésre álló információ pedig rendszerint belső tudásból és külső (szenzorikus) inputból áll.
Mint látjuk, a környezet definíciója szinte egy-az-egyben megegyezik a Bayes-i játékok definíciójával. Két különbség van: a stratégiakombinációk S (vagy Q) halmaza helyett itt most alternatívák A halmazáról beszélünk, továbbá a játékosok ui haszonfüggvényét most vi-vel jelöljük, és értékelési függvénynek nevezzük. Ettől eltekintve minden változatlan. E két változtatás pedig végső soron akár átnevezésnek is tekinthető - a jelölések mögötti jelentés effektíve nem változik.
A logaritmusképzés előnye nyilvánvaló: a log likelihood függvény három tag összege, ahol mindegyik tag csupán egyetlen paramétert tartalmaz. Amikor sorban mindegyik paraméter szerint vesszük az összefüggés deriváltját, majd nullává tesszük a deriváltakat, három független egyenlethez jutunk, és mindegyik csupán egyetlen paramétert tartalmaz:
Most tekintsünk egy p[i] mintapontra és az onnan induló összes többi mintapontra mutató vektorra egy alakzaton. Ezek a vektorok kifejezik a teljes alakzat konfigurációját a referenciaponthoz viszonyítva. Ez a következő ötlethez vezet: rendeljünk minden mintaponthoz egy leírót, az alakzatkontextust (shape context), amely megadja az alakzat további részének durva elrendezését az adott ponthoz viszonyítva. Pontosabban fogalmazva a p[i] alakzatkontextusa a maradék N – 1 p[k] pontra a p[k] –[ ]p[i] relatív koordináták egy közelítő h[i] térbeli hisztogramja. Egy logaritmikus poláris koordináta-rendszert használunk a tartók meghatározására annak biztosítása érdekében, hogy a leíró érzékenyebb legyen a közeli képpontok esetében. A  (c) ábra mutat erre egy példát.
Tegyük fel, hogy egy új algoritmussal tanulási kísérletet folytat. Egy adathalmazt használ, amiben 25-25 példa van az előforduló két osztályra. Azt tervezi, hogy a hagyj-ki-egyet keresztvalidációs módszert fogja használni. Összehasonlítási alapként egy egyszerű többségi szavazót használ. (A többségi szavazónak megadva egy tanító halmazt, a bemenetektől függetlenül mindig azt a kimenetet adja, amely kimeneti érték a halmazban legtöbbször előfordul.) Azt várta, hogy a többségi szavazó, a hagyj-ki-egyet keresztvalidációban nagyjából 50%-ot fog elérni, de meglepetésére 0% az eredmény. Meg tudja magyarázni, hogy miért?
A Sipe (System for Interactive Planning and Execution Monitoring) (Wilkins, 1988, 1990) volt az első tervkészítő, ami az újratervezés problémáját szisztematikusan kezelte. Különböző problémakörök demonstrációs feladataiban is felhasználták, például az űrsiklóhordozó fedélzeti terv készítésére vagy emberi erőforrás ütemezésére egy ausztrál sörgyárban. Egy másik vizsgálat a Sipe rendszert többemeletes épületek tervezésében alkalmazta, ami az egyik legbonyolultabb problémakör, amit tervkészítő valaha kezelt.
Kép „alkotásra” azért van szükség, mert még maguk a kép létrehozására szolgáló fizikai jelenségek – a röntgensugarak elnyelődése, az ultrahangok visszaverődése, az izotópok gammasugárzása, a protonok rezonanciája – is láthatatlanok. A folyamat során a röntgensugár átjárja a testet, az izotópok sugárzása a test belsejéből lép ki, az ultrahang visszaverődik.
Ha nem vigyázunk, ez a két szabály visszacsatolásos módon fog működni úgy, hogy az Eső-re vonatkozó bizonyíték megnöveli a VizesPázsit-hoz tartozó bizonyosságot, ami viszont még inkább megnöveli az Eső-höz tartozó bizonyosságot. Nyilvánvaló, hogy bizonytalansági következtető rendszereknek nyilván kell tartaniuk, hogy milyen utakon terjedt a bizonyíték.
Tehát az induktív tanulást logikai leírással olyan folyamatként jellemezhetjük, mint amelyik fokozatosan kizárja a példáknak ellentmondó hipotéziseket, leszűkítve a lehetőségek körét. Mivel a hipotézistér rendszerint nagy (vagy elsőrendű logika esetén egyenesen végtelen), nem javasoljuk egy rezolúcióalapú tételbizonyító rendszer építését és a hipotézistér teljes számbavételét. Ehelyett két megközelítést tárgyalunk, amelyek sokkal kisebb erőfeszítéssel logikailag ellentmondásmentes hipotéziseket képesek találni.
Ezek alapján a CTL formális szintaxisa a következő szerint adható meg: * Minden p atomi kijelentés egy állapotkifejezés. * Ha p és q egy-egy kifejezés, akkor p és q és ¬p is kifejezés. Ebből következően p vagy q is kifejezés. * Ha p és q állapotkifejezések, akkor X p és p U q útvonalkifejezések. Ebből következően G p és F p is útvonakifejezések. * Ha s útvonalkifejezés, akkor E s és A s állapotkifejezések.
Most mutex kapcsolatokat definiálunk, mind a cselekvések, mind a literálok számára. Egy adott szinten, a mutex kapcsolat akkor érvényes két cselekvés között, ha a következő három feltétel valamelyike fennáll: * Inkonzisztens hatások: egy cselekvés negálja egy másik következményét. Például az Eszik(Süti) és a Van(Süti) megőrzése inkonzisztens hatású, mert a Van(Süti) következmény tekintetében ellentétesek. * Interferencia: egy cselekvés egyik következménye a negálása egy másik cselekvés előfeltételének. Például az Eszik(Süti) interferál a Van(Süti) megőrzésével, mert negálja az előfeltételét. * Versenyhelyzet: egy cselekvés előfeltétele kölcsönösen kizáró egy másik előfeltételével. Például a Süt(Süti) és az Eszik(Süti) mutexek, mert versenyeznek a Van(Süti) előfeltétel tekintetében.
Az együttes tanulás a tanuló algoritmusok teljesítménynövelésének egyre népszerűbb módszere. A zsákolás (bagging)^[187] (Breiman, 1966) az első hatékony módszer, amely többféle indító (bootstrap) adathalmaz alapján megtanult hipotézisek kombinációját hozza létre. Ezeket az adathalmazokat az eredeti alulmintavételezésével állítják elő. A fejezetben ismertetett turbózás (boosting) módszere eredetileg Schapire elméleti munkájából (Schapire, 1990) származik. Az AdaBoost algoritmust Freund és Schapire dolgozta ki (Freund és Schapire, 1996), és Schapire adta meg elméleti analízisét (Schapire, 1999). Friedman és társai statisztikai alapon tárgyalják a turbózás módszerét (Friedman és társai, 2000).
A nem-teljes információjú játékokat tehát nem-tökéletes információjú játékokká alakítottuk, ámde ehhez többlet információ tárolása szükséges (típusok, típusok feletti eloszlás, stb). A Harsányi transzformáció eredményeképpen kapott, úgynevezett Bayes-i játékok tehát az eddig tárgyalt klasszikus játékoktól annyiban térnek el, hogy immár nem csak 3 elemből állnak (játékosok, stratégiák, hasznok), hanem a fenti fóliának megfelelően immár 2-vel többől, azaz 5-ből: (1) a játékosok halmaza, mint eddig; (2) a játékosok stratégiáinak halmaza, mint eddig; (3) a játékosok típusainak halmaza; (4) a játékosok haszonfüggvényeinek halmaza; és (5) a játékosok hiedelmeinek halmaza. A játékosok hiedelmei feltételes valószínűség-eloszlások, melyek a játékos saját típusának függvényében megmondják, hogy mi az eloszlása a többi játékos típus-kombinációinak.
Sejtautomatákat alkalmaznak földrajzi információs rendszereknél, az ún. pont minta analízishez, legközelebbi szomszéd analízishez. Ezzel a módszerrel elemezték Milwaukee rablóinak helyét összevetve az áldozataik címével és a Montana-i település eloszlások Voronoi diagramjait is [PVD].
A legegyszerűbb módja egy adott G DAG struktúrával (irányított körmentes gráf) kompatibilis eloszlás megadásának az, hogy a G gráfból kiolvassuk a feltételes függetlenségi kapcsolatokat a d-szeparáció segítségével. A d-szeparáció a DAG struktúrát, mint információszállító hálózatot kezeli, ahol az információfolyam a csomópontok közötti irányított utakon terjed. Alapeleme a blokkolás, vagyis az információáramlás gátlása két csomópont közötti úton. A d-szeparáció definíciója Pearl (2000) alapján a következő:
Az alapvető probléma a következő: ha egyszer a perspektivikus vetítés alatt a háromdimenziós világ egy pontból kiinduló sugár mentén található minden pontja a képnek ugyanabba a pontjába vetítődött, hogyan nyerjük vissza a háromdimenziós információt? A vizuális érzékelésben ehhez számos segítő dolog áll rendelkezésre, a mozgást, a sztereolátást, a textúrát, az árnyalást és a kontúrokat beleértve. Ezek mindegyike, ahhoz hogy a kép értelmezését (közel) egyértelmű módon megoldhassa, a jelenetre vonatkozó háttér-feltételezéseken alapul. Az alábbiakban ezekkel a módszerekkel foglalkozunk.
Bár a konzisztencia az elfogadhatóságnál szigorúbb követelmény, igazán nehéz olyan elfogadható heurisztikát találni, ami nem lenne egyben konzisztens. Az ebben a fejezetben tárgyalt összes elfogadható heurisztika mind konzisztens. Vegyük például a h[LMT]-t. Tudjuk, hogy az általános háromszög egyenlőtlenség teljesül, ha az oldalakat egyenes vonalú távolságokkal mérjük, és hogy az n és n′ közötti egyenesvonalú távolság c(n, a, n′)-nél nem nagyobb. A h[LMT] így egy konzisztens heurisztika. Fontos A konzisztencia egy másik fontos következménye az, hogy ha h(n) konzisztens, akkor az f(n) értékek akármilyen út mentén nem csökkennek. A bizonyítás bizonyos a mellett közvetlenül következik a konzisztencia definíciójából. Tegyük fel, hogy n′ az n utódja, ekkor:
- Vita? Mit értünk itt az alatt, hogy helyesen működik? Ha az a kérdés, hogy optimális lesz-e, akkor azt hiszem, hogy nem, de a mélység és a kockadobások számának növelésével az algoritmus eredménye konvergálni fog az optimálishoz. Általánosságban a kockadobás tud nagyon rossz eredményhez is vezetni, pontosan a véletlensége miatt, de kellően sok dobással ennek az esélye minimálisra csökkenthető.
1972-ben Colmerauer, Robert Pasero, Henry Kanoui, Philippe Roussel megalkották az első Prolog interpretert, valamint az első Prolog programot is, mely 610 klózból (állításból és szabályból) állt. Ez a program szövegértelmező volt, mely francia nyelven beírt állításokat dolgozott fel, és az alapján kérdésekre adott választ. Megadott szöveget dolgozott fel, és később válaszolni tudott a feltett kérdésekre. [5] Fontos
Az AC3/MAC következtetés tehát, miután az első 9 keresési lépés „mizériáját” követően 1 lépésben gyakorlatilag egy csapásra megoldotta a KKP-t, az eszközölt változtatásokkal visszatér a visszalépéses kereséshez, majd miután a változtatások foganatosítva lettek, a keresés rekurzíve meghívja önmagát.
A  ábra programja egy egyedi porszívókörnyezetre specifikus. Egy általánosabb és rugalmasabb megközelítés az, hogy először egy általános célú értelmezőt építünk feltétel–cselekvés szabályokra, majd szabályhalmazokat hozunk létre specifikus feladatkörnyezetek számára. A  ábra ezen általános program struktúráját adja meg sematikus formában, megmutatva, hogy a feltétel–cselekvés szabályok hogyan teszik lehetővé az ágens számára az észlelések és cselekvések összekötését. (Ne aggódjon, ha ez túl egyszerűnek tűnik: hamarosan sokkal érdekesebbé válik.) Az ágens döntési folyamatának aktuális belső állapotát négyszögekkel jelöljük, a folyamatban felhasznált háttértudás reprezentálására pedig oválisokat használunk. Az ágensprogram, amely szintén igen egyszerű, a  ábrán látható. A Bemenet-Feldolgozás függvény a bemenetből állítja elő az aktuális állapot absztrakt leírását, a Szabály-Illesztés függvény pedig az első olyan szabályt adja vissza a szabályok halmazából, amely illeszkedik az adott állapot leírására. Vegyük észre, hogy a „szabályok” és az „illeszkedés” fogalmakkal történő leírás tisztán koncepcionális – egyedi megvalósítások akár egy logikai áramkört megvalósító logikai kapu halmazból is állhatnak.  ábra - Egy egyszerű reflexszerű ágens sematikus diagramja Egy egyszerű reflexszerű ágens sematikus diagramja Fontos Az egyszerű reflexszerű ágenseknek megvan az az értékelendő tulajdonsága, hogy egyszerűek, ugyanakkor igen korlátozott intelligenciájúnak bizonyulnak. A  ábrán látható ágens csak akkor fog működni, ha a helyes döntés kizárólag az aktuális észlelés alapján meghozható – azaz akkor, ha a környezet teljesen megfigyelhető. Már a megfigyelhetőség legkisebb hiánya is komoly problémát okozhat. Például a korábban bemutatott fékezési szabály feltételezi, hogy az előző-autó-fékez feltétel megállapítható az aktuális észlelésből – az aktuális videoképből – amennyiben az előző autónak középre szerelt féklámpája van. Sajnálatos módon, a régebbi modelleken különbözőképpen helyezkednek el a hátsó helyzetjelzők, a féklámpák és az indexlámpák, így egyetlen kép alapján nem mindig lehetséges azt megállapítani, hogy az autó fékez-e. Egy egyszerű reflexszerű ágens, amely egy ilyen autó mögött halad, vagy folyamatosan szükségtelenül fékezne, vagy ami még rosszabb, sosem fékezne.  ábra - Egy egyszerű reflexszerű ágens. A jelenlegi szituációra (amit az észlelés határoz meg) illeszkedő feltételű szabály megkeresésével, majd a hozzá tartozó cselekvés végrehajtásával működik. Egy egyszerű reflexszerű ágens. A jelenlegi szituációra (amit az észlelés határoz meg) illeszkedő feltételű szabály megkeresésével, majd a hozzá tartozó cselekvés végrehajtásával működik.
Definíciónk nemcsak azt követeli meg a racionális ágenstől, hogy információt gyűjtsön, hanem azt is, hogy amennyit csak lehet tanuljon (learn) a megfigyeléseiből. Az ágens kezdeti konfigurációja valamilyen előzetes tudást tükrözhet a környezetről, de ahogy az ágens tapasztalatot szerez, ez a tudás módosulhat és átértékelődhet. Vannak szélsőséges esetek, amikor a környezet a priori teljesen ismert. Ilyen esetekben az ágensnek nem kell érzékelnie vagy tanulnia, egyszerűen csak helyesen cselekszik. Természetesen az ilyen ágensek nagyon sérülékenyek. Vegyük például az egyszerű ganajtúró bogarat. Miután kiásta a fészkét és elhelyezte a tojásait, a közeli halomból egy ganajlabdát készít, hogy elzárja a fészek bejáratát. Ha útközben a labdát eltávolítjuk a fogói közül, a bogár folytatja útját, és a nem létező labdával is eljátssza a fészek lezárását, miközben észre sem veszi, hogy a labda hiányzik. Az evolúció beépített egy feltételezést a bogár viselkedésébe, és amikor ez megsérül, sikertelen viselkedés lesz az eredménye. Kicsit intelligensebb a szöcskeölő darázs. A nőstény ás egy lyukat, keres és megcsíp egy hernyót, elcipeli a lyukhoz, bemegy a lyukba megnézni, hogy minden rendben van-e, majd bevonszolja a hernyót és lerakja a tojásait. A hernyó szolgál a tojások kikelése után táplálékul. Eddig jó, de ha egy rovarkutató pár centiméterrel odébb rakja a hernyót, miközben a darázs a lyukat ellenőrzi, az visszalép a hernyó odacipelése fázishoz, és a tervét változtatás nélkül folytatja onnan, akár tucatnyi hernyóelmozdító közbelépés után is. A darázs képtelen megtanulni, hogy öröklött terve hibás, így nem fogja megváltoztatni.
 ábra - A repülőtér-elhelyezési probléma egy egyszerűsített reprezentációja. A kimeneti állapotokhoz tartozó véletlen csomópontok hatását kiszámítottuk, majd a csomópontokat elhagytuk. A repülőtér-elhelyezési probléma egy egyszerűsített reprezentációja. A kimeneti állapotokhoz tartozó véletlen csomópontok hatását kiszámítottuk, majd a csomópontokat elhagytuk.
Az előbbi fólián látszik, hogy az számos, az 1 dolláros játékkal analóg élethelyzet van a valós életben is. Mindezeknek, az „1 dolláros játékot” is beleértve, kellene, hogy legyen tehát egy közös absztrakciója, egy közös eredője, amely mindegyiknek egységesen megragadja a lényegét. Nézzünk most egy másik, hasonló játékot - ennek „1000000 dolláros játék” a neve -, hátha közelebb jutunk az említett „eredőhöz”.
A szűrés időpontját illetően a szív-és érrendszeri problémával küzdők közel 40%-ának 1-2 héten belül, 30%-uknak 1-2 hónapon belül mérték a vérnyomását, míg az anyagcsere- problémával küzdők mintegy 50%-a volt 1-2 hónapon belül laborvizsgálaton.
A P(szó) a priori valószínűség valódi szöveges adatból kapható meg. A P(e[1:t]|szó) pedig az akusztikus jegyek sorozatának a valószínűsége a szómodell szerint. Ilyen valószínűségek kiszámítását a  alfejezet tárgyalta; nevezetesen a   egyenlet egy egyszerű rekurzív számítást definiál, lineáris költséggel t-ben és a Markov-lánc állapotainak számában. A legvalószínűbb szó megtalálásánál ezt a számítást minden lehetséges szóra elvégezhetjük, megszorozzuk az a priori valószínűséggel, és e szerint választjuk ki a legjobb szót.
Például egy időben nem korlátos sakkpartiban a számítható racionális ágens egyben tökéletesen racionális is volna (hiszen itt nem számít, hogy mennyi ideig tart egy-egy lépés kiszámítása és megtétele), viszont egy időben korlátozott sakkpartiban, ahol az időkorlát kellően szűk (pl. 1-30 perc), a számítható racionális ágens már nem feltétlen volna tökéletesen racionális. Egyszerűbb esetben talán még igen (ha nagyon kevés bábú van már csak a sakktáblán, és így kevés variáció lehetséges), de bonyolultabb helyzetben, ahol valóban nagyon sok számítást kell végezni (pl. mert sok variációt kell mérlegelni), ott már nem lenne ideje arra, hogy kiszámítsa a tökéletesen racionális lépést. Ez az ágens ettől még természetesen számítható racionális maradna, viszont az általa - adott idő alatt - kiszámított lépés akár kifejezetten rossz is lehetne.
A levelek közötti kis rések mint pinhole kamerák működnek. Ez azt jelenti, hogy a körkörös foltok a kör alakú nap képei. (A teszteléshez javasolt egy fehér lap használata és azon vizsgáljuk a foltokat.)
A minimax algoritmust sokszor Ernst Zermelo, a modern halmazelmélet atyja 1912-es cikkéhez vezetik vissza. A cikk sajnálatos módon tartalmaz hibákat, és a minimaxot nem írja le helyesen. A játékelmélet komoly alapjait a nagy hatású Theory of Games and Economic Behavior c. munkában Neumann és Morgenstern (Neumann és Morgenstern, 1944) fektették le, kimutatva azt is, hogy egyes játékokban szükség van randomizált (avagy nem megjósolható) stratégiákra. (További információért lásd  fejezet.)
Az alfejezetet azzal kezdjük, hogy pontosabban meghatározzuk azt a módot, amellyel az elsőrendű logikában előforduló lehetséges világok leírják az objektumokra és relációkra tett ontológiai megállapításokat. Ezután bemutatjuk a nyelv különböző elemeit, és ahogyan haladunk, megmagyarázzuk ezek szemantikáját.
Most már minden rendelkezésünkre áll, hogy az értékiterációt a gyakorlatban is felhasználjuk. Tudjuk, hogy a helyes hasznosságokhoz konvergál, a hasznosság becslésének hibájára korlátokat tudunk adni, ha véges számú iteráció után leállunk, és korlátokat tudunk adni az eljárásmód veszteségére, ami a becsült hasznosságértékekhez tartozó MVH-eljárásmód végrehajtása miatt lép fel. A végső megjegyzés, hogy az összes eredmény feltétele ebben az alfejezetben a γ < 1-gyel való leszámítolás. Ha γ = 1 és a környezet tartalmaz végállapotokat, akkor bizonyos technikai feltételek teljesülése esetén konvergenciaeredmények és hibakorlátok hasonló halmaza származtatható.
Összefoglalva tehát a kielégíthetőségi tervkészítés, a kiinduló állapotot, a célt, a követő állapot axiómákat, az előfeltételeket, valamint a cselekvéskizárások vagy az állapotaxiómák egyikét tartalmazó mondathoz tartozó modellek megtalálását jelenti. Megmutatható, hogy az axiómák ezen halmaza elégséges, abban az értelemben, hogy a továbbiakban nincsenek hibás „megoldások”. Az ítéletlogikai mondatot kielégítő bármely modell megfelelő megoldása az eredeti problémának, azaz a terv minden sorba rendezése egy megengedett cselekvéssor, ami a célhoz vezet.
A harmadik kategóriába tartoznak a teleoperációval vezérelt robotok, amelyek alapvetően három részből épülnek fel: egy vagy több manipulátorból (slave eszköz), egy irányító konzolból (master eszköz) és egy látórendszerből. Az orvos egy, a látórendszer által közvetített kép alapján végzi mozdulatait, és figyeli azok hatását. A vezérlőjelek a mastertől egy szabályozón keresztül jutnak el a manipulátorhoz, amely pontosan utánozza azokat.
 ábra - A visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. A visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - A fokszám (DEG) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. A fokszám (DEG) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - Az előretekintéssel ellátott visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. Az előretekintéssel ellátott visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - Az előretekintéssel ellátott, és legkevesebb megmaradó érték (MRV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. Az előretekintéssel ellátott, és legkevesebb megmaradó érték (MRV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - Az előretekintéssel ellátott, és legkevésbé korlátozó érték (LCV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. Az előretekintéssel ellátott, és legkevésbé korlátozó érték (LCV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - AC3/MAC élkonzisztencia ellenőrzéssel ellátott visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. AC3/MAC élkonzisztencia ellenőrzéssel ellátott visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - AC3/MAC élkonzisztencia ellenőrzéssel ellátott, és és legkevesebb megmaradó érték (MRV), fokszám (DEG), illetve legkevésbé korlátozó érték (LCV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja. AC3/MAC élkonzisztencia ellenőrzéssel ellátott, és és legkevesebb megmaradó érték (MRV), fokszám (DEG), illetve legkevésbé korlátozó érték (LCV) heurisztikával kiegészített visszalépéses mélységi keresés  szakaszban bemutatott működésének animációja.  ábra - a lokális keresési (Min-Conflicts) algoritmus  szakaszban bemutatott működésének animációja a lokális keresési (Min-Conflicts) algoritmus  szakaszban bemutatott működésének animációja
A Strips és az ADL jelölésrendszere számos valós problémakörre megfelelő. A következő alfejezetek néhány egyszerű példát mutatnak be. Néhány számottevő megkötés azért még megmaradt. A legnyilvánvalóbb, hogy közvetlenül nem tartalmazzák a cselekvések véghatásait (ramifications). Például ha vannak emberek, csomagok vagy porcicák egy repülőn, akkor mindannyian helyet változtatnak egy repülés során. Ezeket a változásokat leírhatjuk, mint a repülés egyenes következményeit, így természetesebbnek tűnik a repülőgép tartalmának helyét, mint a gép helyének logikai következményét ábrázolni. Az ilyen állapotmegkötésekre (state constraints) a  alfejezet tartalmaz példákat. A hagyományos tervkészítő rendszerek meg sem próbálják megoldani a kvalifikációs problémát (qualification problem), azaz a nem reprezentált körülmények problémáját, melyek a cselekvés meghiúsulását okozhatják. A  fejezetben látni fogjuk, hogy a kvalifikációs probléma hogyan közelíthető meg.
A fentebbi fólián láthatunk pár példát a Kalapok Rejtélye kapcsán az egyes játékosok információs halmazaira/függvényeire. Ha pedig vesszük egy i játékos összes Pi(w) információs halmazát, amikről még azt is feltételezzük, hogy diszjunktak (azaz nincs közös elemük - ez ugyancsak igen fontos, ámde ésszerű feltételezés), akkor ezekből megkaphatjuk az i játékos Pi információs partícióját, amely tehát diszjunkt Pi(w) részhalmazokra osztja fel a teljes nagy Omega állapotteret.
A szűrő módszerek valamilyen ésszerűen választott kritérium (pl.: korreláció, kölcsönös információ) alapján válogatják ki a legjobb jegyeket, melyek az ettől független modelltanulás bemeneteként szolgálnak. Mivel a jegyek kiválasztása elkülönül az eredeti problémától (az osztályozó építéstől) ezért a szűrő módszerek révén előálló megoldások jellemzően kevésbé pontosak, mint a másik két módszercsoporté. Ugyanakkor számítási komplexitás terén a szűrők egyszerűbbek, emiatt a leggyorsabbak a három csoport közül. Az egyváltozós szűrő módszerek a jegyeket egyenként vizsgálják, amiatt a jegyek egymás közötti függéseit nem képesek figyelembe venni. Ezzel szemben a többváltozós szűrő módszerek (Fast correlation based feature selection – FCBF [15], Markov blanket filter – MBF [10]) erre alkalmasak, viszont emiatt összetettebbek, ezáltal erőforrás-igényesebbek is.
A  alfejezetben elmagyaráztuk, hogy a kockadobásból eredő bizonytalanság a mély keresést drága luxussá teszi. Az ostáblára vonatkozó kutatások zöme a kiértékelő függvény javítására összpontosult. Gerry Tesauro (Tesauro, 1992) neurális hálós technikákkal (lásd  fejezet) ötvözte Samuel megerősítéses tanulási módszerét, hogy egy figyelemre méltó kiértékelő függvényt hozzon létre, amit 2-es, illetve 3-as mélységű kereséssel együtt használt. A saját magával játszott több mint egy millió tanító játszma után Tesauro programja, a TD-Gammon a világranglistán stabilan az első három játékos között foglal helyet. A program eredményei alapján a játék nyitó lépéseit tekintve egyes esetekben az eddigi gyakorlat radikálisan megváltozott.
Most minden egyes mintát súlyozunk a t + 1 időpontbeli bizonyíték valószínűségével. Egy x[t+1 ]állapotban lévő minta P(e[t+1]|x[t+1]) súlyt kap. Az x[t+1] állapotban lévő minták teljes súlya az e[t+1] megfigyelése után ezért
Elsőként: mi az a teljesítménymérték, amit az automata taxisofőrünk elé állítunk? Elvárt jellemzők a helyes célállomás elérése; az üzemanyag-fogyasztás és az elhasználódás minimalizálása; az út költségének és/vagy idejének minimalizálása; a közlekedési szabályok megszegésének és más vezetők megzavarásának minimalizálása; a biztonság és utaskényelem maximalizálása; a haszon maximalizálása. Nyilvánvalóan ezen célok egy része konfliktusban van egymással, kompromisszumokra lesz szükség.
A kapott {NSW=RED, WA=GREEN, NT=RED, Q=GREEN} értékadás immáron ellentmondásmentes, így újabb rekurzív hivás következhet, aminek során az SA változó, és annak is a RED értéke kerül kiválasztásra (SA=RED).
A kényszerterjesztési módszereket Waltz (Waltz, 1975) sikere tette népszerűvé, amelyet a számítógépes látásnál felmerülő poliéder-élcímkézési problémán ért el. Waltz megmutatta, hogy sok probléma esetén a kényszerterjesztés teljesen kiküszöböli a visszalépést. Montanari (Montanari, 1974) bevezette a kényszerhálózat és az útvonalkonzisztencia-terjesztés fogalmát. Alan Mackworth (Mackworth, 1977) javasolta az AC-3 algoritmust az élkonzisztencia betartatására, csakúgy, mint annak általános lehetőségét, hogy valamilyen fokú konziszentencia-ellenőrzést építsünk be a visszalépéses algoritmusba. Az AC-4, egy jóval hatékonyabb algoritmus, melyet Mohr és Henderson fejlesztettek ki (Mohr és Henderson, 1984). Nem sokkal Mackworth cikkének megjelenése után a kutatók elkezdtek kísérletezni a konzisztencia-ellenőrzések költsége és a kereséslevágásban jelentkező előny csereviszonyának feltérképezésével. Haralick és Elliot (Haralick és Elliot, 1980) a McGregor által leírt (McGregor, 1979) minimális előrenéző ellenőrzés mellett álltak ki, míg Gaschnig (Gaschnig, 1979) minden egyes változó-hozzárendelés után élkonzisztencia-ellenőrzéseket javasolt (ezt az algoritmust nevezte később Sabin és Freuder (Sabin és Freuder, 1994) MAC-nak). Az utóbbi cikk valamennyire meggyőző bizonyítékot hoz fel amellett, hogy nehezebb problémáknál kifizetődik a teljes élkonzisztencia-ellenőrzés. Freuder (Freuder 1978, 1982) megvizsgálta a k-konzisztenciát és ennek kapcsolatát a kényszerkielégítési problémák megoldásának komplexitásával. Apt (Apt, 1999) egy általános algoritmikus keretrendszert mutat be a konzisztenciaterjesztési algoritmusok vizsgálatára.
A „Gyáva nyúl” játék leírását a fentebbi fólia foglalja össze. A játékhoz tartozó történetet egyébként igen érzékletesen jeleníti meg James Dean „Haragban a Világgal (Rebel Without a Cause)” c. filmjének vonatkozó jelenete - valószínűleg innen származik a játék elnevezése is.
Az eddig leírt algoritmusok megkövetelik a hasznosságok vagy eljárásmódok összes állapotbeli egyszerre történő frissítését. Megmutatható, hogy ez nem szigorúan szükséges. Valójában minden iterációnál kiválaszthatjuk az állapotok egy tetszőleges részhalmazát, és alkalmazhatjuk rá bármelyik fajta frissítést (eljárásmód-javítást vagy egyszerűsített értékiterációt). Ezt az általánosított algoritmust aszinkron eljárásmód-iterációnak (asynchronous policy iteration) nevezzük. A kezdeti eljárásmód és a hasznosságfüggvény adott feltételek melletti megválasztásánál az aszinkron eljárásmód-iteráció garantáltan konvergál az optimális eljárásmódhoz. A feldolgozandó állapotok megválasztásának szabadsága azt jelenti, hogy hatékonyabb heurisztikus algoritmusokat tervezhetünk – például olyan algoritmusokat, amelyek azon állapotok értékének frissítésére koncentrálnak, amelyeket egy jó eljárásmód valószínűleg elér. A valós életben ez nagyon hasznosnak tűnik: ha valakinek nincs szándékában a mélybe vetni magát egy szikláról, akkor nem kell aggódással töltenie az időt a bekövetkező állapotok pontos értékéről.
Az aktuális behelyettesítés ennek hatására {} üresről {NSW=RED}-re változik, ami egyetlen korláttal sem ütközik, így következhet a KÖVETKEZTETÉS eljárás meghívása. Ennek során lényegében az előzőekben tárgyalt AC3/MAC algoritmus kerül végrehajtásra.
+time(_) : myenergy(Energy) & Energy<25 & adamant.AlwaysTrue <- wait. 5. Kipróbáljuk a módosított ágenseket a szimulátorban: ehhez az AgentGame.mas2j fájlba beszúrjuk az adamant_ [verbose=0] #5; sort, majd futtatjuk a szimulátort.
A technikai eljárás révén az orvos közvetlenül a számítógép hangkártyájához csatlakozó mikrofonba beszélhet, melyet a szoftver rögzít és átalakítja elektronikus szöveggé. A technológia lényege, hogy a kiejtett szavakat azok írásos formájához kell párosítani.
Egy másik lehetséges eljárás a hiányzó értékekkel rendelkező változó transzformálása, mely során a hiányzó értékeket átkódoljuk. Kategorikus változók esetén egy új kategóriává, folytonos változóknál egy választott értékre, kiegészítve egy új, hiányzást jelző indikátor változó bevezetésével. Ez a megközelítés az előzőhöz hasonlóan bias-hoz vezethet.
Ezen felül minden szabályhoz tartozik egy o paraméter, amely azzal a kimeneti változó értékkel egyenlő, amire a tagsági érték egy. Tehát a P változó azon értékét kell megkeresni, amire a D1 fuzzy halmazhoz egy tagsági értékkel kapcsolódik.
Egy érdekes modell a Sequitur-rendszer (Nevill-Manning és Witten, 1997). Nincs szüksége más bemenetre, csak egy szövegre (amit nem szükséges előzetesen mondatokra bontani). Egy nagyon specializált formátumú nyelvtant állít elő: egy olyan nyelvtant, amely csak egy karakterfüzért generál, nevezetesen az eredeti szöveget. Másképp tekintve a Sequitur épp csak arra elegendő nyelvtant tanul, hogy a szöveget elemezze. Íme, itt van az a zárójelezés, amit egy nagyobb, híreket tartalmazó szövegben levő mondatra felfedezett:
A „lefoglal” metódus egyszerűen elhelyezi a királynőt az adott mezőn (például egy 8 x 8-as kétdimenziós tömb megfelelő elemét 1 értékre állítja), míg a „felszabadit” metódus a királynőt eltávolítja onnan (például nullázza a tömb adott elemét).
A robotika kifejezést Asimov használta először 1950-ben, ugyanakkor a robotika története – más megnevezéssel – sokkal régebbre nyúlik vissza. Az ókori görög mitológiában Talószt, a mechanikus embert állítólag Héphaisztosz, a kovácsisten készítette. Jacques Vaucanson egy csodálatos automatát épített a 18. században. Az 1738-as mechanikus kacsa a korai robotok egy példája, aminek összetett viselkedése még fixen volt beleépítve. Az első programozható, robotszerű alkotás talán Jacquard szövőszéke volt (1805), amiről már az  fejezetben már szót ejtettünk.
Tegyük fel, hogy egy olyan sakkprogrammal rendelkezik, amely képes egymillió csomópontot kiértékelni másodpercenként. Válassza a játékállás egy tömör reprezentációját a transzpozíciós tábla számára. Kb. mennyi állást képes eltárolni memóriában, az 500 Mbájt nagyságú táblán? Elég lesz-e ez a lépésenként allokált 3 perces kereséshez? Hány táblakiolvasást képes megvalósítani egy álláskiértékelés ideje alatt? Most tételezze fel, hogy a transzpozíciós tábla nagyobb, és nem fér el a memóriában. Hány kiértékelést lehetne megvalósítani egy diszkhozzáférés ideje alatt, standard diszkhardver mellett?
(A mondatok fordítását és értelmezését az olvasóra hagyjuk – a ford.) Fontos Azonban legtöbb esetben a beszéd, amit hallunk, egyértelműnek tűnik. Ezért amikor a kutatók először kezdtek a nyelv elemzésére számítógépeket használni a hatvanas években, igen meglepődve tapasztalták, hogy majdnem minden megnyilatkozás erősen többértelmű, bár az alternatív értelmezések egy anyanyelvi beszélő számára lehet, hogy nem nyilvánvalók. Egy nagy nyelvtannal és szókinccsel rendelkező rendszer értelmezések ezreit találhatja egy teljesen átlagos mondatra. Vegyük például a „The batter hit the ball” mondatot, amelynek látszólag egy egyértelmű értelmezése van, miszerint a baseballjátékos megüt egy baseball-labdát. De különböző értelmezést kapunk, ha az ezt megelőző mondat ez: „The mad scientist unleashed a tidal wave of cake mix towards the ballroom.” Ez a példa a lexikális többértelműségen (lexical ambiguity) alapszik, amikor egy szónak egynél több jelentése van. A lexikális többértelműség igen gyakori: a „back” lehet határozószó (go back), melléknév (back door), főnév (the back of the room) és ige (back up your files). A „Jack” lehet név, főnév (kártya, hatszögű fém játékdarab, hajózási zászló, hal, hím szamár, dugasz vagy nehéz objektumok emelésére szolgáló eszköz), illetve lehet ige (felemelni egy autót, fénnyel vadászni vagy erősen megütni a baseball-labdát).
A mérési eredményeket valamilyen matematikai művelettel átalakítjuk (transzferáljuk), majd az átalakított adatokat elemzzük a szokásos eljárással. Ha ezek Gaussi eloszlást mutatnak, akkor most határozzuk meg a tartomány alsó, átlag és felső határát, majd ezeket visszaalakítjuk a kiindulási formába.
Feltételezzük, hogy az ágens mindig képes felismerni azt az állapotot, amiben már járt, és hogy a cselekvései determinisztikusak (e két feltételezéstől a  fejezetben eltekintünk). Végül az ágens hozzáférhet egy h(s) elfogadható heuriszitikus függvényhez, amely a pillanatnyi állapot és a célállapot távolságát becsüli. A  ábrán például az ágens tudhatja a célpozícióját, és képes lehet a Manhattan-távolsági heurisztika használatára.
Ebben az esetben is szeretnénk azt a következtetést levonni, hogy Gonosz(János), mivel tudjuk, hogy János egy király (ez adott), és hogy János mohó (mert mindenki mohó). Egy megfelelő helyettesítés megtalálására van szükségünk ennek elvégzéséhez, mind az implikációs mondatban, mind az ehhez illesztendő mondatokban található változókra. Ebben az esetben az {x/János, y/János} helyettesítés alkalmazása az implikáció Király(x) és Mohó(x) premisszáihoz és a Király(János) és Mohó(y) tudásbázis mondatokhoz létrehozza az azonosságot. Így tehát ki tudjuk következtetni az implikáció konklúzióját.
Az eredeti képen detektáljuk az élpontokat, majd minden élpont esetében megvizsgáljuk, hogy az élpont környezetében lévő pontok intenzitása a “deformált” képen (I[d](x,y)=I(x,y,t[stop])) milyen szórást mutat (lásd  ábra).
A kilenc darab bemenet tehát három csoportra lett felosztva. Az első csoportba tartoznak a személyes jellemzők: életkor, házas-e, gyerekek száma.  ábra -  ábra. Személyes jellemzők: 3 bemenet, 1 kimenet, 18 szabály  ábra. Személyes jellemzők: 3 bemenet, 1 kimenet, 18 szabály
Hiba akkor lép fel, ha az ágens ismételt kísérletei is sikertelenek a cél elérésében, azaz valamilyen, számára ismeretlen előfeltétel vagy következmény blokkolja. Például ha az ágens rossz kulcskártyát kap a hotelszobájához, akkor nincs az a beillesztési és eltávolítási próbálkozás, ami kinyitná az ajtót.^[131] Egy megoldás, hogy ugyanazon terv ismételgetése helyett véletlenszerűen választunk egy lehetséges javítóterv halmazból. Ebben az esetben egy új kulcskártya kérése a recepcióról hasznos alternatív javító terv. Mivel nem biztos, hogy az ágens képes a valóban nemdeterminisztikus esetet és a hiábavalóság esetét megkülönböztetni, egy kis változatosság a javításban általánosságban is jó ötlet.
A VLSI elhelyezési (VLSI layout) problémában komponensek és összeköttetések millióit egy chipen kell elhelyezni, a terület, az áramköri késleltetés és a szórt kapacitások minimalizálása és a gyártási kihozatal maximalizálása mellett. Az elhelyezési probléma a logikai tervezés fázisát követi, és általában két részből áll: cellaelrendezésből (cell layout) és huzalozásból (channel routing). A cellaelrendezés során az áramkör primitív komponenseit cellákba rendezik, amelyek mindegyike valamilyen jól definiált funkciót lát el. Minden egyes cellának rögzített nyomtatási képe (méret és alak) van, és adott számú összeköttetést igényel a többi cellához. A cél az, hogy az egyes cellákat úgy helyezzük el a chipen, hogy azok ne lapolódjanak át, és a cellák között legyen megfelelő hely az összekötő vezetékek számára. A huzalozás során minden egyes vezetéknek megfelelő vezetési útvonalat keresünk a cellák között. Ezek a keresési problémák hihetetlenül összetettek, de mindenképpen érdemes a megoldásukat megtalálni. A  fejezetben látni fogunk néhány algoritmust, amelyek képesek megoldani ezeket a problémákat.
Számos lehetőség létezik a P(R = igaz|D, Q) együttes eloszlás dekomponálására. Itt az ún. nyelvi modellezés (language modeling) megközelítést fogjuk bemutatni, amely minden egyes dokumentumra egy nyelvi modellt becsül, majd az adott dokumentum nyelvi modellje alapján minden egyes lekérdezésre kiszámítja a lekérdezés valószínűségét. Az R = igaz érték jelölésére r-t használva, a következő alakra írhatjuk át a valószínűséget:
Érdemes részletesebben megvizsgálni az AMérő[t]-hez tartozó érzékelőmodell tulajdonságait. Tegyük fel az egyszerűség kedvéért, hogy mind az Akkumulátor[t], mind az AMérő[t] diszkrét értékeket vehetnek fel 0-tól 5-ig – igen hasonlóan egy tipikus laptop számítógép akkumulátormérőjéhez. Ha a mérő mindig pontos akkor a P(AMérő[t]|Akkumulátor[t]) FVT-nek végig 1,0 valószínűségeket kell tartalmaznia az „átlójában” és 0,0 valószínűségeket máshol. A valóságban a mérésekben a zaj mindig felbukkan. Folytonos mérésekre, egy kis szórású Gauss-eloszlást lehetne ehelyett használni.^[160] A diszkrét változóinknál a Gauss-eloszlást egy olyan eloszlással közelíthetjük, amelyben a hiba valószínűsége a megfelelő módon csökken, így nagy hiba valószínűsége igen kicsi. A Gauss-hibamodell (Gaussian error model) kifejezést fogjuk használni mind a folytonos, mind a diszkrét változatokra.  ábra - (a) Az esernyős DBH-hoz tartozó a priori eloszlás, az állapotátmenet-modell és az érzékelő modell megadása. Az összes következő szelet feltevésünk szerint az 1. szelet másolata. (b) Egy X–Y síkon való robotmozgáshoz tartozó egyszerű DBH. (a) Az esernyős DBH-hoz tartozó a priori eloszlás, az állapotátmenet-modell és az érzékelő modell megadása. Az összes következő szelet feltevésünk szerint az 1. szelet másolata. (b) Egy X–Y síkon való robotmozgáshoz tartozó egyszerű DBH.
Ezen interpreter Magyarország akkori egyik legnagyobb teljesítményű számítógépén 20 műveletet tudod elvégezni másodpercenként. Ennek a sikernek a hatására több gyakorlati alkalmazásban használatos Prolog program született az országban [4]. Ez azért volt jelentős, mert addig csak egyetemi környezetben volt használatos a Prolog, gyakorlati környezetben nem. 1976-ban Robert Kowalsky Budapestre látogatott.
A többágenses környezeteknek nem mindegyike áll együttműködő ágensekből. Az egymásnak ellentmondó működésű ágensek versenyben (competition) vannak egymással. Erre egy példát jelentenek a két játékost igénylő, de zérus összegű játékok, mint amilyen a sakk. A  fejezetben láthattuk, hogy a sakkozó ágensnek figyelembe kell vennie az ellenfél következő néhány lehetséges lépését is. Azaz egy versengő környezetben levő ágensnek (a) fel kell ismernie, hogy vannak más ágensek, (b) meg kell határoznia a többi ágens lehetséges terveit, (c) meg kell határoznia, hogy a többi ágens hogyan tervezi a saját terveivel való kölcsönhatást, és (d) el kell döntenie, hogy ezek fényében mi a legjobb cselekvés. A versengés, hasonlóan az együttműködéshez, a többi ágens tervének modelljét igényli. Másrészről, egy versengő környezetben nincs elhatározás egy összetett terv mellett.
A diszjunkt mintaadatbázisok jól működnek a csúszólapka-játékok esetén, mert a problémát úgy lehet felbontani, hogy egy-egy lépés csak egy részproblémára van hatással, hiszen egyszerre csak egy lapkát mozgatunk. A Rubik-kocka problémára ilyen bontást elvégezni nem lehet, mert minden mozgás 8, 9 vagy 26 kockát érint. Nem tudjuk egyelőre, hogy az ilyen problémák számára a diszjunkt mintaadatbázisokat hogyan lehetne definiálni.
Bár ez ésszerű követelménynek tűnik, könnyű belátni, hogy egyes esetekben a legjobb elérhető kompetitív arány a végtelen. Ha például bizonyos cselekvések irreverzíbilisek, az online keresés esetleg egy olyan zsákutcába kerülhet, ahonnan a célállapot nem elérhető.  ábra - Egy egyszerű labirintusprobléma. Az ágens S-nél kezd és C-t kell elérnie, a környezetéről azonban semmit sem tud. Egy egyszerű labirintusprobléma. Az ágens S-nél kezd és C-t kell elérnie, a környezetéről azonban semmit sem tud.  ábra - (a) Két olyan állapottér, amely az online kereső ágenst egy zsákutcába viheti. (b) Egy kétdimenziós környezet, amely arra késztetheti az online kereső ágenst, hogy a célhoz legkevésbé hatékony utat kövesse. Akármit is választ az ágens, az ellenség eltorlaszolja az útját egy másik hosszú, vékony fallal úgy, hogy a követett út sokkal hosszabb lesz, mint a legjobb lehetséges út. (a) Két olyan állapottér, amely az online kereső ágenst egy zsákutcába viheti. (b) Egy kétdimenziós környezet, amely arra késztetheti az online kereső ágenst, hogy a célhoz legkevésbé hatékony utat kövesse. Akármit is választ az ágens, az ellenség eltorlaszolja az útját egy másik hosszú, vékony fallal úgy, hogy a követett út sokkal hosszabb lesz, mint a legjobb lehetséges út. Fontos Lehet, hogy az „esetleg” megfogalmazást nem találja kellően meggyőzőnek – végül is létezhetne olyan algoritmus, amely feltárás közben a zsákutcákban nem köt ki. A kijelentésünk pontosabban megfogalmazva az, hogy nincs olyan algoritmus, amely bármilyen állapottérben el tudná kerülni a zsákutcákat. Nézzük meg közelebbről a  (a) ábra két zsákutcás állapotterét. Egy olyan online kereső algoritmus számára, amely az S és az A állapotokat már meglátogatta, a két állapottér azonosnak tűnik, tehát mindkettőben ugyanolyan döntéshez kell folyamodnia. Ez az ellenség érv (adversary argument) egy példája – el tudunk képzelni egy ellenséget, amely módosítja az állapotteret, miközben az ágens feltárja azt, és a célokat és a zsákutcákat tetszés szerint átrendezi.
A második fejezetben láttuk, hogy a tanuló ágens felfogható úgy, mint aminek van egy cselekvő komponense (performance element), amellyel eldönti, hogy milyen cselekvést válasszon, és egy tanuló komponense (learning element), amellyel módosítja ezt a cselekvő elemet annak érdekében, hogy a későbbiekben jobb döntéseket hozzon (lásd  ábra). A gépi tanulással foglalkozó kutatók a tanuló komponensek széles választékával álltak elő. Ezek megértését segíti, ha megnézzük, hogy a leendő működési területük hogyan befolyásolja tervezésüket. A tanuló komponens tervezését három dolog befolyásolja alapvetően. Ezek: * A cselekvő elem mely komponenseit akarjuk tanítani. * Milyen visszacsatolás áll rendelkezésre ezen komponensek tanítására. * Hogyan reprezentáljuk a komponenseket.
Speciális esetben az AdaBoost „gyenge” tanuló algoritmusaként döntési fát szoktak alkalmazni [5]. Ilyenkor a gyenge hipotézis a tanítóhalmaz mintáinak partícionálását végzi el. A döntési fán alapuló gyenge hipotézis működését a (3), (4), (5) képletek írják le [5] és az  ábra szemlélteti. A W^j[b ]az j-edik partícióban szereplő, b célváltozó értékkel megegyező minták súlyát fejezi ki. A c[j ]értéke a j-edik partíció „jóságát” fejezi ki. A képletben szereplő Z értéke a súlyok kiszámításához felhasznált normalizáló tényező. A példában a döntési fa P[i] partíciókra osztotta fel a tanító adathalmazt (Ebben a példában a döntési fa két attribútum szerint vágott.).
Ez az algoritmus nem specifikálja, hogy milyen módszert használjunk a következő – a döntési listához adni kívánt – teszt kiválasztásához. Bár a bemutatott formálisan elért eredmények nem függnek a kiválasztás módszerétől, mégis az látszik józannak, ha előnyben részesítjük azokat az egyszerű teszteket, amelyek az osztályozott példák nagy részhalmazainak felelnek meg. Ennek eredményeként a döntési lista a lehető legtömörebb lesz. A legegyszerűbb stratégia, ha azt a legrövidebb t tesztet keressük meg, amely egy tetszőleges, egyforma osztálybasorolással rendelkező részhalmaznak megfelel, figyelmen kívül hagyva a részhalmaz méretét. Még ez a megközelítés is egész jól működik, mint az a  ábrán látható.  ábra - Egy döntési lista tanulásra használható algoritmus Egy döntési lista tanulásra használható algoritmus  ábra - A Döntési-Lista-Tanulás algoritmusnak az éttermi adatokon felvett tanulási görbéje. Az összehasonlítás kedvéért feltüntettük a Döntési-Fa-Tanulás algoritmus görbéjét is. A Döntési-Lista-Tanulás algoritmusnak az éttermi adatokon felvett tanulási görbéje. Az összehasonlítás kedvéért feltüntettük a Döntési-Fa-Tanulás algoritmus görbéjét is.
Aliz felfedezte, hogy a tanúskodás a játék domináns stratégiája (dominant strategy). Azt mondjuk, hogy egy p játékos egy s stratégiája erősen dominálja (strongly dominate) az s′ stratégiát, ha az s kimenetele p számára jobb, mint az s′ kimenetele, a többi játékos minden stratégiaválasztása esetén. Egy s stratégia gyengén dominálja (weakly dominates) s′-t, ha s jobb, mint s′ legalább egy stratégiaprofilnál, és nem rosszabb a többiben. Egy domináns stratégia olyan stratégia, ami az összes többit dominálja. Egy erősen dominált stratégiát irracionális követni, és ha létezik, irracionális eltérni a domináns stratégiától. Racionális volta miatt Aliz a domináns stratégiát választja. Már csak egy kevés terminológiára van szükségünk a továbbhaladáshoz: egy kimenetelt Pareto-optimálisnak (Pareto optimal) nevezünk,^[176] ha nincs másik kimenetel, amit az összes játékos preferálna. Egy kimenetel Pareto-dominált (Pareto dominated) egy másik kimenetel által, ha az összes játékos a másik kimenetelt preferálná.
definícióját a KimerítőFelosztás, a Partíció és a KölcsönösenKizáró definíciójával analóg módon. Igaz-e, hogy RészPartíció(s, Kötege(s))? Ha igen, bizonyítsa be, ha nem, adjon meg egy ellenpéldát, és definiáljon elégséges feltételeket, amelyek mellett igaz lesz.
A leírásunkat a Description elembe rakjuk, ezekbe tetszőleges számú és sorrendű információt rakhatunk az állományról. Az #-el jelölt tag, egy újabb leírás. Ennek kifejtése egy másik Description elemben történik.
A mérkőzés döntő játszmája a 2. játszma volt, amely emlékeimben sebként maradt meg … láttunk valamit, ami a legvadabb elképzeléseinket is felülmúlta, hogy egy számítógép hogyan lesz képes döntéseinek hosszú távú pozíciós konzekvenciáit előre látni. A gép – emberi veszélyérzetet produkálva – megtagadta, hogy egy döntően rövid távú előnyt jelentő állásba kerüljön.
és a releváns cselekvés a Kirakodás(C[1], p, B), ami az első literált adja. A cselekvés csak akkor működik, ha az előfeltételei teljesülnek. Ebből kifolyólag az elődállapotnak tartalmaznia kell a Benne(C[1], p) ∧ Ott(p, B) előfeltételeket. Mindezeken túl az Ott(C[1], B) részcél nem lehet igaz a megelőző állapotban.^[114] Így a megelőző állapot leírása az alábbi:
Ez a megfigyelés, amely különböző mintahalmazokra és hipotézisterekre nézve meglehetősen robusztusnak bizonyult, nagy meglepetést keltett a felfedezésekor. Az Ockham borotvája elv azt sugallja, hogy ne tegyük a hipotéziseket komplexebbé, mint szükséges, de a görbe azt mutatja, hogy a predikciók javulnak, ahogy az együttes hipotézis összetettebbé válik! Számos magyarázatot javasoltak erre a jelenségre. Az egyik megközelítés szerint a turbózás nem más, mint a Bayes-tanulás közelítése (lásd  fejezet), amely egy bizonyíthatóan optimális tanulási eljárás. Ez a közelítés javul, ahogy több és több hipotézist adunk az együtteshez. Másik lehetséges magyarázat, hogy további hipotéziseknek az együtteshez adása lehetővé teszi, hogy határozottabb legyen a megkülönböztetés a pozitív és negatív példák között, ami segít az új minták osztályozásában.  ábra - (a) A döntési tönkök és a turbózott döntési tönkök teljesítményének az étterem példán történő összehasonlítása M = 5 esetén. (b) A tanító halmazon és a teszthalmazon kapott jó válaszok aránya M-nek – az együttesben található hipotézisek számának – a függvényében. Figyeljük meg, hogy a teszthalmazon mért pontosság még azután is javul egy kicsit, amikor a tanító halmazon elérte az 1-et, azaz miután az együttes pontosan illeszkedett az adatokra. (a) A döntési tönkök és a turbózott döntési tönkök teljesítményének az étterem példán történő összehasonlítása M = 5 esetén. (b) A tanító halmazon és a teszthalmazon kapott jó válaszok aránya M-nek – az együttesben található hipotézisek számának – a függvényében. Figyeljük meg, hogy a teszthalmazon mért pontosság még azután is javul egy kicsit, amikor a tanító halmazon elérte az 1-et, azaz miután az együttes pontosan illeszkedett az adatokra.
Az induktív tanuló program tárgya egy olyan állításhalmaz Hipotézis-ként való előállítása, amely a vonzatkényszert kielégíti. Átmenetileg tegyük fel, hogy az ágens nem rendelkezik háttértudással, a Háttértudás üres. Akkor egy lehetséges megoldás a Hipotézis-re a következő:
Tegyük fel, hogy a porszívóvilág ágens tudja, hogy a jobb oldali négyzeten van, és az tiszta, de nem tudja érzékelni a piszok jelenlétét vagy hiányát a többi négyzeten. Ekkor, legjobb ismeretei szerint két állapotban lehet: a bal oldali négyzet vagy piszkos, vagy tiszta. Ezt a hiedelmi állapotot a  ábrán A-val jelöltük. Az ábra a „váltakozó dupla-Murphy” porszívóvilág és-vagy gráfjának egy részét mutatja be, melyben a tiszta négyzetet elhagyó ágens piszkot hagyhat maga után.^[126] Ha a világ teljesen megfigyelhető volna, az ágens egy „Mozogj balra és jobbra, és szívd fel a piszkot, amennyiben találsz, amíg mindkét kocka tiszta nem lesz, és a bal oldali kockán vagyok” formájú ciklikus megoldást készíthetne (lásd  feladat). Sajnos csak lokális szemétérzékeléssel ez a terv végrehajthatatlan, hiszen a „mindkét kocka tiszta” teszt igazságértéke nem határozható meg.
A keresés hatékonyságának becslésénél gondolhatunk a keresési költségre (search cost), amely tipikusan az időigénytől függ, de tartalmazhat egy tárigényt jelző komponenst is; vagy használhatjuk a keresés összköltségét (total cost), amely a megoldás útköltségét és a keresési költséget kapcsolja össze. Az Aradról Bukarestbe vezető útkeresési probléma esetén a keresési költség a keresés ideje, a megoldási költség pedig a teljes út hossza km-ben. Így amikor a teljes költséget akarjuk kiszámítani, kilométert és milliszekundumot kellene összeadnunk. Ez nem mindig egyszerű, mert nem létezik semmilyen „hivatalos váltószám” a kettő között, ennél a problémánál azonban értelmesnek tűnhet a kilométereket milliszekundumokra átszámítani a gépkocsi átlagsebességét használva (mert az ágensnek az idő a fontos). Ez lehetővé teszi, hogy az ágens megtalálja azt a kompromisszumot, amikor a legrövidebb út keresését célzó minden további számítás improduktívvá válik. A különböző javak közötti kompromisszum általános problémájára a  fejezetben még visszatérünk.
Az izületi koordináták tere (Q) n dimenziós, a (valós számokon alapuló) R^n valós tér altere. Q = {q: q[i min] < q[i] < q[i max]}, ahol q az izületi koordináták vektora, q[i min] és q[i max], i = 1, ..., n az egyes izületi koordinátáknak a konstrukciós feltételek által meghatározott határértékei, n pedig az izületek száma. A manipulátor helyzete a munkatérben egyértelműen meghatározott, ha adott egy q vektor, amely eleme a Q térnek.
A játékot legelőszőr egy dán matematikus mutatta be, Piet Hein. 1942-ben írt egy cikket, melyben [5] egy játékról volt szó, a Polygon névvel. Ebben a játékban a tábla 11x11-es volt, hatszögekből állt és a szabályai megegyeztek a Hex-ével. 1968-ban a Con-Tac-Tix nevet adta neki, és árusítani kezdte.
Ez a fejezet megmutatta, hogy hogyan kombinálhatjuk össze a hasznosságelméletet a valószínűségekkel, lehetővé téve az ágensnek olyan cselekvések kiválasztását, amelyek maximalizálni fogják a várható teljesítményét. * A valószínűség-elmélet (probability theory) előírja, hogy az ágens a tényei alapján milyen hiedelmekkel rendelkezzen, a hasznosságelmélet (utility theory) azt írja elő, hogy az ágens mit akar, a döntéselmélet (decision theory) pedig összeilleszti a kettőt, és előírja, hogy az ágens mit tegyen. * A döntéselmélet felhasználásával építhetünk egy olyan rendszert, ami a döntések meghozatalakor figyelembe veszi az összes lehetséges cselekvést, és azt választja ki, ami a legjobb várható kimenetelt adja. Egy ilyen rendszert racionális ágensnek (rational agent) nevezhetünk. * A hasznosságelmélet megmutatja, hogy az ágens, amelynek szerencsejátékok közötti preferenciái eleget tesznek bizonyos egyszerű axiómáknak, leírható egy hasznosságfüggvénnyel; továbbá az ágens a cselekvéseit úgy választja meg, mintha a várható hasznosságát maximalizálná. * A többattribútumú hasznosságelmélet (multiattribute utility theory) olyan hasznosságokkal foglalkozik, amelyek az állapotok több, különböző attribútumától függnek. A sztochasztikus dominancia (stochastic dominance) egy különösen hasznos technika egyértelmű döntések meghozatalára még az attribútumok pontos hasznosságértékei nélkül is. * A döntési hálók (decision networks) egy egyszerű formalizmust biztosítanak a döntési problémák leírására és megoldására. Ezek a valószínűségi hálók magától értetődő kiterjesztései döntési és hasznossági csomópontokkal a véletlen csomópontokon túl. * Bizonyos esetekben a probléma megoldásához hozzátartozik a többletinformáció megszerzése a döntés meghozatala előtt. Az információ értékét (value of information) úgy definiáljuk, mint a várható hasznosság javulását az információ nélküli helyzethez hasonlítva. * Azok a szakértő rendszerek (expert systems), amelyek hasznosságinformációkat is tartalmaznak, többletképességgel rendelkeznek a tisztán következtető rendszerekhez képest. Azon túl, hogy döntéseket tudnak hozni, dönthetnek az információ megszerzéséről annak értéke alapján, és kiszámíthatják döntéseiknek a valószínűségek és hasznosságok kis megváltozására vonatkozó érzékenységét.
3. lépés: ebben a lépésben LCV nélkül az WA=RED értékadás történt. Ugyanez a választás történik akkor is, ha LCV heurisztika szerint választunk értéket, mivel ekkor a heurisztikus értékek a következőképp alakulnak: WA{RED(1), GREEN(2), BLUE(2)}. E szerint tehát nem meglepő, hogy egyértelműen a WA=RED értékadásra esik a választás. De pontosan miért is ezek a heurisztikus értékek? RED(1) azért, mert a WA-val szomszédos változók közül csak NT értékkészletében van még RED. GREEN(2) és BLUE(2) pedig azért, mivel ebben a lépésben a WA-val szomszédos mindkét változó, azaz NT és SA értékkészletében is ott van a GREEN, illetve a BLUE érték (és ezek bizonyos egy alkalmas WA=GREEN vagy WA=BLUE értékadást követően már nem volnának értelmesek az NT és/vagy SA számára).
Megjegyzés: Ha több mint két bemenetünk van a rendszerben, akkor általában két-két bemenetet választva több úgynevezett szabályzó-felületet is megjeleníthetünk, hogy érzékeltessük egyes bemeneti paraméterek kölcsönhatását.
 Megjegyzés Tekintsük a  ábrán látható 4 × 3-as világot. a. Valósítson meg egy olyan környezetszimulátort erre a környezetre, amelyben a környezet földrajzi sajátosságai könnyen megváltoztathatók. Ehhez egy bizonyos forráskód már elérhető a könyv forráskódtárában a világhálón. b. Hozzon létre egy eljárásmód-iterációt használó ágenst, és mérje meg a teljesítményét a környezetszimulátorban különböző kiindulási állapotokból. Végezzen nagyszámú kísérletet mindegyik kiindulási állapotból, és hasonlítsa össze a futásonként kapott összjutalmak átlagát az állapot hasznosságával (amit a létrehozott algoritmus határozott meg). c. Kísérletezzen a környezet méretének a megnövelésével. Hogyan változik az eljárásmód-iteráció futási ideje a környezet méretének a változásával?
A különböző szimulációs alkalmazások, az ágens, és a multiágens rendszerek egyik legérdekesebb, legbonyolultabb része a számítógép-rendszereken belüli modellezésben. A természetben, és a társadalomtudományokban, közlekedéstervezésben, üzleti életben, bűnüldözésben stb. egyaránt bevált.
Ezzel tehát most már mindkét „irányban” definiáltuk a 3-as és 14-es kijelentéseknek megfelelő korlátokat, pontosabban az egyszerűség kedvéért nem is korlátokat, hanem értékkészlet-szintű megkötéseket feleltettünk meg az említett két informális állításnak.
A szenzorok másik fontos osztályát képezik a képérzékelők (imaging sensors). A kamerák képet adnak számunkra a környezetről, valamint – a  fejezetben tárgyalt, számítógépes gépi látási módszerek alkalmazásával – modellekkel, illetve tulajdonságokkal írják le a környezetet. A sztereolátás különösen fontos a robotikában, mivel mélységi információt is közvetít. Jövőjük mindamellett elég bizonytalan, mert sikeresen folyik új, aktív távolságleképezési technológiák kidolgozása.
Az elfogyasztott vagy teljesen megromlott ennivaló helyett azonnal új ennivaló jelenik meg valahol a pályán. Az új ennivaló értéke 300 és 1200 egység között egyenletes eloszlású véletlen változóként sorsolódik. A játék indulásakor az első négy ennivaló teljesen véletlenül kerül a pályán elhelyezésre. Ezt követően minden elfogyasztott ennivaló a többi, még meglévő ennivaló értéke alapján a következő képlet alapján generálódik:
A  fejezetben megmutattuk, hogy a Kalman-szűrők, a rejtett Markov-modellek és a dinamikus Bayes-hálók alkalmasak egy részlegesen megfigyelhető környezet állapotátmenet- és érzékelő modelljeinek a reprezentálására. Ismertettünk egzakt és közelítő algoritmusokat a belső hiedelmi állapot (belief state) – a környezet állapotváltozói felett értelmezett a posteriori valószínűség-eloszlás – frissítésére. Erre számos dinamikus Bayes-hálós modellt mutattunk be a  fejezetben. Robotikai problémák esetében a modellhez általában megfigyelt változóként a robot saját korábbi cselekvéseit is hozzáveszszük, mint ahogy az a  ábra hálózatán látható. A  ábra ennek a fejezetnek a jelöléseit mutatja: X[t] a környezet állapota (beleértve a robotot is) a t időpillanatban; Z[t] a t időpontbeli megfigyelés (az érzékelésből származó adatok), az A[t] pedig az érzékelést követően végzett cselekvés.
 Fontos Vegyük észre, hogy szigmoid perceptronok esetén a maximum-likelihood tanulás súlyfrissítési vektorát megadó egyenlete alapvetően azonos a négyzetes hiba minimalizálásán alapuló frissítés vektorával. Tehát azt mondhatjuk, hogy a perceptronnak még akkor is van valószínűségi interpretációja, ha a tanulási szabályt determinisztikus megközelítéssel származtattuk.
Avrim Blum és Merrick Furst felpezsdítette a tervkészítés területét Graphplan rendszerével (Blum és Furst, 1995; 1997), ami több nagyságrenddel gyorsabb volt az akkori részben rendezett tervkészítőknél. Más gráftervező rendszerek, mint az IPP (Koehler és társai, 1997), a Stan (Fox és Long, 1998) és az SGP (Weld és társai, 1998) hamarosan követték. Egy kicsivel korábban Ghallab és Laurelle a tervkészítő gráfhoz nagyon hasonló adatstruktúrát fejlesztettek ki (Ghallab és Laurelle, 1994). Az IxTeT részben rendező tervkészítőjük az adatstruktúrát a keresés irányítására szolgáló heurisztikák kinyerésére alkalmazta. Nguyen és társai a tervkészítő gráfokból származtatott heurisztikák nagyon alapos áttekintését adták (Nguyen és társai, 2001). A mi tervkészítő gráfokról szóló leírásunk részben ezen, részben pedig Subbarao Kambhampati jegyzetein alapul. Ahogyan már a fejezetben említettük, többféle módon használható a tervezési gráf a megoldás keresésének irányítására. A 2002-es AIPS tervkészítő győztese, az LPG (Gerevini és Serina, 2002), a WalkSAT által ösztönözve lokális keresési technikát használt a tervkészítési gráf keresésére.
Az (x, y) az (x[0 ], y[0]) képpont körüli tömb képpontjainak halmazán fut végig. Megkeressük most azt a (D[x], D[y])-t, amely az SSD-t minimizálja. Az (x[0 ], y[0])-beli optikai folyam ilyenkor: (v[x ], v[y]) = (D[x]/D[t], D[y]/D[t]). Alternatívaként maximalizálhatjuk a keresztkorrelációt (cross-correlation):
A tétel számos valós életbeli helyzetben érvényesül. Ha például van két nyomozónk, akik ugyanolyan elven következtetnek, úgy még hogyha más nyomokat is gyűjtöttek össze, amennyiben kölcsönös tudás kettejük között, hogy e nyomok alapján mire következtetnek (pl. ha elég sokáig beszélnek róla), akkor végül ugyanarra fognak következtetni.
i) A algoritmus optimális, ha 0 ≤ w ≤ 1. Intuitívan arról van szó, hogy az eddig megtett utat (amiről pontos adatunk van) nagyobb (pontosabban legalább akkora) súllyal kell figyelembe venni, mint a (csupán becslésre szolgáló) heurisztikus függvényt ahhoz, hogy a legolcsóbb utat találhassuk meg.
Példarobotunk konfigurációs terét mutatja a  (b) ábra a  (a) ábrán látható akadályok esetére. A konfigurációs teret két altérre bonthatjuk: azoknak a konfigurációknak a terére, amelyek elérhetők – ez a szabad tér (free space), illetve az elérhetetlen konfigurációk által alkotott térrészre, amit foglalt térnek (occupied space) hívnak. A  (b) ábrán a fehér rész jelenti a szabad teret, míg az összes többi a foglalt tér. A különböző árnyalatok jelzik, hogy melyik akadályról is van szó: a szabad teret körbehatároló fekete részek jelentik azokat a konfigurációkat, amelyeknél a robot önmagával ütközik össze. Könnyű belátni, hogy a váll- és könyökcsuklók szélső állásai ezt eredményezik. A robot két oldalán lévő ovális részek az asztalnak felelnek meg, amin a robot áll. A harmadik ovális terület a bal oldali fal. Végezetül, a konfigurációs térben a legkülönösebb alakzat a robot mozgásterébe belelógó, egyszerű függőleges elem. Ennek igen különös alakja van, erősen nemlineáris, és helyenként még konkáv is. Egy kis képzelőerővel felismerhetjük a megfogó alakját a bal felső szélén. Szakítsunk egy kis időt ennek a fontos ábrának a tanulmányozására. Az alakzat egyáltalán nem nyilvánvaló! A  (b) ábrán a pont a robot  (a) ábrán lévő állapotának megfelelő konfigurációt mutatja. A  ábrán további három konfigurációt is láthatunk, mind a munkatérben, mind a konfigurációs térben. A „conf-1” állásban a megfogó megközelíti a középső akadályt.
A kiterjesztett tudás bázis tehát ellentmondásos, és ha igaznak tartjuk az eredeti tudás bázist (állítás I. – III.), akkor a bajt csak a IV. állítást okozhatja. Az állítás tehát hamis, avagy a ponált állítás és az eredeti kérdés igaz.
Ebben a fejezetben a tervkészítési feladatot determinisztikus és teljesen megfigyelhető környezetek esetén definiáltuk. Bemutattuk a tervkészítési feladatok leírására használt főbb reprezentációkat, valamint számos algoritmikus megközelítést a megoldásukra. Idézzük fel a fontosabb állításokat: * A tervkészítő rendszerek olyan problémamegoldó algoritmusok, melyek állapotok és cselekvések explicit ítéletlogikai (vagy elsőrendű) reprezentációin működnek. Ezek a leírások lehetővé teszik hatékony heurisztikák származtatását, valamint erős és rugalmas algoritmusok kifejlesztését a problémamegoldáshoz. * A Strips nyelv az előfeltételek és következmények segítségével írja le a cselekvéseket, valamint a kiindulási és célállapotokat pozitív literálok konjunkciójaként adja meg. Az ADL nyelv lazít ezeken a megkötéseken, megengedi a diszjunkciót, a negálást és a kvantorokat. * Az állapottér-keresés történhet előrefelé: progreszív (progression) vagy hátrafelé: regresszív (regression). Hatékony heurisztikák származtathatók a részcélok függetlenségét feltételezve és a tervkészítési feladat különböző gyengítéseivel. * A részben rendezett tervkészítő (RRT) algoritmusok a tervek terében keresnek anélkül, hogy cselekvések teljesen rendezett sorozatára jutnának. A céltól visszafelé működnek a részcélok eléréséhez szükséges cselekvéseknek a tervhez való hozzáadásával. Ezek az algoritmusok különösen hatékonyak az „oszd meg és uralkodj” megközelítéssel kezelhető problémákra. * Egy tervkészítési gráf (planning graph) megalkotható inkrementálisan a kiinduló állapotból elindulva. Minden egyes réteg az adott időpillanatban végrehajtható öszszes cselekvés és literál halmazát tartalmazza, rögzíti a kölcsönös kizárásokat vagy mutexeket, azaz olyan literálok és cselekvések közötti relációkat, melyek egyszerre nem következhetnek be. A tervkészítési gráfok hasznos heurisztikákra vezetnek az állapottér és a részben rendezett tervkészítők esetén, és közvetlenül felhasználhatók a Graphplan algoritmusban. * A Graphplan algoritmus a tervkészítési gráfot dolgozza fel visszafelé keresést alkalmazva a terv kinyeréséhez. Lehetőséget biztosít továbbá még a cselekvések között bizonyos részben rendezésekre. * A SATplan algoritmus a tervkészítési problémát ítéletlogikai axiómákra fordítja, és egy kielégíthetőségi algoritmust használ a helyes tervnek megfelelő modell megtalálására. Számos különböző ítéletlogikai reprezentációt fejlesztettek ki különböző mértékű tömörséggel és hatékonysággal. * A tervkészítés fő megközelítései mindegyikének vannak nehézségei, és egyelőre nincs egyetértés abban, hogy melyik a legjobb. A versengés és a módszerek ötvözése komoly előrehaladást eredményezett a tervkészítő rendszerek hatékonyságában.
Most definiáljuk az ágensek jóságát. Adjuk meg az ágensek adott környezetben vett hasznát, s e haszon maximalizálását tekintsük az ágens által megoldandó problémának. Ennek alapján tehát f Ágens  hasznossága a Környezet-ben: V(f[Ágens], Környezet) = U(Hatás(f[Ágens], Környezet))
Harmadszor pedig tekintsük az operáció visszacsinálása utáni helyzetet, amikor a kísérlet alanyának ismét normális agya van. Emlékezzünk arra, hogy a definíció szerint a kísérleti alany külső viselkedésének olyannak kell lennie, mintha az operáció nem is történt volna meg. Ennek speciális eseteként fel kell tudnunk tenni azt a kérdést: „Milyen volt az operáció alatt? Emlékszik a mutatópálcára?” A kísérleti alanynak pontos emléknyomokkal kell rendelkeznie a tudatos tapasztalatai tényleges természetéről, beleértve a qualiát is; mindannak ellenére, hogy Searle szerint nem is voltak ilyen tapasztalatai.
Egyenletes felosztás: Az alsó ábrán látható egy felosztás eredménye, aminek során hét fuzzy halmazt hoztunk létre. Első lépésben meghatároztuk azt az intervallumot, ami tartalmazza a számunkra érdekes változó értékeket. Az így kialakult intervallumra négy darab azonos nagyságú egyenlőszárú háromszöget illesztettünk, ezeket kék színnel jelöltük. A szükséges további három darab piros színű háromszöget pedig a meglévők közé illesztettük.  ábra - Egyenletes felosztás Egyenletes felosztás
Most pedig az előbbiekre építve kimondhatjuk és bizonyíthatjuk a kölcsönös tudás elméletének egyik legalapvetőbb és egyben leginkább meghökkentő tételét, Robert Aumann Egyetértési Tételét (Agreement Theorem), amely a kölcsönös tudás akár józan intuíción is túlmutató következtetési potenciáljára mutat rá.
Ennek eléréséhez ésszerű megkötéseket írunk elő a preferenciarelációkra, hasonlóan ahhoz, ahogyan racionális kényszereket vártunk el a meggyőződés mértékeivel kapcsolatban a valószínűség axiómáinak a származtatásánál a  fejezetben. Egy ésszerű megkötés, hogy a preferenciák tranzitívak (transitive) legyenek: azaz ha A ≻ B és B ≻ C, akkor elvárhatjuk, hogy A ≻ C. A tranzitivitás indoklásaként megmutatjuk, hogy egy ágens, amelynek a preferenciái nem tranzitívak, irracionálisan viselkedne. Tételezzük például fel, hogy egy ágens preferenciái nem tranzitívak: A ≻ B ≻ C ≻ A, ahol A, B és C áruk, amik szabadon cserélhetők. Ha az ágensnél jelenleg A van, akkor felkínálhatjuk C-t cserébe A-ért és egy kis pénzért. Ha az ágens jobban szereti C-t, akkor hajlandó lesz pénzt is áldozni C megszerzésére. Ezután felkínálhatjuk B-t C-ért, még több pénzt begyűjtve, és végül elcseréljük B-t A-ért. Ezzel visszaértünk a kezdeti állapotba, azt leszámítva, hogy az ágensnek kevesebb a pénze ( (a) ábra). Ezt a ciklust addig folytathatjuk, ameddig az ágens összes pénze el nem fogy. Indokoltnak látszik azt állítani, hogy az ágens ebben a helyzetben nem cselekedett racionálisan.
Legyen C[1] és C[2] két klóz, amelyeknek nincsenek közös változói. Legyenek C[1]′ és C[2]′ az alappéldányai C[1]-nek és C[2]-nek. Ha C′ rezolvense C[1]′-nek és C[2]′-nek, akkor létezik egy olyan C klóz, amelyik (1) rezolvense C[1]-nek és C[2]-nek és (2) C′ alappéldánya C-nek.
Az értékadás eredményeképp az {SA=RED, NSW=GREEN} behelyettesítés áll elő. Mivel ez konzisztens, ezért jöhet az AC3/MAC alapú következtetés. Ennek során először is az előbbi NSW=GREEN értékadásnak megfelelően {GREEN}-re redukáljuk az NSW változó értékkészletét, majd pedig az NSW-re mutató élekből létrehozzuk a tankönyv  ábráján látható AC3 algoritmus működéséhez szükséges kiindulási él-listát. Ez a lista most tehát a következő:
Mint a legtöbb esetben a hibrid megközelítés feloldja az ellentmonást. Jól tervezett hierarchiák használatával a HFH tervkészítők, mint a PRS (Georgeff és Lansky, 1987) és a RAP (Firby, 1996), valamint a folytonos tervkészítő ágensek használható válaszidőket és összetett hosszú távú terveket érhetnek el számos problémakörben.
A sorozatok közti pontosság mérhető naponta (egy hónapon keresztül, azaz day to day vagy naponkénti pontosság), de mérhető az egyes sorozatok között (run tor un pl automata analizátornál minden tárcsába vagy minden 50 minta után mért kontrollal).
Ez a kifejezés egy ciklussal értékelhető ki, sorban végighaladva a változókon, miközben összeszorozzuk az FVT-bejegyzéseket. Minden összegzésnél, a változó lehetséges értékei szerint is iterálnunk kell. A számítás struktúráját a  ábra mutatja. Felhasználva a  ábra értékeit, azt kapjuk hogy P(b∣j,m) = α × 0,00059224. A ¬b-hez tartozó számítást átfutva az α × 0,0014919-et eredményez; így  ábra - A felsoroló algoritmus Bayes-hálós lekérdezések megválaszolására A felsoroló algoritmus Bayes-hálós lekérdezések megválaszolására
A problémát az okozza, hogy bizonyos éleket feleslegesen teszünk vissza a sorba, és ezeket újra (feleslegesen) ellenőrizzük. Javulást érhetünk el, ha csak azokat az éleket tesszük vissza a sor végére, amelyekre hatással volt a változtatás.
A rekurzívan felsorolható nyelvtanok (recursively enumerable grammars) korlátozás nélküli szabályokat használnak: az átíró szabályok mindkét oldala tetszőleges számú záró és nem záró szimbólumot tartalmazhat, mint például az A B → C szabály. Ezen nyelvtanok kifejezőereje megegyezik a Turing-gépével.
Feltehetjük magunknak a kérdést, hogy vajon a h[2] mindig jobb-e, mint a h[1]? A válasz: igen. A két heurisztikus függvény definíciójából jól látszik, hogy minden n csomópontra h[2](n) ≥ h[1](n). Azt mondjuk, hogy h[2] dominálja h[1]-et. A domináció közvetlenül átvihető a hatékonyságra: a h[2]-t használó A^* algoritmus kevesebb csomópontot fog kifejteni, mint a h[1]-et használó (talán csak az f(n) = C^* tulajdonságú néhány csomópontot kivéve). Ezt az alábbi egyszerű gondolatmenettel mutathatjuk meg. Idézzük fel a 4. szakasz - A* keresés: a teljes becsült útköltség minimalizálása részben tett észrevételünket, miszerint minden csomópont kifejtésre kerül, amelyre f(n) < C^*. Ezzel egyenértékű az az állítás, mely szerint minden csomópont kifejtésre kerül, amelyre h(n) < C^*– g(n). Mivel azonban h[2]minden csomópontra legalább akkora, mint h[1], így minden olyan csomópontot, amit kifejt a h[2]-t alkalmazó A^* algoritmus, kifejti a h[1]-et alkalmazó A^* algoritmus is, de h[1] alkalmazása más csomópontok kifejtését is okozhatja. Ebből adódóan mindig jobb nagyobb értékeket adó heurisztikus függvényeket alkalmazni, amíg nem becsüljük túl a valódi költséget, és a heurisztika számítási ideje nem túlságosan nagy.
Haladjunk sorban: először is az AC3 a (Q, NSW) élet veszi ki a fenti él-listából, majd végighalad Q változó értékkészletén, és megnézi, hogy van-e olyan értéke, amelyhez nem társítható konzisztens NSW-érték. Q értékkészletében egyetlen ilyen érték van: a RED, mivel NSW értékkészlete az előbbi redukció eredményeképp már csak {RED}, és a Q és NSW változó a 8-as korlát miatt nem lehet azonos értékű. A Q változó RED értékét tehát az algoritmus, mint inkonzisztens értéket, kiveszi a Q értékkészletéből.
Mivel az egzisztenciális kvantorok nincsenek univerzális kvantor hatáskörében, kiküszöbölhetjük őket konstansok bevezetésével: a premisszában szereplő y-t helyettesítsük Y-nal, a következmény negáltjában szereplő x-et pedig X-szel! Nincs változóütközés sem, az univerzális kvantorokat pedig elhagyhatjuk. Az így kapott mondat már konjunktív normál formájú:
A betegazonosítók formáját és tartalmát törvények, a betegbiztosítóval kötött szerződés és egyéb jogszabályok határozzák meg. Fontos továbbá ismernünk a beküldő osztályt, a beküldő orvost és elérhetőségi telefonszámát, a beküldő orvos személyi pecsétjének számát is.
Az osztályozás és regresszió esetében a predikciós teljesítmény mellett a másik cél a bemeneti változók szerepének a megértése a kimeneti változó predikciójában. Mindkét esetet lefedve az egyszerűség kedvéért feltesszük, hogy ez azt jelenti, hogy a p(Y|X)  feltételes eloszlás vizsgálata a célunk. Ehhez szükségünk van a relevancia fogalmára.
Az ítéletlogika feltételezi például, hogy a definiált tények fennállnak vagy nem állnak fenn a világban. Minden ténynek két állapota lehet: igaz vagy hamis.^[75] Az elsőrendű logika bonyolultabb feltételezésekre épít, nevezetesen, hogy a világ objektumokból épül fel, amelyek között relációk léteznek, amelyek vagy fennállnak, vagy nem. A speciális célú logikák további ontológiai hozzárendeléseket tesznek; például a temporális logika (temporal logic) feltételezi, hogy a tények csak bizonyos időben állnak fenn, és hogy ezek az idők – amelyek lehetnek időpontok és időtartamok is – rendezhetők. Így ezek a speciális célú logikák az objektumok egy bizonyos fajtájához (és az azokhoz tartozó axiómákhoz) „első osztályú” státust rendelnek a logikán belül, ahelyett hogy egyszerűen meghatároznák őket a tudásbázison belül. A magasabb rendű logika (higher-order logic) úgy tekint az elsőrendű logikában relációknak és függvényeknek nevezettekre, mintha azok maguk is objektumok lennének. Ez lehetővé teszi, hogy minden relációról állításokat alkossunk – például arra, ha valaki azt szeretné meghatározni, mit jelent, hogy egy reláció tranzitív. Eltérően más speciális célú logikáktól a magasabb rendű logika egyértelműen nagyobb kifejezőerővel rendelkezik, mint az elsőrendű logika, abban az értelemben, hogy néhány magasabb rendű logikai állítás nem írható le véges számú elsőrendű logikai állítással.
Egy kicsit robusztusabb megközelítést kapunk, ha a  fejezetben definiált iteratívan mélyülő algoritmust alkalmazzuk. Ha a program kifut az időből, akkor a legmélyebb, befejezett keresés által kiválasztott lépést adja vissza. Az ilyen megközelítések azonban hibákhoz vezethetnek a kiértékelő függvény közelítő jellege miatt. Tekintsük ismét a sakkban a pontelőnyön alapuló kiértékelő függvényt. Tegyük fel, hogy a program a mélységkorlátig keres, és a  (b) ábra táblaállásába jut, ahol Feketének huszár és két gyalog előnye van. A program a heurisztikus érték alapján ezt az állapotot Fekete számára valószínű győzelemnek fogja minősíteni. Fehér következő lépésében azonban leüti a fekete vezért, kárpótlás nélkül. Az állás így valójában Fehér számára jelent győzelmet, viszont hogy ezt belássuk, egy további lépésváltásig előre kellene nézni.
Ennek minimálmodellje M[1], így M[1] egy stabil modell. A válaszhalmaz programozás (answer set programming) a logikai programozásnak a negálás mint kudarc-cal bővített fajtája, amely úgy működik, hogy a logikai programot rögzített formába transzformálja, majd stabil modelleket (amelyeket válaszhalmazoknak – answer sets – is neveznek) keres, ítéletkalkulus modell-ellenőrzési technikákat felhasználva. A válaszhalmaz-programozás leszármazottja tehát mind a Prolognak, mind az olyan gyors ítéletkalkulus-beli kielégíthetőségi tételbizonyítóknak, mint a WalkSAT. A válaszhalmaz programozást sikerrel alkalmazták tervkészítési problémákra, hasonlóan az ítéletkalkulus-beli kielégíthetőségi tételbizonyítókhoz. A válaszhalmaz-programozás előnye más tervkészítőkkel szemben a rugalmassága: a tervkészítő operátorokat és a kényszereket logikai programokkal fejezi ki, és így azok nem kötődnek a konkrét tervkészítési formalizmus korlátozott formátumához. A válaszhalmaz-programozás hátránya ugyanaz, mint minden ítéletkalkulus szintű technikáé: ha az univerzumban túl sok objektum létezik, egy exponenciális lassulással kell számolnunk.
Ebben a szócikkben a Mesterséges Intelligencia tankönyv korlátkielégítési problémákkal (KKP) foglalkozó  fejezetében szereplő fontosabb algoritmusok és heurisztikák kerülnek részletes bemutatásra. Az algoritmusokat és heurisztikákat az áttekinthetőség és szemléletesség érdekében a könyvfejezetben is szereplő „Ausztrália térképének kiszínezése” példán mutatjuk be, lépésről-lépésre, bőséges magyarázattal. Ezen felül egy összetettebb KKP feladat (a könyvfejezethez kapcsolódó feladatok közül az utolsó, 5.13-as, „Zebra feladvány (Zebra Puzzle)”, vagy „Einstein feladványa (Einstein’s riddle)” néven is ismert példa) megoldását is bemutatjuk kezdve a modellezéstől egészen a futási eredmények értékeléséig.
Magyarország. Ekkoriban az MTA (Magyar Tudományos Akadémia) és a KFKI (Központi Fizikai Kutató Intézet) laboratóriumaiban igen kutatott téma volt a tételbizonyítás és a programhelyesség bizonyítás. 1973-ban elkészült az első rezolúciós tételbizonyító. A NIM IGÜSZI (Nehézipari Minisztérium Ipargazdasági és Üzemszervezési Intézete) pedig implementációval is foglalkozott. A NIM akkori szoftverfejlesztési vezetője potenciált látott a logikai programozásban, ezért több konferenciát is szervezett a témában. Ezek (és a Prolog I 1972-es megjelenésének) hatására Szeredi Péter 1975-ben megírta a világ második Prolog interpreterjét CDL nyelven [4].
Ha csak a görbe pozitív részét vizsgáljuk, ahol a meredekség csökken, akkor itt bármely L szerencsejáték esetén a várható hasznosság kisebb, mint annak a biztos eseménynek a hasznossága, amikor az átadott pénzügyi érték a várható pénzügyi értékkel egyezik meg.
Ez a technika a leg számításigényesebb az itt felsoroltak közül: nem csak, hogy meg kell becsülni a várható hibát az U felett, de minden egyes új címke esetén újra kell tanítani a modellt. A nagy számításigény miatt ennél a módszernél sokszor alkalmaznak pl.: Monte Carlo szimulációt.
A CV koncentrációfüggő ezért referens, és patológiás (alacsony és magas) értékeknél is meg kell határozni. A módszer kidolgozásánál a klinikai követelmény definiálja, hogy milyen értéknél/koncentrációnál legyen a legkisebb a CV. Általában a referens/patológiás küszöbértéknél kell a legpontosabban mérni, tehát úgy kell kialakítani a mérési paramétereket, hogy itt legyen legkisebb a CV (1.sz ábra).  ábra - A Gauss-eloszlás esetén az X, SD a CV (VK) számításához A Gauss-eloszlás esetén az X, SD a CV (VK) számításához
mondatatot tényleg könnyű elolvasni. Az olvasó arra gondolhat, hogy ezt az teszi lehetővé, hogy teljesen ismerjük az angol szintaktikát, szemantikát és pragmatikát. Meg fogjuk mutatni, hogy a mondatot egy egyszerű unigram modellel is könnyen dekódolni lehet.
Két megfigyelést érdemes még tennünk, mielőtt tovább lépünk: egyrészt vegyük észre, hogy már a ChesterfieldsHouse, FoxHouse, és CHC változókra vonatkozó, 2-1. táblázatban foglalt sima ternáris érték-korlátot is úgy definiáltuk, hogy amennyiben például a FoxHouse az első ház lenne, úgy a ChesterfieldsHouse ne lehessen tőle balra. Illetve fordítva is igaz, ha a ChesterfieldsHouse lenne az első ház, azaz ChesterfieldsHouse=House1 állna fenn, akkor a FoxHouse nem lehetne tőle balra, hanem mindenképpen jobbra helyezkedne el, azaz ekkor FoxHouse=House2 kellene, hogy teljesüljön, és emellett persze CHC=-1 (ami azt jelezné, hogy a FoxHouse-tól egyel balra van a ChesterfieldsHouse). Ezen felül nyilván ugyanez igaz az értéktartomány jobb oldalán: azaz, ha a ChesterfieldsHouse volna a legjobboldalibb ház, azaz ChesterfieldsHouse=House5 teljesülne, akkor a FoxHouse már nem lehetne tőle jobbra, azaz FoxHouse=House4 lehetne csak, és így CHC=+1. Hasonlóképp, ha FoxHouse volna a legjobboldalibb ház, azaz FoxHouse=House5 teljesülne, akkor a ChesterfieldsHouse-nak kellene tőle balra lennie (hiszen jobbra már nincs hely további, úgymond hatodik háznak), azaz ChesterfieldsHouse=House4 kellene, hogy teljesüljön, és persze CHC=-1.
Azonban nem minden esetben ilyen markáns egy releváns változó hatása, hiszen lehetséges, hogy csak bizonyos körülmények között jut meghatározó szerephez. A gyenge relevancia pontosan ezt a hatást írja le.
Egy trükköt alkalmaztunk a teljes algoritmus elindításához: hozzáadtunk egy [0, 0, S' → • S] élet a diagramhoz, ahol S a nyelvtan kezdeti szimbóluma, S' pedig egy általunk most kitalált új szimbólum. Az Él-hozzáad meghívása azt eredményezi, hogy a Jósló éleket ad az olyan szabályokhoz, melyek S-t eredményezhetnek, azaz [S → NP VP]. Ezek után megvizsgáljuk ezen szabály első alkotóelemét, NP-t, és mindenféle módon olyan szabályokat adunk hozzá, melyek NP-t eredményeznek. Végső soron a Jósló fentről lefelé módon minden lehetséges élt hozzáad, ami felhasználható a végső S megalkotásában.  ábra - A diagramelemző algoritmus. S a kezdő szimbólum és S' egy új nem záró szimbólum, diagram[j] pedig azon élek listája, melyek a j csomópontban végződnek. A görög ábécé betűi nulla vagy több szimbólumból álló füzérekre illeszkednek. A diagramelemző algoritmus. S a kezdő szimbólum és S' egy új nem záró szimbólum, diagram[j] pedig azon élek listája, melyek a j csomópontban végződnek. A görög ábécé betűi nulla vagy több szimbólumból álló füzérekre illeszkednek.
Az akusztikai modell második része a beszédhangok akusztikus jelként való megvalósulási módjával foglalkozik: azaz a rejtett Markov-modell E[t] bizonyítékváltozója adja meg az akusztikus jel t időpontban megfigyelt jellemzőit, és az akusztikus modell adja meg az P(E[t]|X[t]) feltételes valószínűséget, ahol X[t] az aktuális beszédhang. A modellnek lehetőséget kell adni eltérésekre a hangmagasságban, a sebességben és a hangerőben, és jelfeldolgozási (signal processing) technikákon alapul, hogy olyan jelreprezentációt biztosítson, ami megfelelően robusztus módon kezeli ezeket a fajta eltéréseket.  ábra - A DARPA fonetikus ábécé, avagy ARPAbet, amely az amerikai angolban használt beszédhangokat sorolja fel. Számos alternatív felírás létezik, köztük az International Phonetic Alphabet (IPA), mely az összes ismert nyelv beszédhangját tartalmazza. A DARPA fonetikus ábécé, avagy ARPAbet, amely az amerikai angolban használt beszédhangokat sorolja fel. Számos alternatív felírás létezik, köztük az International Phonetic Alphabet (IPA), mely az összes ismert nyelv beszédhangját tartalmazza.
A  fejezet egy kifejező, de mégis megfelelően korlátozott nyelvet mutat be az állapotokat és cselekvéseket tartalmazó tervkészítési feladatok leírására. Ez a nyelv szoros kapcsolatban áll a 7. és  fejezetben szereplő ítéletlogikai, illetve elsőrendű leírásokkal. A  fejezet megmutatja, hogy az előre- és a hátrafelé kereső algoritmusok hogyan tudják előnyösen kihasználni ezt a reprezentációt, elsősorban a leírás szerkezetéből automatikusan levezethető heurisztikák segítségével. (Ez analóg az  fejezetben bemutatott, a kényszerkielégítési problémához kialakított heurisztika elkészítésével.) A 11.3– alfejezetben olyan tervkészítő algoritmusok kerülnek bemutatásra, amelyek felhasználva a probléma reprezentációját, képességeikben túlmutatnak az előre-, illetve hátrafelé keresésen. Nevezetesen olyan eljárásokat vizsgálunk meg, amelyek nem csak teljesen rendezett cselekvések sorozatát képesek figyelembe venni.
* Zebra: a házakban háziállatként tartott zebrát reprezentáló érték * Dog: a házakban háziállatként tartott kutyát reprezentáló érték * Fox: a házakban háziállatként tartott rókát reprezentáló érték * Snails: a házakban háziállatként tartott csigákat reprezentáló érték * Horse: a házakban háziállatként tartott lovat reprezentáló érték
Az előző alfejezetben bemutattuk a hálót, de annak jelentését nem. A Bayes-hálók szemantikáját kétféle módon lehet megérteni. Az első szerint a háló az együttes valószínűség-eloszlás függvény egy leírása. A második szerint a háló feltételes függetlenségekről szóló állítások együttesét írja le. A két szemlélet ekvivalens, de az első inkább abban segít, hogyan hozzunk létre egy hálót, míg a második a következtetési eljárások tervezését segíti.
Foglalja össze a legfontosabb különbségeket a Java (vagy bármely más számítógépes nyelv, amelyet ismer) és a magyar között, megjegyezve a „megértés” problémáját mindkettőre! Gondoljon olyan dolgokra, mint a nyelvtan, a szintaxis, a szemantika, a pragmatikus elemzés, az összetételek, a környezetfüggőség, a lexikális többértelműség, a szintaktikai többértelműség, az utalások megtalálása (a névmásokat is ideértve), a háttértudás és leginkább arra, hogy mit is jelent „megérteni”.
Ha az ágens egymaga van a környezetében (azaz nincsenek más ágensek), a környezet determinált, és nem dinamikus, úgy általában igen egyszerű meghatározni, hogy mit kellene tennie az ágensnek ahhoz, hogy egy adott teljesítménymértéket maximalizáljon. Viszont, ha a környezet nem determinisztikus, vagy dinamikus (például azért, mert más ágensek is tevékenykednek benne), úgy a teljesítménymérték maximalizálása már egyáltalán nem triviális feladat. Több ágens esetén a játék-elmélet kínál használható megoldást, nem-determinisztikus esetben a Markov-folyamatok, míg nem teljesen hozzáférhető (kvázi nem-determinisztikus) környezet esetén többek között a megerősítéses tanulás.
Definíció (Gyenge relevancia) Egy X[i] jegy gyengén releváns akkor és csak akkor, ha nem erősen releváns, és létezik egy S[i]’ jegy-részhalmaz ( S i ' ⊂ S i  ), melyre létezik valamely x[i], y és s[i]’, melyekre p(x[i], s[i]’)>0 úgy, hogy p(y|x[i], s[i]’) ≠ p(y|s[i]’).
A valós világgal összevetve e probléma jellemzői a diszkrét lokációk, a diszkrét kosz, a megbízható takarítás, valamint az, hogy takarítás után piszok soha nem keletkezik újra (a  alfejezetben ezeket a feltételezéseket feladjuk). Fontos megjegyezni, hogy az állapotot az ágens és a kosz helyzete együttesen határozza meg. Egy nagyobb, n helyű környezetben n 2^n állapot van.
Az iteratív mélyülés megkezdése előtt a program egy 3 mélységű alfa-béta vágást alkalmaz, majd az innen kapott eredmények alapján dönt a további lépésekről. Ha az alfa-béta vágás eredményeképpen egy lépés sokkal jobbnak bizonyul a többinél, akkor ez esetben az iteratív mélyülés ebben az irányban kezd el futni, és kiértékeli, hogy a többinél sokkal jobbnak tűnő lépés valóban jobb lenne-e további vizsgálatok esetén is. Ha az iteratív mélyülés is hasonló eredményre jut, akkor ezt a lépést fogja a CHINOOK rögtön meglépni, hogy egy ember számára esetlegesen nyilvánvalóan jó lépést a program is gyorsan felfedezzen [7].
Alkalmazása a következő egészségügyi területeken lehetséges: * kezelés, szervátültetés, * mindenütt elérhető orvosi információ e-health, * palliatív kezelések, * orvosi szótár, orvosi adatbázisok, * döntéstámogatás, * járványügyi modellek, * egészségügyi képfeldolgozás, * beszédfelismerés, * orvosi robotok,
Az A[0] tartalmazza az összes cselekvést, ami az S[0] állapotban előfordulhat, de ugyanilyen fontos, hogy rögzíti a cselekvések közötti ütközéseket is, amelyek megakadályozzák, hogy egyszerre történjenek. A  ábra szürke vonalai ezeket a kölcsönös kizárási (mutual exclusions vagy mutex) kapcsolatokat jelölik. Például az Enni(Süti) kölcsönös kizárásban van a Van(Süti) vagy a ¬Megevett(Süti) megőrzésével. Hamarosan látjuk, hogy a mutex kapcsolatokat hogyan számíthatjuk.  ábra - A „legyen süti és együnk is” probléma tervkészítési gráfja az S[2] szintig. A téglalapok az akciókat jelentik (a kicsi négyzetek a megőrző cselekvések), az egyenesek az előfeltételeket és a következményeket jelölik. A kölcsönös kizárásokat ívelt szürke vonalak jelölik. A „legyen süti és együnk is” probléma tervkészítési gráfja az S2 szintig. A téglalapok az akciókat jelentik (a kicsi négyzetek a megőrző cselekvések), az egyenesek az előfeltételeket és a következményeket jelölik. A kölcsönös kizárásokat ívelt szürke vonalak jelölik.
A részben rendezett tervkészítés alapötletei tartalmazzák az ütközések érzékelését (Tate, 1975a) és az elért feltételek kölcsönhatásoktól való védelmét (Sussman, 1975). A részben rendezett tervek (melyeket akkor feladathálóknak – task network – neveztek) készítését a Noah tervkészítő (Sacerdoti, 1975; 1977) és Tate Nonlin rendszere vezette be (Tate, 1975b; 1977).^[117]
A játékban a kupacaink stílusosan pókerzsetonokból állnak, és két lépést engedünk meg [5]. Vagy kiválasztunk egy kupacot, és elveszünk belőle legalább egy, legfeljebb az összes zsetont, mint a Nim játékban, vagy kiválasztunk egy kupacot, és hozzáteszünk néhány zsetont, legalább egyet, legfeljebb annyit, amennyit előzőleg valamely kupacokból elvettünk. A játék izomorf a Nimmel, hisz egy kupac duzzasztása lényegtelen lépés olyan szempontból, hogy a következő játékos elveheti az előző által pluszban odarakott zsetonokat, ezzel csökkentve a másik játékos plusz zsetonjait. Így előbb vagy utóbb hagyományos Nim-lépést kell végeznie, ezen állásokra pedig igaz az eredeti Nim vesztő- és nyerőállás karakterisztikája.
Általános gyakorlat a kontrollgrafikonok (Levey –Jennings-diagram) használata a vizsgálati eredmények hosszabb idő alatti variációjának szemléltetésére. Ezt grafikus formában a képernyőn kaphatjuk meg. Itt ellenőrizhetjük egy hónap, vagyis 31 nap alatt végzett összes napi vizsgálataink eredményét, az adatok átlaga (MEAN), variációja (range), szórása (SD) és variációs koefficiense (CV%) vizsgálatával.
Első megközelítésben celladekompozíciót (cell decompisition) használunk pályatervezéshez, ami azt jelenti, hogy a szabad teret véges számú folytonos régióra, cellákra osztjuk fel. Fontos tulajdonsága ezeknek a régióknak, hogy a pályatervezés egyszerű módszerekkel megoldható egy-egy cellán belül (pl. egyenes vonalú mozgással). Ezután a pályakeresés diszkrét gráfkeresési problémává egyszerűsödik, nagyon hasonlóvá ahhoz, amit a  fejezetben már bemutattunk.
Az ROC görbével jellemzett teljesítmény természetesen annál jobb, minél nagyobb az ROC görbe alatti terület (Area Under the ROC Curve, AUC). Sok esetben ez használt, mint egy "hasznosságfüggetlen" skalár jellemző, mindazonáltal az AUC értéknek van egy intuitive értelmezése is: megegyezzik azzal a valószínűséggel, hogy a g  függvény jól rendezi sorba egy véletlenszerűen sorsolt "beteg"/"nem beteg" esetpárt, azaz AUC= P(g(xnem−beteg)<g(xbeteg))  .
A konzulens, felügyelő és kritikai programok egyaránt azt a célt szolgálják, hogy segítsék az állapot- és kockázatfelmérést, a diagnózis felállítását, továbbá a legmegfelelőbb megelőzés és terápiás beavatkozás kiválasztását. A rendszerek működését hatékony tudásmenedzselő eszközök biztosítják, melyek képesek feldolgozni a betegségek molekuláris mechanizmusaira vonatkozó különböző szintű ismereteket és az adott páciensre vonatkozó nagyszámú genomikai és klinikai adatot.
- Vegyünk egy olyan állapotteret, ahol minden él súlya ez a bizonyos c negatív konstans, és a gyökérből elérhető n csúcs, ebből n-1 csúcsból elérhető egy újabb csúcs, stb. Az állapottér így egy hárfához hasonlító fa lesz, és ha az ágak 'rossz' sorrendben vannak, akkor mind mélységi, mind szélességi gondolatmenettel a teljes fa bejárása szükséges, hogy a leghosszabb (és így legolcsóbb) utat megtaláljuk! A probléma tehát az, hogy még ha tudunk is egy alsó korlátot mondani az élek költségére, nem tudunk felső korlátot mondani azok számára, és emiatt az
Klinikai igény, -POCT módszer laboratóriumi és klinikai megfelelőssége, - javítja –e a POCT módszer a betegellátás eredményességét, - POCT költséghatékonysága, - kívánatos TAT a központi laborban elérhető e?
Trudel figyelt fel Robert Kowalsky 1971-ben publikált SL-rezolúciós módszerére [10]. 1971 júniusában el is hívta magukhoz Kowalsky-t. Ezen találkozás örökre megváltoztatta mindannyijuk pályafutását. Első ízben volt lehetőségük egy szakértővel beszélgetni az automatikus tétel-bizonyításról és az ehhez kapcsolódó nyelvfelismerésről és analizálásról. Abban az évben még egyszer volt szerencséjük Kowalsky-val találkozni Ekkor ismerték meg Carl Hewitt Planner nevű programozási nyelvét. Az 1970-es években a Planner a mesterséges intelligencia kutatások meghatározó eszköze volt^ [9].
Vezessük be az állapot-történetek X^T={X^T: T → X} halmazát, ahol X^T egy állapot-történetet jelöl, egy függvényt, amely minden időpillanathoz hozzárendel egy-egy környezeti állapotot. Magyarán egy állapot-történt azt mondja meg, hogy a környezet mikor melyik állapotban volt egy adott lefutás során. Ezekhez a lehetséges lefutásokhoz rendel tehát a teljesítménymérték egy-egy valós számot. Vegyük észre, hogy itt nem foglalkozunk azzal, hogy aktuálisan (pl. a lefutás során) mi a jelen, a múlt, vagy a jövő. Ennek oka egyszerű: a bevezetett leírások nem igénylik a „jelen” bevezetését. Az ágensről egy-egy teljes lefutás kapcsán beszélünk most.
A  ábrán egy fekete-fehér képet mutattunk, nagyban figyelmen kívül hagyva azt a tényt, hogy a látható fény a hullámhosszak egész tartományát öleli át – a 400 nm-től kezdve a spektrum ibolyaszínű végénél, egészen a 700 nm-ig a vörös színű végénél. Bizonyos fény csak egyetlen hullámhosszból áll, amely a szivárvány egy színének felel meg. De más fények különböző hullámhosszok keverékei. Azt jelenti ez, hogy az I(x, y) mértékére egyetlen szám helyett az értékek egy keveréke kell? Ha a fény fizikáját pontosan akarnánk reprezentálni, akkor bizony igen. De ha csak utánozni akarjuk az emberek (és más gerincesek) fényérzékelését, akkor köthetünk kompromisszumokat. Kísérletek mutatják (egészen 1801-től, Thomas Youngtól), hogy hullámhosszak bármilyen komplex keverékét is előállíthatjuk mindössze három szín keverésével. Azaz, ha van egy fénygenerátor, amely képes lineárisan kombinálni három hullámhosszt (tipikusan a vörös [700 nm], a zöld [546 nm] és a kék [436 nm] színeket választjuk), akkor az egyes színeket erősítő, másokat gyengítő gombok csavargatásával bármilyen hullámhossz-kombináció beállítható, legalábbis az emberi érzékelés szempontjából. Ez a kísérleti tény azt jelenti, hogy a képek egy olyan vektorral reprezentálhatók, amely képpontokként csak három intenzitásértéket tárol: minden elsődleges hullámhosszra egyet. A gyakorlatban mindegyiket egy bájttal ábrázolva a kép igen jó minőségű reprodukcióját kapjuk. A színek ezen háromszínű észlelése azzal a ténnyel van összefüggésben, hogy háromféle csap található a retinában, amelyek érzékelési maximuma 650, 530 és 430 nm, de az összefüggés pontos részletei az egy az egyben történő leképezésnél bonyolultabbak.
Ez tehát azt jelenti, hogy a Program akkor átlagosan aszimptotikusan korlátozott optimális egy adott Környezet osztályban, ha egy k-szor gyorsabb (vagy nagyobb tárhellyel rendelkező) Architektúrá-n várhatóan nem teljesít rosszabbul, mint bármely másik Program’ program (a korlátozottan optimális programot is beleértve). Ha tehát k nem túlzottan nagy, úgy nem-triviális komplexitású környezetek és architektúrák esetén igencsak örülnénk, ha ilyen programot tudnánk találni (remélve, hogy előbb-utóbb valóban rendelkezésünkre áll majd egy k-szor gyorsabb/nagyobb architektúra, és így ágensünk korlátozottan optimális lesz). Természetesen egy ágens környezet egyszerűsítése is (amennyiben lehetséges) vezethet ahhoz, hogy egy adott architektúrával és programmal rendelkező ágens korlátozottan optimális lesz. A két megközelítés elméletileg ekvivalens, ámde a fenti definíciók azt feltételezik, hogy nekünk, mint az ágens tervezőjének, csak az ágenshez van hozzáférésünk, azon változtathatunk (az architektúráján, vagy a programján), míg a környezet adott.
Ha egy algoritmus minden meglátogatott állapotra emlékszik, akkor közvetlen módon fedezi fel az állapotteret. A Fa-Keresés algoritmust módosíthatjuk úgy, hogy egy zárt listának (closed list) nevezett adatszerkezetet tartalmazzon, amely minden kifejtett csomópontot tárol. (A még ki nem fejtett csomópontokból álló peremet néha nyitott listának [open list] nevezik.) Ha az aktuális állapot egybeesik a zárt listán lévő állapotok egyikével, akkor eldobható, ahelyett hogy a kifejtésével kellene foglalkozni. Az új algoritmus neve Gráf-Keresés ( ábra). Az ismétlődő állapotokat tartalmazó problémák esetén a Gráf-Keresés sokkal hatékonyabb, mint a Fa-Keresés. Az algoritmus a legrosszabb esetre számított tár- és időkomplexitása arányos az állapottér méretével, ami sokkal kisebb lehet, mint O(b^d).
A hiedelmi állapot az egyes háromszög csomópontoknál kiszámítható egy szűrési algoritmusnak a hozzávezető megfigyelések és cselekvések sorozatán való alkalmazásával. Ezzel a módszerrel, az algoritmus figyelembe veszi azt a tényt, hogy egy A[t+i] döntésnél az ágens számára elérhetők lesznek az E[t+1], ..., E[t+i] érzékelések, még ha a t időpontban nem is tudja, hogy ezek az érzékelések mi is lesznek. Ezen a módon a döntéselméleti ágens automatikusan figyelembe veszi az információ értékét és információgyűjtő cselekvéseket hajt végre, ha szükséges.
Kezdjük a 03-assal. Melyik legyen balról az első ház (honnan nézzük, sorszámozzuk a házakat magyarán)? Az 1-es, vagy az 5-ös? ...a legkézenfekvőbb (számunkra, akik a nyugati kultúra gyermekei vagyunk), hogy balról jobbra haladva növeljük a számozást, így balról az első ház az 1-es. Azaz csak annyit kell kimondanunk, hogy az 1-es számú házban a norvég lakik. Ez kétféleképpen tehető meg. Vagy bevezetünk egy egyenlőséget előíró korlátot/korlát-típust, amely egy változónak egy bizonyos értéket, vagy egy másik változót ad kényszerítő erővel értékül (ilyen eddig nem volt), vagy pedig talán még ennél is egyszerűbb, ha az érintett House1Person változó értékkészletét már a probléma modelljének szintjén leszűkítjük a probléma definíciója szerint lehetséges egyetlen elemre/értékre, esetünkben a Norwegian-re. Tehát a 03-as korlát eddigi változókkal való megfogalmazása a következő értékkészlet definíciót vonja maga után:
Az előbbi részben a „hogyan jutunk el Bukarestbe” probléma olyan megfogalmazását javasoltuk, amely a kezdeti állapotból, az állapotátmenet-függvényből, a célállapottesztből és az útköltségből áll. Ez a megfogalmazás értelmesnek tűnik, a valós világ számos aspektusát mégis figyelmen kívül hagyja. Hasonlítsuk csak össze az általunk választott egyszerű állapotleírást, Benn(Arad), egy tényleges országjáró kirándulással, ahol a világ állapota rengeteg mindent tartalmazhat: kivel utazunk, mit közvetít a rádió, milyen tájat látunk az ablakon át, van-e közelben rendőr, milyen messze van a következő pihenőhely, milyen az út állapota, milyen az időjárás és sok-sok más.
Lehetünk-e pontosabbak a megfelelő absztrakciós szint meghatározásában? A megválasztott absztrakt állapotokra és cselekvésekre úgy gondolunk, hogy azok a részletes valós állapotok és cselekvések egész halmazaihoz tartoznak. Most tekintsük az absztrakt probléma egy megoldását: például az Aradról Nagyszebenbe, majd Rimnicu Vilceába, Pites¸tibe és Bukarestbe vezető utat. Ez az absztrakt megoldás rengeteg részletesebb útnak felel meg. Például Nagyszeben és Rimnicu Vilcea között vezethettünk bekapcsolt rádióval, majd az utazás további részére kikapcsolhattuk a rádiót. Az absztrakció érvényes, ha az absztrakt megoldást megoldássá fejthetjük ki egy részletesebb világban is. Elégséges feltétel az, hogy minden olyan részletes állapothoz, mint az „Aradon van”, létezik egy részletes út valamelyik olyan állapothoz, mint a „Nagyszebenben van” stb. Az absztrakció hasznos, ha a megoldásbeli cselekvések végrehajtása az eredeti problémánál egyszerűbb. Ebben az esetben ezek eléggé egyszerűek ahhoz, hogy egy átlagos gépkocsivezető ágens végre tudja azokat hajtani minden további keresés vagy tervkészítés nélkül. Így egy jó absztrakció megválasztása magában foglalja az érvényesség megőrzése mellett a lehető legtöbb részlet törlését, és annak biztosítását, hogy az absztrakt cselekvéseket könnyű legyen véghezvinni. Ha nem lenne meg a hasznos absztrakciók megalkotásának képessége, akkor az intelligens ágensek a valós világban teljesen hasznavehetetlenek lennének.  ábra - Románia egy részének sematikus országúti térképe Románia egy részének sematikus országúti térképe
(Ezt a feladatot Michael Genesereth és Nils Nilsson találták ki, és az első évesektől a végzősökig mindenkinek szól.) Az emberek annyira ügyesek alapfeladatok elvégzésében – mint például csészék felemelése vagy dobozok egymásra rakása –, hogy gyakran elfelejtik milyen összetettek ezek a mozdulatok valójában. A feladat során felfedezzük e feladatok komplexitását, és összefoglaljuk a robotika elmúlt 30 évének fejlődését. Először határozzuk meg a feladatot (például egy kapu készítése három dobozból), majd építsünk egy robotot négy emberből az alábbiak szerint: * Agy: az Agy feladata a cél elérését biztosító terv kidolgozása és a kezek irányítása a terv megvalósítása során. Az Agy információt csak a Szemtől kaphat, közvetlenül nem láthatja a helyszínt! Az Agy az egyetlen, aki tudja, mi a cél. * Szem: a Szem feladata a helyszín rövid leírása az Agy számára. A munkakörnyezettől kb. egy méternyire kell lennie, és vagy kvalitatív leírást (mint például: „Az oldalán fekvő zöld doboz tetején egy piros doboz van”), vagy kvantitatív leírást („A zöld doboz kb. 50 cm távolságra balra van a kék hengertől”) adhat. A Szem az Agytól jövő kérdésekre is válaszolhat, így például: „Van-e hely a Bal Kezem és a piros doboz között?” Ha van kéznél egy videokamera, akkor állítsuk rá a munkakörnyezetre, és a Szem ezen keresztül nézze azt, ne lássa közvetlenül! * Kezek (Jobb és Bal): mindkét Kezet más ember játssza. A két Kéz egymás mellett áll. A Balkéz csak a saját balkezét használja, a Jobbkéz pedig csak a jobbat. A Kezek csak egyszerű, az Agytól érkező feladatokat hajthatnak végre. Például: „A Balkéz mozduljon 5 cm-t előre”. Csak mozgásparancsot képesek végrehajtani – például a „vedd fel a dobozt” nem olyan parancs, amit a Kéz végre tud hajtani. A csalások megakadályozása végett megkövetelhetjük, hogy a kezeken legyen kesztyű vagy használjanak csipeszt. A Kezeknek bekötött szemmel kell tevékenykedniük. Egyetlen érzékelési képességük az, hogy meg tudják mondani, amikor útjukat valamilyen elmozdíthatatlan akadály, például egy asztal vagy a másik Kéz zárja el. Ilyen esetben hangjelzést adhatnak az Agy informálása céljából.
Ebben a részben a Bongard problémák megoldására készített programok működési elvét és eredményeit vizsgálom meg [4]. Az elsőt 1975-ben Bongard egy tanítványa, Maksimov készítette. Ez a program valójában nem Bongard problémákat oldott meg, hanem az azokhoz nagyon hasonló, 48 “Maksimov problémát” (a továbbiakban MP). Bemenetként 45x64 pixel felbontású, fekete-fehér képeket használt. Egy MP-ben tetszőleges számú kép lehet a bal és a jobb oldalon, amikhez egy megkülönbözetető szabályt kell találni, illetve egy új problématípust is létrehozott, ahol a két osztály csak egy-egy mintapéldánnyal adott, és az alattuk lévő többi kép közül kell kiválogatni a kategóriákba illőket. Ha a program mindig visszajelzést kap egy embertől, hogy helyesen választott-e, akkor egy tanulási folyamat valósul meg.
A t  döntési küszöb különböző értékeihez ekkor egy g  esetén, amit a gépi tanulás esetében a véges megfigyelésből tanultunk, különböző érzékenység (sensitivity) és specifikusság (specificity) értékpárok tartoznak. A g  függvény és így a tanulás általános, döntési küszöbtől független jellemzésére szolgál a "Receiver Operator Characteristic" ROC diagram, amely az érzékenységet (sensitivity) ábrázolja az és (1-specifikusság) (1-specificity) függvényében. Ez más megfogalmazásban a lehetséges döntési küszöbökhöz tartozó érzékenységet (sensitivity) és (1-specifikusság) (1-specificity) párokat ábrázolja. Továbbá mivel a döntési küszöb optimális megválasztását a bináris hasznosság/veszteség mátrix határozza meg, így az ROC diagram a lehetséges hasznosság/veszteség mátrixok esetére is jellemzi a g függvényt, azaz esetünkben a tanulás eredményességét.  ábra - Receiver Operating Characteristics (ROC) görbék Receiver Operating Characteristics (ROC) görbék
W. A. Wythoff holland matematikus 1907-ben adta meg a játék nyerő stratégiáját, innen ered a neve. A játékot az ősi Kínában is játszották már. Legfeljebb 7 elemű halmokkal játszva a Wythoff-Nim izomorf a Sarokba a királynőt! nevű játékkal, melyben a cél a királynőt a kezdőhelyéről a bal alsó sarokba juttatni, és csak olyan lépéseket tehetünk, mely közelebb viszi a királynőt a céljához.
Mint ahogy természetesnek tűnik az ⇒ használata összekötőjelnek a ∀ használatakor, az ∧ az a természetes összekötőjel, amit az ∃-vel használunk. Az ∧ használata fő összekötőjelként az ∀-val túlságosan is erős állításokhoz vezetett az előző alfejezetben tárgyalt példában; a ⇒ használata az ∃-vel igazából nagyon gyenge állításhoz vezet:
A Q-tanuló ágens megtanulja a 4 × 3-as világra az optimális stratégiát, de ezt sokkal lassabban teszi, mint az ADP-ágens. Ennek oka, hogy az IK nem kényszerít modellje alapján konzisztenciát az értékekre. Ez az összehasonlítás felvet egy általános kérdést: mi a jobb: egy modellt és egy hasznosságfüggvényt tanulni, vagy egy cselekvésértékfüggvényt? Más szavakkal, mi a legjobb módja egy ágensfüggvény reprezentálásának? Ez a mesterséges intelligencia alapjait érintő kérdés. Mint az  fejezetben kijelentettük, az MI legnagyobb részében kulcsfontosságú történeti örökség a (gyakran ki nem mondott) ragaszkodás a tudásalapú (knowledge-based) megközelítéshez. Ez vezet ahhoz a feltevéshez, hogy az ágensfüggvény reprezentációjának legjobb módja, ha valamilyen szempontból az ágenst körülvevő környezet modelljét építjük meg.  ábra - Egy felfedező Q-tanuló ágens. Egy aktív tanuló, amely megtanulja minden cselekvésnek minden egyes állapotban a Q(a, s) értékét. Ugyanazt az f felfedezési függvényt használja, amelyet a felfedező ADP-ágens is használt, de elkerüli az állapotátmenet-modell tanulását, mivel az állapot Q-értékét közvetlenül a szomszédos állapotok Q-értékeihez tudja kapcsolni. Egy felfedező Q-tanuló ágens. Egy aktív tanuló, amely megtanulja minden cselekvésnek minden egyes állapotban a Q(a, s) értékét. Ugyanazt az f felfedezési függvényt használja, amelyet a felfedező ADP-ágens is használt, de elkerüli az állapotátmenet-modell tanulását, mivel az állapot Q-értékét közvetlenül a szomszédos állapotok Q-értékeihez tudja kapcsolni.
Pontosan konvertálható-e egy véges keresési probléma Markov döntési problémává úgy, hogy az utóbbi optimális megoldása az előzőnek is optimális megoldása legyen? Ha igen, magyarázza el pontosan, hogyan konvertálja a problémát, és hogyan konvertálja a megoldást visszafelé; ha nem, akkor magyarázza el, hogy miért nem (azaz adjon ellenpéldát).
A döntések (keresési irányok) megválasztása több módon alakulhat. Néha fontos tudni visszakerülni egy korábbi döntési helyzetbe (ezt visszalépésnek nevezzük), hogy az utólag rendelkezésre álló információ birtokában jobb döntést hozhassunk. Néha a visszalépés túlságosan költséges, túl sok időt, energiát, anyagiakat igényel és célszerűbbnek tűnik a döntésekben csakis előre rohanni. A tér jellege nagyban befolyásolja, hogy pl. a visszalépés egyáltalán lehetséges, vagy felvállalható-e.
A fuzzy rendszerre jellemző bizonyos mértékű pontatlanság. Ezért az adott probléma során célszerű ellenőrzést végezni. Lehetséges módszer, hogy a rendelkezésre álló mérési adatok egy részével tanítjuk a rendszert, a másik rész segítségével pedig összevetjük a becsült kimeneti változó érték és a mért érték közötti különbséget.
A következő vizsgált él az (NT, Q). Ez az él is inkonzisztens, mivel Q értékkészlete már csak {BLUE}, és ezen belül az NT változó BLUE értékéhez már nincs megfelelő, konzisztens érték. Tehát az NT változó értékkészletéből töröljük a BLUE értéket, azaz NT értékkészletét {GREEN}-re redukáljuk, majd az (NT, Q) élet töröljük az él-listáról, és a lista végére tesszük az összes NT-be mutató élet. Ezzel a következő él-listához jutottunk:
A Radon transzformáció céljában és működésében is nagyban hasonlít a Hough féle módszerre. A transzformáció kimenete egy alakzatra a bemenő kép alakzat menti integrálja. Két paraméterrel leírt egyenesek esetén például teljesen megegyezik az eredmény a Hough transzformáció kimenetével, csak a számítások mögötti elméleti háttér különbözik valamelyest.
Az ORKI információs rendszerével kapcsolatban megjegyezhetjük, hogy az egészségügyi intézmények közvetlenül és díjmentesen kérhetnek és kaphatnak adatot, amellyel elősegítené az egészségügyi szakemberek „naprakész” informáltságát. Természetesen nem –egészségügyi intézmények is kérhetnek információt, azonban ilyen esetekben mindig egyedileg vizsgáljuk meg a kérést, hogy ezáltal elkerülhető legyen a más cégekre vonatkozó bizalmas adatok kiadása.
d) Van három kancsója, egy 12 l-es, egy 8 l-es és egy 3 l-es és egy vízcsapja. A kancsókat megtöltheti, kiöntheti belőlük a vizet a földre, vagy az egyikből áttöltheti a vizet egy másikba. Pontosan 1 l vizet kell kimérnie.
Tegyük fel, hogy kapott egy n darab egyforma pénzérmét tartalmazó zsákot. Azt is tudjuk, a pénzek közül n–1 szabályos, azaz egyik oldalán fej, a másikon írás található, ugyanakkor van egy darab hamis, amelynek mindkét oldala fej. a. Tegyük fel, hogy benyúl a zsákba, majd véletlenszerűen kiemel egy pénzdarabot. Ezt feldobva fejet kap eredményül. Mekkora a (feltételes) valószínűsége annak, hogy a kivett pénzdarab a hamis? b. Tegyük fel, hogy k-szor folytatva a pénz feldobását, ezután is minden alkalommal fejet kap. Most mennyi lesz a feltételes valószínűsége annak, hogy a kivett pénzdarab a hamis? c. Tegyük fel, hogy a k-szori feldobással akarta eldönteni, hogy a hamis vagy egy jó pénzérmét emelt ki a zsákból. A döntési eljárás HAMIS-at ad, ha mind a k feldobás alkalmával a fej van fölül, egyébként pedig JÓ-t. Mekkora annak a (feltétel nélküli) valószínűsége, hogy az eljárás tévedni fog?
A multiágens tervkészítési probléma megoldása egy együttes terv (joint plan), amely minden ágenshez tartalmaz cselekvéseket. Az együttes terv egy megoldás, ha a célt elérjük azáltal, hogy mindegyik ágens végrehajtja a számára kijelölt cselekvéseket. A következő terv a tenisz probléma egy megoldása :
Objektumok: egy, kettő, három, egy meg kettő; Reláció: egyenlő; Függvény: plusz. („Egy plusz kettő a neve annak az objektumnak, amelyet úgy kapunk, hogy a „plusz” függvényt alkalmazzuk az „egy” és „kettő” objektumokra. A három egy másik neve ugyanennek az objektumnak.) * „A wumpus helyével szomszédos négyzetek büdösek.”
A SpeechMagic egy alaptechnológiai rendszer, melyhez a Philips partnerei részére szoftverfejlesztő készletet biztosít. Ennek segítségével illeszthető az egészségügyi szolgáltatóknál már működő szakmai informatikai rendszerhez (HIS-hez), radiológiai informatikai rendszerhez (RIS-hez), digitális képarchiváló rendszerekhez(PACS-hoz) vagy akár egyéni PC-hez. Az illesztést a Philips partner cég, illetve az egészségügyi informatikai rendszert szállító és követő informatikai cég közösen végzi.
Ezeket az állításokat együttesen egyedi cselekvés axiómáknak (unique action axioms) nevezzük. A kezdeti állapotleírás, a követő állapot axiómák, az egyedi elnevezések axiómák és az egyedi cselekvésaxiómák együttese elegendő annak bizonyításához, hogy a javasolt terv megvalósítja a célt.
A hasznosság egy függvény, ami valós számokat rendel az állapotokhoz. Vajon ez minden, amit a hasznosságfüggvényről el lehet mondani? Szigorúan nézve, igen, ennyi. A korábban felsorolt megkötéseknek eleget téve az ágensnek tetszőleges preferenciái lehetnek. Például az ágens preferálhatja, hogy a bankbetétjein elhelyezett dollárok száma prímszám legyen; ekkor, ha 16 dollárja volna, 3-at elajándékozna. Lehet, hogy egy 1973-as ütött-kopott Ford Pintót jobban szeret, mint egy csillogó új Mercedest. A preferenciák kapcsolatban állhatnak egymással; például lehet, hogy csak akkor preferál prímszámú dollárt, ha Pintója van, de amikor Mercedese van, akkor a több dollárt jobban szereti, mint a kevesebbet.
Vegyük észre, hogy a Szülők(Z[i]) tartalmazhat mind rejtett, mind bizonyítékváltozókat is. A P(z) a priori eloszlástól eltérően az S[WS] eloszlás szentel valamennyi figyelmet a bizonyítéknak is: a mintavételezett értékeket minden Z[i] esetén befolyásolják a Z[i] ősei között levő bizonyítékok. Másfelől, S[WS] kevesebb figyelmet szentel a bizonyítékoknak, mint a P(z∣ e) valódi a posteriori eloszlás, mivel a mintavételezett értékek minden Z[i] esetén figyelmen kívül hagyják azokat a bizonyítékokat, amelyek Z[i]-nek nem ősei.^[150]
Az a tény, hogy a potenciálteres módszer ilyen hatékonyan megtalálja az utat a célhoz a konfigurációs térben még nagy távolságok esetén is, megkérdőjelezi a tervezés szükségességét. Vajon csak szerencsénk volt a példánk esetében, vagy tényleg elegendő a potenciálterek használata? A válasz az, hogy szerencsénk volt. A potenciáltereknek számos olyan lokális minimumhelye lehet, amely csapdába ejti a robotot. Ebben az esetben a robot úgy közelíti meg az akadályt, hogy csak vállcsuklóját mozgatja egészen addig, amíg bele nem ütközik az akadályba a rossz oldalon. A potenciáltér nem ad annyi információt, hogy a robot behajlítsa a könyökét, hogy átférjen az akadály alatt. Más szavakkal, a potenciáltéren alapuló technikák nagyon jók lokális robotirányításra, de globális tervezést igényelnek. A másik hátulütője ennek a módszernek az, hogy az erők, amiket generál, csak az akadályoktól és a céltól való távolságtól függnek, nem veszik figyelembe a robot sebességét. Így a potenciáltér-alapú vezérlés tényleg egy kinematikai módszer, és nem feltétlenül működik, ha a robot gyorsan mozog.
Az ostábla tipikusan olyan játék, amely ötvözi a szerencsét és a tudást. A játékosok a lépések előtt kockát dobnak, amivel meghatározzák az adott játékos számára megengedett lépéseket. A  ábra ostáblaállásánál Fehér egy 6-ost és egy 5-öst dobott, és négy lehetséges lépés közül választhat.  ábra - Egy tipikus ostáblaállás. A játék lényege, hogy az egyik fél összes bábuját eltávolítsuk a tábláról. Fehér az óramutató járásával egyező irányban halad a 25-ös felé, míg Fekete az óramutató járásával ellentétes irányban, a 0 felé halad. Egy bábuval bármelyik helyre lehet lépni, kivéve azokat a helyeket, ahol az ellenfélnek már legalább két bábuja található. Ha egy olyan helyre lépünk, ahol az ellenfélnek egy bábuja található, akkor azt a bábut elfogtuk, és a lépkedést az elejéről kell újrakezdenie. Ennél a táblaállásnál Fehér épp most dobott 6–5-öt, és négy megengedett lépés közül választhat: (5–10,5–11), (5–11,19–24), (5–10,10–16) és (5–11,11–16). Egy tipikus ostáblaállás. A játék lényege, hogy az egyik fél összes bábuját eltávolítsuk a tábláról. Fehér az óramutató járásával egyező irányban halad a 25-ös felé, míg Fekete az óramutató járásával ellentétes irányban, a 0 felé halad. Egy bábuval bármelyik helyre lehet lépni, kivéve azokat a helyeket, ahol az ellenfélnek már legalább két bábuja található. Ha egy olyan helyre lépünk, ahol az ellenfélnek egy bábuja található, akkor azt a bábut elfogtuk, és a lépkedést az elejéről kell újrakezdenie. Ennél a táblaállásnál Fehér épp most dobott 6–5-öt, és négy megengedett lépés közül választhat: (5–10,5–11), (5–11,19–24), (5–10,10–16) és (5–11,11–16).  ábra - Egy ostáblaállás sematikus játékfája Egy ostáblaállás sematikus játékfája
A mélységi kereséshez hasonlóan a hegymászó keresésre (hill-climbing search) jellemző a csomópontkifejtés lokalitása. A tény az, hogy mivel a hegymászó keresés a memóriában csak egy aktuális állapotot tart, ez az algoritmus már egy online keresési algoritmus! A legegyszerűbb formájában azonban nemigen használatos, mert az ágenst a lokális maximumokban bennrekedve hagyja, ahonnan nincs hová mennie. Ráadásul a véletlen újraindítást sem lehet használni, mert az ágens nem képes magát egy új állapotba áthelyezni.
A következő, (WA, NT) él konzisztens, hiszen WA egyetlen GREEN értékéhez NT mindkét (RED és BLUE) értéke megfelelő. Tehát ez az él törlődik a listáról, és következik az (SA, NT) él is konzisztens, hiszen SA egyetlen BLUE értékéhez ott van az NT változó RED értéke, ami konzisztens vele. Ezért tehát az (SA, NT) él is törlődik a listáról.
Az AgentGame az 1.10-es verziótól felfelé JFreeChart komponenseket használ a grafikonok megjelenítéséhez. Az ezekhez szükséges .két jar fájl az AgentGame-et tartalmazó tömörített fájl \install könyvtárában található. Helyezzük őket az előbbi két lépésben meghatározott \lib és \lib\ext könyvtárakba!
Mindegyik játékos tudja, hogy az összes többi játékos is tudja, ismeri ezt a játékot, tisztában van a szabályokkal, az egyes szereplők preferenciáival (hogy melyik kimenetelt mennyire preferálják), illetve stratégiai lehetőségeivel. Azaz a játék kölcsönös tudás a játékosok számára. Mindenki ismeri, és tudja, hogy a többiek is ismerik, és a többiek ezt tudják, és ezt is, és ezt is, a végtelenségig. Röviden: mindenki tudja, hogy mindenki tudja, hogy...ismeri a játékot.
A látás azon alapul, hogy az igen rövid hullámhosszúságú (380-780 nm) tartományba eső elektromágneses sugárzás a szemünkben fényérzetet kelt. A sugárzási energia hatására a szem retinájának idegvégződései működésbe lépnek, fizikai-kémiai folyamatok indulnak be, és az idegek segítségével az agy megfelelő központjába továbbítódnak ingerület formájában.
Megfigyelhető, hogy a hálóban nincsenek olyan csomópontok, amelyek azt írnák le, hogy Mária éppen hangos zenét hallgat, vagy hogy a telefon cseng, és megzavarja Jánost. Ezeket a tényezőket a Riasztás csomóponttól a JánosTelefonál és MáriaTelefonál csomópontig tartó élekhez rendelt bizonytalanság foglalja magában. Ebben mind a lustaság, mind pedig a tudatlanság tetten érhető: rengeteg munka volna olyan tényezők meghatározása, amelyek valószínűbbé vagy kevésbé valószínűvé válnának adott esetekben, és nincs semmilyen ésszerű módszerünk, hogy ezekre vonatkozóan érdemi információt szerezzünk. A valószínűségek valójában a lehetséges körülmények egy potenciálisan végtelen halmazát összegzik, amikor is a riasztó elmulaszt megszólalni (magas páratartalom, elektromos hálózat hibája, lemerült elem, elvágott drót, döglött egér beragadva a csengőbe…), vagy, hogy János és Mária elmulaszt értesíteni minket (ebédelni mentek, szabadságon vannak, időlegesen megsüketültek, egy áthaladó helikopter…). Ezen a módon egy kis ágens is képes megbirkózni egy igen bonyolult világgal, legalábbis közelítőleg. A közelítés mértéke tetszőlegesen javítható további releváns információk bevezetésével.
A verziótér algoritmus tiszta változatát először a Meta-Dendral rendszerben alkalmazták. Ezt a rendszert olyan szabályok megtanulására tervezték, amelyek azt írják le, hogy egy tömegspektrométerben a molekulák hogyan esnek szét darabokra (Buchanan és Mitchell, 1978). A Meta-Dendral képes volt olyan szabályok generálására, amelyek elég újak voltak egy analitikus kémiával foglalkozó újságban való publikálhatósághoz. Ezek voltak az első – egy számítógépes program által felfedezett – valós tudományos eredmények. Az algoritmust az elegáns Lex rendszerben (Mitchell és társai, 1983) szintén használták, ez a rendszer saját sikereinek és kudarcainak tanulmányozása alapján szimbolikus integrálási problémák megoldásának megtanulására volt képes. Bár a verziótér-módszerek valószínűleg a legtöbb valós probléma tanulása esetén – főleg a zaj miatt – nem praktikusak, jó betekintést adnak a hipotézistér logikai szerkezetébe.
Az adatbázisok az IBM PC-ken legelterjedtebben használt dBase állományok formájában állnak rendelkezésre. Az adatállományokat kezelő felhasználói rendszerek legnagyobb hányadát az ORKI munkatársai készítették. A számítógéppark adottságai és a nagy adattömeg kezdettől a leggyorsabb működésű szoftvereszköz alkalmazására kényszerítették a programok készítőit. A programok alapvetően kétféle céllal és ezeknek megfelelő változatban készültek és készülnek, ezáltal is biztosítva az adatvédelmet, illetve az „egyszemélyi” felelősséget: * az adatállományok naprakészen-tartására – nem teljes körű jogosultsággal * a különféle jellegű információszolgáltatások – általános jogosultsággal.
Azonban Turing szerint Jefferson hajlandó lenne ezt az udvarias feltételezést a gépekre is kiterjeszteni, ha találkozna intelligensen cselekvő gépekkel. Az alábbi dialógust idézi, amely annyira beépült a mesterséges intelligencia szájhagyományába, hogy egyszerűen nem hagyhattuk ki:
Először is a kiértékelő függvénynek a végállapotokat ugyanúgy kellene sorba rendeznie, mint az igazi hasznosságfüggvénynek, máskülönben az azt használó ágens még akkor is szuboptimális lépéseket választhatna, ha történetesen a játék végéig előre látna mindent. Másodsorban a kiértékelő függvény kiértékelése nem tarthat túl sokáig! (A kiértékelő függvény szubrutinként meg tudná hívni a Minimax-Döntés-t, és ki tudná számítani a pozíció pontos értékét, ez azonban az egész gyakorlat értelmét – az időmegtakarítást – megkérdőjelezné.) Harmadsorban a nem végállapotok tekintetében a kiértékelő függvénynek pontosan kell tükröznie a nyerés valódi esélyét.
Az itt bemutatott heurisztika mind a progresszív (előrefelé), mind pedig a regresszív (hátrafelé) irányba használható. A könyv írásának pillanatában az üres-törlési-lista heurisztikát használó előrefelé keresők a listavezetők. Ez valószínűleg változik, ahogy újabb heurisztikák és keresési technikák jelennek meg. Mivel a tervkészítés exponenciális komplexitású,^[115] nincs olyan algoritmus, amely minden problémára hatékony lesz, de számos gyakorlati feladat oldható meg ezen fejezet heurisztikáival – sokkal több, mint ami néhány éve megoldható volt.
Tételezzük fel, hogy látjuk a locsolót bekapcsolva. Előreláncolással a szabályainkon keresztül ez megnöveli azt a bizonyosságot, hogy a pázsit vizes, ami viszont megnöveli az esőhöz tartozó bizonyosságot. De ez képtelenség: a locsoló bekapcsolt állapota megmagyarázza a pázsit vizességét, így csökkentenie kell az esőhöz tartozó bizonyosságot. Egy igazságfüggvényen alapuló rendszer úgy viselkedik, mintha a Locsoló↦Eső szabályban is hinne.
A valószínűség objektivista és szubjektivista értelmezése közötti vita a 20. században kiélesedett. A relatív gyakoriság szószólói Kolmogorov (Kolmogorov, 1963), R. A. Fisher (Fisher, 1922) és Richard von Mises (von Mises, 1928) voltak. Karl Popper (Popper, 1959; első megjelenés németül 1934) hajlam (propensity) értelmezése a mögöttes fizikai szimmetriát látja a relatív gyakoriságban. Frank Ramsey, Bruno de Finetti, R. T. Cox, Leonard Savage és Richard Jeffrey úgy értelmezték a valószínűséget, mint meghatározott egyének meggyőződésének mértékét (Ramsey, 1931; de Finetti, 1937; Cox, 1946; Savage, 1954; Jeffrey, 1983). A hiedelem mértékére vonatkozó elemzésük szorosan kötődött a hasznossághoz és a viselkedéshez, különösen ahhoz a szándékhoz, hogy fogadásokat kössünk. Rudolf Carnap, Leibnizet és Laplace-t követve, a szubjektív valószínűség egy másfajta értelmezését javasolta: nem mint az adott egyén meggyőződését, hanem mint azt a hiedelemfokot, amelyet egy eszményi személynek kell hinnie egy adott a állításról e tény fennállása esetén. Carnap megpróbált Leibniz-nél és Laplace-nál tovább menni, és ezt a megerősítési (confirmation) mérték fogalmat, mint a és e között fennálló logikai kapcsolatot, matematikailag is pontosan leírni. A kapcsolat tanulmányozása révén akarta a megszokott deduktív logika (Carnap, 1948; 1950) mintájára, a matematika egy induktív logikának (inductive logic) nevezett diszciplináját megalapítani. Carnap képtelen volt az induktív logikát lényegesen kiterjeszteni az ítéletkalkuluson túlra, és Putnam mutatta meg, hogy alapvető nehézségek hiúsítják meg az aritmetikát is kifejezni képes nyelvek irányába történő szabatos kiterjesztést (Putnam, 1963).
Mivel a kapott {NSW=RED, WA=GREEN, NT=RED, Q=GREEN, SA=BLUE, V=GREEN} behelyettesítés is konzisztens, ezért tovább léphetünk az előretekintésre. Az előretekintés során azonban nincs olyan szomszédja V-nek, nincs olyan változó, amely V-hez kapcsolódna, és még ne lenne behelyettesítve, így hát az előbbiekhez hasonlóan a KÖVETKEZTETÉS eljárás üres értékkel tér vissza (nincs teendőnk szerinte).
Az ennek megfelelő egzisztenciális példányosítási (Existential Instantiation) szabály az egzisztenciális kvantorra egy kicsivel komplikáltabb. Minden α mondatra, v változóra és olyan k konstans szimbólumra, amely sehol máshol nem jelenik meg a tudásbázisban:
Adaptálja a  fejezetben bemutatott porszívóvilágot megerősítéses tanulásra! Az ágens kapjon jutalmat minden felszívott szemétért, a helyére való visszaéréskor és kikapcsoláskor! Megfelelő észlelésekkel tegye megfigyelhetővé a világot! Kísérletezzen különböző megerősítéses tanulást végző ágensekkel! Szükség van-e függvényapproximációra a siker érdekében? Milyen approximátor működik erre az alkalmazásra?
Kevert stratégiakombinációkra is ugyanúgy kimondhatjuk a Nash-egyensúlyi kritériumot, mint tiszta stratégiakombinációkra. ...és itt talán már jobban is látszik, hogy a Nash-egyensúlyi stratégiák valójában rendre legjobb válaszok a többiek Nash-egyensúlyi stratégiakombinációjára.
Ez a példa azt mutatja, hogy a racionalitás nem azonos a tökéletességgel. A racionalitás az elvárt teljesítményt maximalizálja, míg a tökéletesség a tényleges teljesítményt. Az, hogy elállunk az ágensekkel szemben támasztott tökéletesség követelményétől, nem csak a velük szembeni méltányosság kérdése. Az a helyzet, hogy ha azt a cselekvést várjuk el egy ágenstől, ami a megtörténte után a legjobbnak bizonyul, akkor lehetetlen lesz egy ilyen ágens megalkotása – hacsak nem javítjuk a kristálygömbök vagy az időgépek teljesítményét.
Abból a tényből, hogy nem volt se bűz, se szellő az [1, 1]-ben, az ágens kikövetkeztetheti, hogy az [1, 2] és [2, 1] négyzetek veszélytelenek. Ennek jelzésére a megfelelő négyzetekbe OK-t írunk. Egy óvatos ágens csak olyan négyzetbe lép, amelyről tudja, hogy OK. Feltételezzük, hogy az ágens úgy dönt, hogy a [2, 1]-be megy, előállítva a  (b) ábrán látható helyzetet.
Ha a tárgy a kamrától vett távolságához képest viszonylag lapos, a perspektivikus vetítést a skálázott ortografikus vetítéssel (scaled orthographic projection) közelíthetjük. Ennek elve a következő. Ha a tárgy pontjainak Z mélysége egy bizonyos Z[0 ]± ∆Z tartományban változik, és ∆Z = Z[0], akkor a perspektíva f/Z skála tényezőjét az s = f/Z[0] állandóval lehet közelíteni. Az (X, Y, Z) jelenet koordinátákból a képsíkra történő vetítés egyenletei az x = sX és az y = sY lesznek. Jegyezzük meg, hogy a skálázott függőleges síkú vetítés egy közelítés, amely a jelenet azon részeire érvényes, amelyeknek a saját mélységváltozásuk elenyésző. A globális tulajdonságok tanulmányozására ezt a módszert nem szabad használni. Hogy az óvatosság nem árt, erre egy példa: függőleges síkú vetítésben a párhuzamos vonalak párhuzamosak maradnak ahelyett, hogy egy távlatpontban összefutnának.
Ha minden változó bináris, akkor f[1]-nek és f[2]-nek 2^j+k és 2^k+l eleme van, pontonkénti szorzatuknak pedig 2^j + k + l. Például az f[1](A,B) és az f[2](B,C) tényezők esetén a lentebb mutatott valószínűség-eloszlással, az f[1] × f[2] pontonkénti szorzata f[3](A, B, C)-vel adott:
Nézzünk most módszereket a stratégia javítására. Kezdjük a legegyszerűbb esettel: determinisztikus stratégia, determinisztikus környezetben. Ebben az esetben a stratégia értékelése triviális: egyszerűen végrehajtjuk a stratégiát, és megfigyeljük az összegyűjtött jutalmat, ez adja számunkra a stratégia értékét (policy value), ρ(θ)-t. A stratégia javítása ezek után egy standard optimalizációs probléma, ilyeneket a  fejezetben tárgyaltunk. Követhetjük a stratégiagradiens (policy gradient) vektort, ∇[θ] ρ(θ)-t, feltéve, hogy ρ(θ) differenciálható. Másik lehetőség, ha az empirikus gradienst (empirical gradient) követjük hegymászó módszerrel – azaz kiértékeljük a stratégiát minden egyes paraméter kis megváltozása esetére. A szokásos feltételek esetén ez a stratégiatér egy lokális optimumához fog konvergálni.
Az értékadás eredményeképpen az {NSW=RED, WA=GREEN, NT=RED} behelyettesítést kapjuk, ami konzisztens, ezért következhet az előretekintés. Ennek során az eddigieknek megfelelően a Q és SA változók értékkészletéből kellene kivennünk a RED értéket, de mivel a RED már egyikben sincs benne (még a kezdeti NSW=RED értékadás miatt), ezért valójában nem teszünk semmit, nincsenek következtetések, avagy üres következtetésekkel tér vissza a KÖVETKEZTETÉS eljárás.
 Megjegyzés Implementálja a véletlen csomópontokat tartalmazó játékfák lenyesésére alkalmas várhatóminimax és ^*-alfa-béta algoritmust, amit Ballard (Ballard, 1983) ír le. Próbálja ki azokat az algoritmusokat olyan játékokon, mint az ostábla, és mérje meg a ^*-alfa-béta nyesési hatékonyságát.
ahol ρ és η két eloszlás Ω valószínűségi tér felett. Fontos kiemelni, hogy esetünkben a két eloszlás nem felcserélhető, tehát ρ mindig az eredeti eloszlást, míg η az approximációt jelöli. Esetünkben az „igazi” eloszlás helyére kerül a teljes jegyhalmazt tartalmazó P(C | F = f), míg értelemszerűen a szelektálás utáni eloszlás lesz a közelítés: η = P(C | G = g). Az Ω valószínűségi teret pedig a lehetséges osztályozási csoportok alkotják.
Elképzelhetjük táblázatos formában az ágensfüggvényt, amely forma mindenféle ágenst leír; az ágensek többségére ez igen nagy táblázatot jelentene – valójában végtelen nagyot, hacsak nem korlátozzuk a figyelembe veendő érzékelési sorozatok hosszát. Ha adott egy ágens, amivel kísérletezhetünk, akkor elméletileg megalkothatjuk ezt a táblázatot az összes lehetséges érzékelési sorozat kipróbálásával és az ágens válaszul végrehajtott cselekvéseinek feljegyzésével.^[19] Természetesen a táblázat az ágens külső jellemzése. Egy mesterséges ágens belsejében az ágensfüggvényt egy ágensprogram (agent program) valósítja meg. Fontos, hogy megkülönböztessük e két dolgot. Az ágensfüggvény egy absztrakt matematikai leírás, az ágensprogram egy konkrét implementáció, amely az ágens architektúráján működik.
A Tárol és a Betölt megvalósításának legegyszerűbb módja, hogy a tudásbázisban az összes tényt egyetlen hosszú listán tároljuk, és egy q lekérdezés megválaszolásakor meghívjuk az Egyesít (q, s)-t minden s mondatra a listán. Ez az eljárás nem hatékony, de működik, és egyelőre elég a fejezet további részének megértéséhez. Az alfejezet hátralévő része áttekinti azokat az eljárásokat, amelyekkel a visszakeresést hatékonyabbá tehetjük, így ezt első olvasásra átugorhatjuk.
Itt viszont érdemesebb egyszerre csak betűket beírni, azaz betűket választani változónak, mert így lényegesen egyszerűbb a kényszerek megfogalmazása: tulajdonképpen a betűk felelnek meg a változóknak, a szavak pedig a kényszereknek. Vezessük be az üres karaktert, és kezdetben legyen minden mezőn üres karakter! A kényszerek így n-változósak lesznek, ahol n az adott szó hossza, és akkor teljesül egy kényszer, ha létezik olyan szó, amelynek a megfelelő betűi éppen az adott változók értékei, vagy a változó értéke az üres karakter. Például egy négyváltozós kényszert kielégít a [v lt] betű-négyes, mert létezik olyan szó, amelynek a megfelelő betűi megegyeznek a betű-négyes betűinek, de az [aa a] betűnégyes nem elégíti ki, mert nincs olyan szó, aminek az első két betűje a.
Az attribútumalapú információkinyerő rendszerek reguláris kifejezések sorozatából építhetők fel, ahol minden egyes attribútumhoz egy reguláris kifejezés tartozik. Ha a reguláris kifejezés pontosan egyszer illeszkedik a szövegre, akkor kivehetjük a szöveg illeszkedő részét, amely az attribútum értéke lesz. Ha nincs illeszkedés, akkor nem tudunk mit tenni, azonban ha több illeszkedés is van, akkor szükségünk van egy eljárásra, hogy hogyan válasszunk közülük. Az egyik megoldás szerint minden egyes attribútumhoz több reguláris kifejezést rendelünk, prioritás szerint sorba rendezve. Például a legnagyobb prioritású árra vonatkozó reguláris kifejezés a dollárjel előtti „ár:” karakterfüzérre kereshet, ha ezt nem találja meg, akkor átlépünk egy kevésbé megbízható reguláris kifejezésre. Egy másik stratégia az összes találat kinyerése, majd valamilyen módon választani közülük. Például vehetjük azt a legalacsonyabb árat, amely a legnagyobbnak legalább fele. Ez a megoldás kezelni tudja az olyan szövegeket, mint a „Listaár 99,00 $, akciós ár 78,00 $, szállítás 3,00 $”.
Az algoritmus működésének demonstrálására Freund egy alkalmazást is készített [7]. A felhasználó a mintapontokat egér bal és jobb gombjának segítségével helyezheti el. Ezután a felvett adathalmazt tanítóhalmazra és teszthalmazra oszthatjuk fel. Az alkalmazás az algoritmus lépései során grafikonon ábrázolja a tanító és a tesztadathalmazon az AdaBoost hibáját és annak elméleti határát, valamint az adott lépésben meghatározott „gyenge” hipotézis hibáját. Az AdaBoost működését egy MATLAB programkód [8] segítségével is megvizsgálhatjuk, valamint lehetőséget biztosít különböző gyenge osztályozó eljárások kipróbálására is.
Sajnos a kibontás naiv alkalmazása nem volna valami hatékony. Ha szűrést vagy simítást szeretnénk elvégezni megfigyelések e[1:t] hosszú sorozatán, akkor a kibontott háló tárigénye O(t) lenne, és korlát nélkül nőne, amint még több megfigyelést adnánk hozzá. Ráadásul ha egyszerűen újra lefuttatjuk a következtető algoritmust minden időpontban, amikor megfigyelés érkezik, a következtetési ideje frissítésenként szintén O(t) szerint fog növekedni.  ábra - Egy dinamikus Bayes-háló kibontása: a szeletek ismétlődnek befogadva a megfigyelési sorozatot (a halvány csomópontok). További szeleteknek már nincs hatása a megfigyelési perióduson belüli következtetésre. Ha a DBH-t kibontottuk, bármely következtetési algoritmus használható – változó eliminálás, egyesítési fa módszerek és így tovább –, ami a  fejezetben szerepel. Egy dinamikus Bayes-háló kibontása: a szeletek ismétlődnek befogadva a megfigyelési sorozatot (a halvány csomópontok). További szeleteknek már nincs hatása a megfigyelési perióduson belüli következtetésre. Ha a DBH-t kibontottuk, bármely következtetési algoritmus használható – változó eliminálás, egyesítési fa módszerek és így tovább –, ami a  fejezetben szerepel.
A logika jelentős pedagógiai előnye, hogy egy tudásbázisú ágens egyszerű reprezentációs formáját jelenti, viszont van néhány komoly korlátja is. A helyzet az, hogy az emberek vagy más ágensek által részlegesen megfigyelhető környezetekben végzett következtetések jelentős része függ bizonytalan tudás felhasználásától. A logika nem tudja ezt a bizonytalanságot jól reprezentálni, ezért az V. részben a valószínűséget tárgyaljuk, amely már képes erre. A VI. és VII. részben számos reprezentációt tárgyalunk, köztük néhány folytonos matematikán alapulót, mint a Gauss-görbék keverését, neurális hálózatokat és más reprezentációkat.
Az érintkezéses szenzorok geometriai és fizikai jellemzőkről szolgáltatnak információt. Alkalmazási helyeik robotokon: * A környezettel való kapcsolattartásban erő, nyomaték, út (helyzet) érzékelése, * A megfogó szerkezetben erő és elmozdulás lehatárolása, * A karokon erő, nyomaték, elmozdulás értékének meghatározása, * A hajtórendszerben erő, nyomaték, nyomás, áramerősség és feszültség mérése.
A Nemzeti Jégkorong Szövetségben (National Hockey League) 1999 előtt a csapatok 2 pontot kaptak a győzelemért, 1-et a döntetlenért és 0-t, ha veszítettek. Ez zérusösszegű játék? 1999-ben a szabályokat úgy módosították, hogy egy csapat 1 pontot kap, ha hosszabbításban kap ki. A győztes csapat továbbra is 2 pontot kap. Hogyan változtatja meg ez a módosítás a fenti kérdésekre a válaszokat? Ha törvényes lenne, mikor volna racionális a két csapatnak titokban megegyezni, hogy a rendes játékidőben döntetlent érnek el, és majd a hosszabbításban döntik el a küzdelmet? Tegyük fel, hogy mindegyik csapat számára a kapott pontszámok jelentik a hasznosságot, és hogy létezik egy kölcsönösen ismert p a priori valószínűség, hogy az első csapat hosszabbításban fog nyerni. Milyen p értékekre értene egyet mindkét csapat ezzel az egyezséggel?
John McCarthy Dartmouth-ból átment az MIT-re, és ott egyetlen év alatt, a történelminek nevezhető 1958-as évben, három kulcsfontosságú eredményt ért el. Az 1. számú MIT AI Lab Memóban definiálta a Lispet, amely elsődleges MI-programozási nyelvvé nőtte ki magát. A Lisp a második legrégebbi nyelv, amely még használatban van, a Fortrannál csak egy évvel fiatalabb. A Lisp esetén McCarthy rendelkezett már a szükséges eszközzel, de a ritka és drága számítógépes erőforrásokhoz való hozzáférés számára is komoly problémát jelentett. Így aztán az MIT-n McCarthy és mások kitalálták az időosztást. Szintén 1958-ban McCarthy Programs with Common Sense címen cikket publikált, amelyben az Advice Takert írta le. Ez egy hipotetikus program, amit az első teljes MI-rendszernek tekinthetünk. A Logic Theoristhez és a Geometry Theorem Proverhez hasonlóan McCarthy programja is tudást használt fel egy probléma megoldásának megtalálásához. Azonban másokkal ellentétben, ennek a programnak a világra vonatkozó általános tudással kellett rendelkeznie. McCarthy megmutatta például, hogy néhány egyszerű axióma elegendő ahhoz, hogy programja képes legyen terveket generálni arra vonatkozóan, hogyan kell a repülőtérre kimenni ahhoz, hogy a repülőgépet le ne késsük. A programot úgy tervezte, hogy képes legyen normális működés közben új axiómákat is elfogadni, és ennek eredményeként átprogramozás nélkül új területeken is kompetenciát mutatni. Az Advice Taker ily módon a tudásreprezentáció és a következtetés leglényegesebb elveit testesítette meg, miszerint hasznos, ha rendelkezünk a világot és az ágens cselekvéseinek eredményét leíró explicit és formális reprezentációval, és képesek vagyunk ezt a reprezentációt deduktív módon manipulálni. Figyelemre méltó, hogy 35 év múltával még mennyire releváns maradt az 1958-as cikk.
A statisztikai gépi fordítás Weaver 1947-es megjegyzéséig nyúlik vissza, azonban csak az 1980-as évekre jutott el gyakorlati szintre. Az általunk bemutatottak Brown és kollégáinak IBM-beli munkáin alapulnak (Brown és társai, 1988; 1993). Nagyon komoly matematikát alkalmaznak, ezért a Kevin Knight által írt kísérő oktatóanyag sokat segít a megértésben (Knight, 1999). A még frissebb statisztikai gépi fordítás területén végzett munkák továbblépnek a bigram modellen, és olyan modelleket használnak, amelyek valamennyi szintaxist is figyelembe vesznek (Yamada és Knight, 2001). A mondatszegmentáció terén Palmer és Hearst végezték a korai munkákat (Palmer és Hearst, 1994). Michel és Plamondon a kétnyelvű mondatillesztésről értekezik (Michel és Plamondon, 1996).
él szerint egy NP lefedi a 0-tól 2-ig terjedő karaktersorozatot (az első két szó), és ha tudunk találni egy ezt követő VP-t, akkor lenne egy S-ünk. Az ilyen élek, ahol a pont a végük előtt van, úgynevezett befejezetlen (nem teljes) élek, és azt mondjuk, hogy az él egy VP-t vár.  ábra - A „The agent feels a breeze” mondat diagramjának egy részlete. Mind a hat csomópont látható, de csak három él szerepel azok közül, melyek teljes elemzést eredményeznének. A „The agent feels a breeze” mondat diagramjának egy részlete. Mind a hat csomópont látható, de csak három él szerepel azok közül, melyek teljes elemzést eredményeznének.
A HELP alkalmazások: * ADT * rendelés/Charge Capture * gyógyszertár * klinikai labor * radiológia * vérgáz * tüdő funkciók * szívkatéterezés * kardiológiai / EKG * haemodinamika / EKG Monitoring * orvosi feljegyzések * felülvizsgálat * kutatás
Ez azonban helytelen, mert a „p ⇒ q” füzér tartalmazza ugyan a „p” és „q” betűket, de semmi köze az olyan füzérekhez, amelyekben a p és q változók értékei szerepelnek. A helyes megfogalmazás az alábbi:
Mivel tehát mindhárom említett él konzisztens volt, ezért ezek az algoritmus szerint sorra le is kerülnek a listáról, a lista kiürül, és a következtetés hibajelzés nélkül visszatér a visszalépéses kereséshez, ahol a rekurzív hívás előtt még átvezetésre kerülnek az előbbi következtetések (konkrétan az NT változó értékkészlet-redukciója). Így lényegében a következő ábrán látható állapot adódik.
Az egyszerűség kedvéért a továbbiakban fel fogjuk tenni, hogy a változóknak ugyanaz a részhalmaza minden időpontban megfigyelhető (bár az elkövetkezőkben ez teljes mértékben sehol sem szükségszerű). A t időpillanatban nem megfigyelhető változók halmazának a jelölésére X[t]-t fogjuk használni, és E[t]-t a megfigyelhető változók halmazának a jelölésére. A t időpontbeli megfigyelés E[t] = e[t]az értékek valamely e[t ]halmazára.
A házépítés egy szép, konkrét példa, ezért ezt használjuk fel a cselekvésdekompozíció bemutatására. A  ábra a HázatÉpít cselekvés egy lehetséges dekompozícióját mutatja négy alacsonyabb szintű cselekvésre. A  ábra néhány cselekvés leírását tartalmazza erre a feladatkörre, valamint a házépítés dekompozícióját, ahogyan az a tervkönyvtárban megjelenne. A könyvtárban más lehetséges dekompozíciók is szerepelhetnek.
Amíg tehát a tökéletes racionalitás mintegy a környezet viszonylatában, abszolút értelemben definiálja a racionális cselekvéseket, addig a korlátozott optimalitás már az ágens adott környezetben vett lehetőségeit, képességeit is figyelembe veszi (relatív). Épp ezért ez utóbbi racionalitás definíció, mint elvárás, már sokkal reálisabb, megvalósíthatóbb, mint az előbbi.
A  ábrán a szkeletonizációra látható egy példa: a szabad tér Voronoi-gráfja (Voronoi graph) azokból a pontokból áll, amelyek kettő vagy több akadálytól egyenlő távolságra vannak. Ahhoz, hogy a pályatervezéshez a Voronoi-gráfot lehessen használni, a robotnak először is meg kell változtatnia aktuális konfigurációját úgy, hogy rajta legyen a gráfon. Könnyen belátható, hogy a konfigurációs térben ez mindig megoldható egyenes vonalú mozgással. Ezután a robot a gráf mentén halad mindaddig, amíg el nem éri a célkonfigurációhoz legközelebb eső pontot. Ekkor elhagyja a Voronoi-gráfot, és elmegy a célig. A konfigurációs térben ez egyenes vonalú mozgással ismét megvalósítható.  ábra - (a) A Voronoi-diagram azon pontok halmaza a konfigurációs térben, amelyek egyenlő távolságra vannak két vagy több akadálytól. (b) A szabad tér 400 véletlenszerűen kiválasztott pontjából felépített valószínűségi úthálózat. (a) A Voronoi-diagram azon pontok halmaza a konfigurációs térben, amelyek egyenlő távolságra vannak két vagy több akadálytól. (b) A szabad tér 400 véletlenszerűen kiválasztott pontjából felépített valószínűségi úthálózat.
Nézzük a további paramétereket, mint pl. θ[I1]-et. A teljesen megfigyelhető esetben ezt közvetlenül a megfigyelt, 1. zsákból származó meggy- és citromízű cukorkák számából becsülnénk. Az 1. zsákból származó meggycukorkák várható száma:
A boosting tanuló algoritmus gyorsan népszerűvé vált a gépi tanulással foglalkozó kutatók körében. Ez a téma lassan a statisztikai kutatások főirányává vált [4]. A gépi tanulás elméletével foglalkozó kutatók létre kívántak hozni egy „gyengébb” osztályozó eljárások becslési eredményeit egybevonó „erősebb” osztályozó eljárást (Schapire 1990, Freund 1995). A munkájuk 1997-ben teljesedett ki, amikor Freund és Schapire publikálta az AdaBoost algoritmust. Ezt az algoritmust az irodalomban gyakran nevezik még diszkrét AdaBoost tanuló algoritmusnak is [4]. (A diszkrét elnevezés abban gyökerezik, hogy az algoritmus első változata bináris célváltozó (nyer, nem nyer) értékét tudta becsülni.)
A fenti extenzív formájú játékot (fa), és alatta a normál-formájú megfelelőjét (táblázat) 1-lapos pókernek nevezik. A 1-lapos póker a valódi póker egy kifejezetten leegyszerűsített változata, azonban amíg a valódi pókerjáték extenzív alakja áttekinthetetlenül nagy, addig ezen a kis játékon gyakorlatilag mindent be lehet mutatni, ami az extenzív alakkal kapcsolatban lényeges.
Ezeknek a gépeknek a felhasználó felé mutatott arca lehet egy weboldal, ezen keresztül böngészheti a távoli felhasználó azt, amit a gép tulajdonosa meg akar a világnak mutatni. Kezdetben ezek a weboldalak statikusas voltak, csak HTML-ben (Hyper Text Markup Language) kódolva. Már ilyenkor is léteztek webes enciklopédiák, könyvtárak.(Magyar Elektronikus Könyvtár[3] például). A kereslet az ilyen oldalak iránt szükségesen maga után vonta a különböző szolgáltatások megszületését. A statikus oldalak dinamikusabbá váltak, kommunikációt folytatnak a böngésző felhasználóval, a felhasználó igénye szerint jelennek meg. Ez a web 2.0[4] egyik nagy előnye, a könnyebb, változatos tartalomszolgáltatás és saját tartalom generálása. Mindennapi életből hozhatunk példákat: Facebook, különböző blogok, wikipediák, moodle rendszerek és még sorolhatnánk.
A gyíksütés esetén az ősemberek úgy általánosítottak, hogy megmagyarázták a nyárs sikerét: a nyárs megtartja a gyíkot a tűz felett, miközben a kéz sértetlen marad. Ebből a magyarázatból azt az általános szabályt vonhatják le, hogy egy hosszú, merev, hegyes objektum alkalmas a kis, puha testű, ehető dolgok sütéséhez. Az ilyen általánosító folyamatot magyarázatalapú tanulásnak vagy MAT-nak (explanation-based learning, EBL) nevezzük. Figyeljük meg, hogy az általános szabály az ősemberek birtokában lévő háttértudásból logikai módon következik. A MAT által teljesített vonzatkényszerek tehát a követezők:
Az És-Vagy-Gráf-Keresés által visszaadott tervek feltételes lépéseket tartalmaznak, melyek a teljes állapotleírást megvizsgálják, hogy egy ágról döntsenek. A legtöbb esetben ennél jóval kevésbé kimerítő ellenőrzésekkel is megúszhatjuk. Például a  ábrán látható megoldás a [Balra, if TisztaBal then [] else Szív] megadással egyszerűen leírható. Ennek oka, hogy a TisztaBal teszt elegendő, hogy az és csomópont állapotait két egyelemű halmazba sorolja úgy, hogy a tesztek után az ágens pontosan ismerje az állapotát. Valójában az egyváltozós if-then-else tesztek sorozata mindig elegendő, hogy állapotok egy halmazát egyelemű halmazokra ossza, feltéve, hogy az állapot teljesen megfigyelhető. Ezért az általánosság teljes megőrzése mellett a teszteket egyváltozós tesztekre szűkíthetjük.
Ahogy a fejezet bevezetőjében elmagyaráztuk, a magyarázatalapú tanulás olyan módszer, ami megfigyelésekből általános szabályokat nyer ki. Tekintsük például az algebrai kifejezések egyszerűsítését és differenciálását (lásd  feladat). X^2-et X szerint differenciálva 2X-et kapunk (vegyük észre, hogy az aritmetikai ismeretlent nagy X betűvel jelöljük az x logikai változótól megkülönböztetve). Egy logikai következtető rendszerben a célt így a Kérdez(Derivált(X^2, X) = d, TB) kifejezéssel fejezhetjük ki, ahol a megoldás a d = 2X.
2002-ben jelent meg Wolfram 1280 oldalas könyve az „A New Kind of Science”, amelyben részletesen kifejti, hogy a sejtautomatákról tett felfedezések nem elszigetelt tények, hanem valamilyen szinten minden tudománnyal kapcsolatosak [TSW].
Vegyük például azt a hipotézist, hogy „Minden smaragd zölék”, ahol a zölék azt jelenti, hogy „zöld, ha t idő előtt figyeljük meg, utána azonban kék”. A t idő előtt bármikor példák millióit láthatnánk, amelyek mind alátámasztják, hogy a smaragdok zölékek, negatív példák nélkül, mégis hezitálnánk a szabályt elfogadni. A jelenség csakis az indukciós folyamat szempontjából releváns a priori tudás szerepével magyarázható. Goodman a hasznosítható a priori tudás teljes választékát javasolja, a meghatározások túlzott hipotézisnek (overhypothesis) nevezett változatát is beleértve. Sajnos a gépi tanulással foglalkozó korai kutatásnál Goodman munkájára nem figyeltek fel.
ami feltételezhetően sokkal nagyobb, mint az a valószínűség, amit az egyszerű Gauss-hibamodell rendelne hozzá. Hívjuk ezt az átmenetihiba-modellnek (transient failure model). Hogyan segít ez, amikor a 0 jelzéssel kerülünk szembe? Feltételezve, hogy az az előre jelzett valószínűség, hogy az akkumulátor lemerült, az eddigi jelzések alapján sokkal kisebb, mint 0,03, akkor az AMérő[21 ]= 0 megfigyelés legjobb magyarázata az, hogy az érzékelő időlegesen meghibásodott. Szemléletesen úgy képzelhetjük el a bizonyosságot az akkumulátor feltöltöttségéről, mint aminek van egy bizonyos mennyiségű „tehetetlensége”, ami segít úrrá lenni a mérőjelzés időleges zavarain. A  (b) ábra felső görbéjén látható, hogy az átmenetihiba-modell képes átmeneti hibákat kezelni a bizonyosság katasztrofális változásai nélkül.
A feladat kiinduló állapotából indulva végignézzük a lehetséges cselekvéssorozatokat, amíg olyat nem találunk, ami a célállapothoz vezet. A tervkészítési probléma állapottér-keresési problémaként formalizálva a következő: * A keresés kiindulási állapota (initial state) megegyezik a tervkészítési probléma kiindulási állapotával. Általánosságban minden állapot pozitív alapliterálok egy halmaza; az itt nem szereplő literálok hamisak. * Egy állapotban minden olyan cselekvés (action) alkalmazható, aminek az előfeltételei teljesülnek. A cselekvés végrehajtása után következő állapotot úgy állítjuk elő, hogy a következményrész pozitív literáljait hozzáadjuk, negatív literáljait pedig töröljük. (Elsőrendű logika esetén az előfeltételekből az egyesítőt alkalmazni kell a következményliterálokra.) Megjegyezzük, hogy annak következményeként, hogy egy explicit cselekvés reprezentációt használunk, egyetlen állapotátmenet-függvény működik az összes tervkészítő problémára. * A célteszt (goal test) ellenőrzi, hogy az adott állapot kielégíti-e a tervkészítési probléma célját. * A lépésköltség (step cost) minden cselekvésre tipikusan 1. Habár könnyű lenne a különböző cselekvésekhez különböző költségeket hozzárendelni, ezt a Strips tervkészítőkben nagyon ritkán alkalmazzák.
Tovább mehetünk azonban még egy lépéssel! Vegyük észre, hogy a kosz eredeti elrendezésétől függetlenül a (Szív; Átmegy-a-szomszéd-mezőbe; Szív) cselekvéssor mindenképp megtisztítja az összes mezőt! (Feltettük, hogy az ágens ismeri a helyzetét, tehát értelmes az 'Átmegy-a-szomszéd-mezőbe' cselekvés.) Az olvasó most azt mondhatja: 'Hohó! Mi van akkor, ha mindkét szoba üres? Akkor feleslegesen mozog az ágens!' Ez igaz. Ettől azonban racionális marad, hisz a pillanatnyi állapotában még nem tudhatja, hogy a másik mezőben van-e kosz, így racionális viselkedés feltételeznie, hogy van! Ha ugyanis azt feltételezné, hogy nincs, holott van, és nem megy át megnézni, elesik 998 ponttól!
A konfigurációs tér rekurzív cellákra bontásával való tervezés Brookstól és Lozano-Perez-től származik (Brooks és Lozano-Perez, 1985), ezt később Zhu és Latombe fejlesztették jelentősen tovább (Zhu és Latombe, 1991). A legkorábbi szkeletonizációs algoritmusok a Voronoi-diagramokon (Rowat, 1979) és a láthatósági gráfokon (visibility graph) alapultak (Wesley és Lozano-Perez, 1979). Guibas és társai (Guibas és társai, 1992) hatékony technikát fejlesztettek ki a Voronoi-diagramok inkrementális számítására. Choset (Choset, 1996) általánosította sokkal átfogóbb mozgástervezési problémákra a Voronoi-diagramokat. John Canny alapozta meg PhD-tézisével (Canny, 1991) az első különálló exponenciális algoritmust pályatervezésre. Másféle szkeletonizációs módszert használt, amit sziluett (silhouette) algoritmusnak hívnak. Jean-Claude Latombe írása (Latombe, 1991) a különböző mozgástervezési metódusok széles skáláját fedi le. Kavraki és társai valószínűségi útitervet fejlesztettek (Kavraki és társai, 1991), amely még ma is a leghatékonyabb. Lozano-Perez és társai (Lozano-Perez és társai, 1984), valamint Canny és Reif (Canny és Reif, 1987) is foglalkoztak a finom mozgások tervezésével korlátozott érzékelés esetén, és intervallumra vonatkoztatott bizonytalanságot használtak valószínűségi bizonytalanság helyett. A referenciapont-alapú navigálás a mobil robotok esetében sok hasonló ötletet használ (Lazanas és Latombe, 1992).
Láttuk, hogy a kategóriák alapvető építőkockái bármely nagyobb méretű reprezentációs sémának. Ebben az alfejezetben a kategóriák szervezéséhez és a kategóriákkal való következtetéshez célzottan megtervezett rendszereket mutatjuk be. Két szoros kapcsolatban lévő rendszercsaládról beszélhetünk. A szemantikus hálók (semantic network) grafikus segítséget nyújtanak a tudásbázis vizualizálásában, és hatékony algoritmusokat biztosítanak, hogy egy objektum tulajdonságait a kategóriához való tartozásából kikövetkeztessük. A leíró logikák (description logics) formális nyelvet adnak a kategóriadefiníciók konstruálásához és kombinálásához, valamint hatékony algoritmusokat annak eldöntéséhez, hogy a kategóriák között fennáll-e a részhalmaz- és a szuperhalmaz-reláció.
Az egységrezolúció (unit resolution) a rezolúció egy korlátozott formája, amelyben minden rezolúciós lépésnek tartalmaznia kell egy egységklózt. Az egységrezolúció általában nem teljes, de a Horn-tudásbázisokban teljes. Az egységrezolúciós bizonyítások a Horn-tudásbázisokon emlékeztetnek az előrefelé láncolásra.
Két statisztikus elmegy az orvoshoz, és mindkettőről ugyanazt a diagnózist állítja fel az orvos: 40% eséllyel a halálos A betegségben, 60% eséllyel a szintén végzetes B betegségben szenvednek. Szerencsére van az A és B betegségnek is olcsó, 100%-ban hatásos, mellékhatás nélküli gyógyszere. A statisztikusoknak lehetőségük van az A elleni, a B elleni vagy mindkét gyógyszert szedni, de dönthetnek úgy is, hogy egyiket sem szedik. Mit fog az első statisztikus – aki elszánt Bayes-hívő – választani? Mit tesz a második, aki mindig a maximum-likelihood hipotézist választja?
Az ágens tökéletesen racionális, ha minden pillanatban a lehető leghasznosabban viselkedik környezetében. Ez egy ideális elvárás az ágenssel szemben, ami valós bonyolultságú környezetek esetén legtöbbször kivitelezhetetlen (valós bonyolultságú problémák megoldására általánosságban nem tudunk ilyen ágenst készíteni).
A telemedicina jól alkalmazható módszer akkor, ha távoli helyen élők, dolgozók orvosi ellátása válik szükségessé. Ilyenkor nincs lehetőség arra, hogy egy kisebb csoport kedvéért egy teljes szakértő gárda is a helyszínen tartózkodjon, az orvosi elérhetőséget azonban biztosítani lehet és kell is.
Helyesen választott ki olyan összetevőket, mint például az „of the House of Lords” PP, bár a tradicionális elemzéssel ellentétesen dolgozik akkor például, amikor a „the”-t a megelőző igével és nem a követő főnévvel csoportosítja.
 Megjegyzés A  feladat navigációs problémáját a következőképpen transzformálja át egy környezetbe: * Az érzékelés az ágens által látható sokszögcsúcsok az ágenshez viszonyított helyzetének listája. Az érzékelés a robot pozícióját nem tartalmazza! A robotnak a saját pozícióját a térkép alapján kell megtanulnia. Azt is tételezzük fel ideiglenesen, hogy minden lokációnak eltérő a „panorámája”. * Minden cselekvés egy a követendő egyenes utat leíró vektor lesz. Ha az úton nincsenek akadályok, a cselekvés sikeres. Egyébként a robot azon a ponton megáll, ahol az út az első akadályt metszi. Ha az ágens a zérus mozgás vektorát adja vissza és a célpozícióban van (ami rögzített és ismert), akkor a környezetnek teleportálnia kell az ágenst egy véletlen lokációba (de nem egy akadályon belülre). * A hatékonysági mérték 1 pontot kér az ágenstől a megtett távolság minden egységtávjáért és 1000 ponttal díjazza a cél mindenkori elérését.
A vizsgálatot kezdjük magával a stratégiával. Emlékezzünk vissza, hogy a π stratégia egy olyan függvény, amely állapotokat képez le cselekvésekre. Mi elsősorban olyan paraméterezet π stratégiák vizsgálatában vagyunk érdekeltek, amelyeknek sokkal kevesebb paramétere van, mint ahány állapot található az állapottérben (éppúgy, mint az előző alfejezetben). Például paraméterezett Q-függvényekkel reprezentálhatjuk a π stratégiát, egy-egy Q-függvényt rendelve minden cselekvéshez, és azt a cselekvést választjuk, amely a legnagyobb jósolt értéket adja:
Fontos megjegyezni, hogy a hasznosságfüggvény léte, ami leírja az ágens preferenciáit, nem jelenti azt szükségszerűen, hogy az ágens explicit módon maximalizál egy hasznosságfüggvényt a mérlegelései során. Amint azt a  fejezetben megmutattuk, racionális viselkedés számos módon előállítható, némelyik sokkal hatékonyabb, mint a hasznosság maximalizálásának explicit elvégzése. Az ágens preferenciáinak megfigyelésével azonban lehetségessé válik a hasznosságfüggvény megkonstruálása, ami az ágens elérni szándékozott céljait reprezentálja.
Arisztotelész (i. e. 384–322) volt az első, aki megkísérelte precízen megfogalmazni az elme racionális részén uralkodó törvényszerűségeket. A korrekt következtetéseket leíró szillogizmusok informális rendszerét fejlesztette ki, ami elvben lehetővé tette a konklúziók gépies származtatását, ha a kezdeti premisszák egyszer adottak voltak. Sokkal később Ramon Lull (élt 1315-ig) javasolta, hogy a hasznos következtetést éppenséggel egy gépezetre rá lehetne bízni. Az általa javasolt „fogalmi kerekek” a könyv borítóján láthatók. Thomas Hobbes (1588–1679) felvetette, hogy a következtetés olyan, mint egy numerikus számítás, vagyis hogy „csendes gondolatainkban kivonunk és összeadunk”. Eközben a számítások automatizálása már a legjobb úton haladt. 1500 körül Leonardo da Vinci (1452–1519) megtervezett – bár meg nem épített – egy mechanikus kalkulátort. A kalkulátor közelmúltbeli rekonstrukciója bebizonyította, hogy a tervezet működőképes volt. Az első ismert számítógépet 1623 körül Wilhelm Schickard (1592–1635) német tudós konstruálta, bár a Blaise Pascal (1623–1662) által 1642-ben épített Pascaline ismertebb. Pascal azt írta, hogy „az aritmetikai gép olyan hatást produkál, ami közelebbinek tűnik a gondolathoz, mint az állatok összes cselekvése”. Gottfried Wilhelm Leibniz (1646–1716) egy mechanikus berendezést épített, amely nem a számokon, hanem a fogalmakon végzett műveleteket, bár működési köre igen korlátozott volt.
Ennek az információnak egy része reguláris kifejezésekkel (regular expression) kezelhető, amelyek reguláris nyelvtant definiálnak egy karakterfüzérben. Reguláris kifejezéseket használnak olyan Unix-parancsokban, mint a grep, olyan programozási nyelvekben, mint a Perl, valamint olyan szövegszerkesztőkben, mint a Microsoft Word. A részletek eltérők az egyes eszközökben, és legjobban a megfelelő útmutatóból tanulhatók meg, azonban itt bemutatjuk, hogyan lehet reguláris kifejezéseket építeni dollárban megadott árakra, bemutatva a közös részkifejezéseket:
* Milk: a házakban fogyasztott tejet reprezentáló érték * Coffee: a házakban fogyasztott kávét reprezentáló érték * Tea: a házakban fogyasztott teát reprezentáló érték * OrangeJuice: a házakban fogyasztott narancslevet reprezentáló érték * Water: a házakban fogyasztott vizet reprezentáló érték
A  alfejezetben a passzív tanulással (passive learning) kezdünk foglalkozni, amelyben az ágensnek rögzített stratégiája van, és az állapotok hasznosságának (vagy az állapot-cselekvés párok hasznosságának) megtanulása a feladat. Ez magában foglalhatja a környezet modelljének megtanulását is. A  alfejezet az aktív tanulással (active learning) foglalkozik, amikor is az ágensnek azt is meg kell tanulnia, hogy mit tegyen. Az alapvető elv a felfedezés (exploration), az ágensnek a lehető legtöbbet meg kell tapasztalnia környezetéről ahhoz, hogy megtanulja, mi a célszerű viselkedés benne. A  alfejezet bemutatja, hogy az ágens hogyan tudja felhasználni az induktív tanulást arra, hogy sokkal gyorsabban tanuljon a tapasztalataiból. A  alfejezet a közvetlen stratégiareprezentáció tanulásának módszereivel foglalkozik reflexszerű ágenseknél. A Markov döntési folyamatok (lásd  fejezet) megértése döntő fontosságú ennek a fejezetnek a tárgyalása szempontjából.
 Fontos Most azonban, ha egy állapot hasznossága a leszámítolt jutalmak várható értékének összege attól a kiindulóponttól kezdve, akkor közvetlen kapcsolat áll fenn egy állapot hasznossága és a szomszédainak a hasznossága között: egy állapot hasznossága az állapotban tartózkodás közvetlen jutalmának és a következő állapot várható leszámítolt hasznosságának az összege, feltéve, hogy az ágens az optimális cselekvést választja. Azaz egy állapot hasznossága a következő:
Megjegyeznénk, hogy itt nem csak az iskolában tanult aritmetikai, vagy más egyéb függvényekre gondolhatunk. Itt egy-egy függvény matematikai apparátusa attól függ, hogy milyen módon ábrázoltuk az ágens problématerét, és a környezeti tényeket rögzítő tudásbázisát. Ettől függ a függvény értelmezési tartománya és értékkészletete. Jelen esetben tehát az értelmezési tartomány az ágens adott időpillanatig lehetséges érzét-sorozatainak halmaza (minden lehetséges időpillanatra), az értékkészlet pedig az ágens adott környezetben lehetséges - fizikai, kommunikációs, vagy kognitív - cselekvéseinek halmaza.
A logikai következtetéseket széles körben vizsgálták már az ókori görög matematikusok is. Az Arisztotelész által legtöbbet vizsgált következtetési módszer a szillogizmus (syllogism) volt. A szillogizmus „szituációkra” és „hangulatokra” bontható a termek mondatbeli sorrendjétől függően (amelyeket predikátumoknak nevezünk ma) és az egyes termek általánosságának mértékétől függően (amit ma a kvantorok segítségével értelmezünk), illetve attól függően, hogy a termek negálva voltak-e. A legalapvetőbb szillogizmus, amely az első szituáció első hangulata, a következő:
Az V. részben rámutattunk, hogy a valós életre jellemző környezetben gyakran előfordul, hogy a tudás bizonytalan. Az ágensek a bizonytalanságot valószínűség- és döntéselméleti módszerekkel tudják kezelni, de ehhez először tapasztalataik alapján fel kell állítaniuk a világra vonatkozó valószínűségi modelljüket. Ez a fejezet bemutatja, hogy milyen módon tudják ezt megtenni. Látni fogjuk, hogyan kell úgy formalizálni a tanulási feladatot, mint egy valószínűségi következtetési folyamatot (lásd  alfejezet). Bemutatjuk, hogy a tanulás bayesi megközelítése rendkívül hatékony: általános megoldást ad a zaj, a túlilleszkedés és az optimális predikció problémáira. Továbbá figyelembe veszi azt a tényt, hogy a nem-egészen-mindentudó ágens soha nem tudhatja biztosan, hogy a világról alkotott melyik elmélet helyes, mégis döntéseket kell hoznia.
Kezdjük tehát el az  ábrán látható kiindulási problémánk megoldását. Első lépésben először is egy aktuálisan behelyettesítetlen változót kell választanunk. Ezt az MRV és a DEG heurisztika fentebb leírt kombinált alkalmazásával tesszük (először kiemeljük az MRV szerint minimális értékkészletű változókat, majd ezek közül DEG szerint kiválasztjuk valamely maximális fokszámút). Jelenleg ugyebár még minden változónk behelyettesítetlen (üres behelyettesítéssel indulunk). A változók heurisztikus értékeit az alábbi táblázat foglalja össze.
A  fejezetben láttuk, hogyan képes egy ágens kommunikálni egy másik (szoftver vagy emberi) ágenssel közös nyelvi megnyilatkozások segítségével. A megnyilatkozások teljes szintaktikai és szemantikai elemzése szükséges a jelentésük teljes kinyeréséhez, ami azért lehetséges, mert a megnyilatkozások rövidek és körülhatárolt tárgyterületre korlátozottak.
Egy tanuló ágens négy koncepcionális komponensre bontható fel, ahogy azt a  ábra mutatja. A legfontosabb különbségtétel a javításokért felelős tanuló elem (learning element) és a külső cselekvések kiválasztásáért felelős végrehajtó elem (performance element) között van. A végrehajtó elem az, amit eddig a teljes ágensnek tekintettünk: ez végzi az észleléseket, és ez dönt a cselekvésekről. A tanuló elem a kritikustól (critic) kapott, az ágens működéséről szóló visszajelzést használja annak megállapítására, hogy a végrehajtó elemet hogyan kell módosítani annak érdekében, hogy a jövőben jobban működjön az ágens.
Rajzolja fel a 22. szakasz - Egy koherens szöveg struktúrája részben található „John egy elegáns étterembe megy” történet szövegelemzési fáját! Használja a Segment-re vonatkozó két szabályt, megadva a helyes CoherenceRelation-t mindent egyes csomópontra! (Nem kell az egyes mondatok elemzéseit megmutatnia.) Most tegye meg ugyanezt egy Ön által választott tetszőleges 5–10 mondat hosszúságú szövegre!
Annak fényében, amit most tudunk a számítógépekről, nem meglepő, hogy jól teljesítenek a sakkhoz hasonló kombinatorikai problémákban. De az algoritmusok az emberhez mérhető szinten hajtanak végre olyan tevékenységeket is, amelyek látszólag emberi döntést igényelnek, amelyek során, mint Turing mondta, képesnek kell lenni a „tapasztalatból tanulni” és „megkülönböztetni a helyest a helytelentől”. Még egészen korán, 1955-ben Paul Meehl (lásd még Grove és Meehl, 1996) képzett szakértők döntéshozatali folyamatát tanulmányozta olyan szubjektív esetekben, mint például hogy egy diák elvégez-e egy képzési programot, vagy egy bűnöző visszaesik-e. A megvizsgált 20 eset közül 19-nél Meehl azt találta, hogy egyszerű statisztikai tanulási algoritmusok (mint például a lineáris regresszió vagy a naiv Bayes-tanulás) jobb előrejelzéseket adnak, mint az emberi szakértők. Az Educational Testing Service a GMAT-vizsga esszékérdéseinek millióit osztályozza 1999 óta egy automatizált programmal. A program osztályzata az osztályzást végző emberek minősítésével az esetek 97%-ában egyezik meg, és ez az arány hasonló ahhoz, amennyire a különböző emberek által adott minősítések megegyeznek (Burstein és társai, 2001).
A részecskeszűrős lokalizációs algoritmust Monte Carlo lokalizációnak (Monte Carlo localization, MLC) hívják. Az MCL alapvetően megegyezik a  ábrán bemutatott részecskeszűrős algoritmussal, mindössze annyit kell módosítanunk, hogy szükségünk van a megfelelő mozgás és szenzor modellre. A  ábra bemutat egy változatot, pásztázó távolságméréses modellel. Az algoritmus működését, azt, hogy hogyan azonosítja a robot saját tartózkodási helyét az irodaépületben, a  ábra szemlélteti. Az első képen még a részecskék egyenletesen oszlanak el, az előzetes információ alapján, jelképezve a teljes bizonytalanságot a robot pozícióját illetően. A második képen már az első mérések alapján klasztereket alkotnak a részecskék, oda tömörülve, ahol valószínűleg tartózkodik a robot. A harmadik esetben pedig már elég információ áll rendelkezésre a mérésekből, hogy az összes részecske ugyanoda jusson.
A megkonstruálandó tudásbázisnak képesnek kell lennie a továbbiakban közölt kérdéslistát megválaszolni. A kérdések közül néhány közvetlenül magával a történettel foglalkozik, de a többségük a háttértudást – a sorok közötti olvasást – igényli. Foglalkozni kell azzal, hogy milyen dolgokat lehet egy bevásárlóközpontban vásárolni, hogy a kiválasztott tételek vásárlásánál mi történik, hogy mi a vásárolt tételek rendeltetése stb. Kísérelje meg a reprezentációját a lehető a legáltalánosabbra kialakítani. Egy triviális példával szemlélve, ne azt állítsa, hogy „Az emberek a Skálában ennivalót vásárolnak”, mert ez nem segít azok kezelésében, akik máshol szoktak vásárolni. Ne azt állítsa, hogy „János darált hússal és paradicsommal spagettit készített”, mert ez semmilyen más dolog kezelésében nem segít. Ne rejtsen válaszokat a kérdésekbe, a (c) kérdés például azt kérdezi, hogy „Vásárolt János húst?”, és nem azt, hogy „Vásárolt-e János fél kiló darált húst?”.
Az eseménykalkulusban a folyó események időpillanatokra és nem szituációkra vonatkoznak, és a kalkulust úgy tervezték, hogy az időintervallumokról is lehessen következtetni. Az eseménykalkulus axióma azt mondja ki, hogy egy folyó esemény igaz egy időpontban, ha a folyó eseményt valamilyen múltbeli esemény kezdeményezte, és időközben a folyó eseményt semmilyen közbenső esemény nem állította le. Az Inicializál és a Leállít relációk a szituációkalkulus-beli Eredmény relációhoz hasonló szerepet töltenek be. Az Inicializál(e, f, t) azt jelenti, hogy az e esemény t időpontban történő bekövetkezése az f folyó eseményt igazzá teszi, míg a Leállít(w, f, t) jelentése, hogy f igaz értéke megszűnt. A Történik(e, t) azt fogja jelenteni, hogy az e esemény a t időpontban történik, a Levág(f, t, t[2])-t pedig arra fogjuk használni, hogy leírhassuk, hogy az f-et valamilyen esemény a t és a t[2] időpontok között leállította. Formálisan az axióma a következő:
 Megjegyzés Generáljon sok 8-as kirakójáték- és 8-királynő feladat-példányt, és oldja meg ezeket (ha lehetséges) a hegymászó kereséssel (legmeredekebb emelkedő és az első-választás változatok), véletlen újraindítású hegymászó kereséssel és szimulált lehűtéssel. Mérje meg a megoldási költségeket, és százalékosan adja meg a megoldások arányát. Mindezeket ábrázolja egy grafikonon az optimális költség függvényében. Fűzzön kommentárt az eredményekhez.
A   egyenlet definiálja, hogy mit is jelent egy adott Bayes-háló. Azonban arra nézve nem ad felvilágosítást, hogyan építhetünk olyan Bayes-hálót, hogy az általa meghatározott együttes valószínűség-eloszlás függvény az adott tárgytartomány megfelelő leírása legyen. Most megmutatjuk, hogy a   egyenlet tartalmaz bizonyos feltételes függetlenségi relációkat, amelyeket a tudásmérnök felhasználhat a háló topológiájának meghatározásánál. Elsőként írjuk fel az együttes valószínűség-eloszlás függvényt feltételes valószínűségek szorzataként, felhasználva a szorzatszabályt (lásd  fejezet):
Az objektivista (objectivist) megközelítés szerint a valószínűségek inkább az univerzum valóságos jellemzőinek tekinthetők – ahol egy jellemző a dolgok hajlama bizonyos típusú viselkedésre –, mint egy megfigyelő meggyőződési foka leírásának. Például az, hogy egy nem hamis pénzdarab feldobásánál 0,5-es valószínűséggel fej jön ki, magának a pénzdarabnak a hajlandósága. E szemlélet szerint a frekvencionista mérések ezen hajlandóságok megfigyelésére tett kísérletek. A legtöbb fizikus egyetért abban, hogy a kvantumjelenségek valójában valószínűségiek, de makroszkopikus szinten – például a pénzfeldobásnál – a bizonytalanság a kezdeti feltételek elhanyagolása miatt lép fel, ami látszólag nem konzisztens a hajlandóságalapú szemlélettel.
A h függvényt hipotézisnek (hypothesis) nevezzük. A tanulás nehézsége elvi szempontból abban áll, hogy nem könnyű egy adott h függvényről megmondani, hogy jól approximálja-e f-et. Egy jó hipotézis jól általánosít (generalize) – azaz jó becslést kapunk a még nem látott mintákra is. Ez az induktív következtetés alapproblémája (problem of induction). A problémát már évszázadok óta kutatják, a  alfejezet egy részmegoldást szolgáltat.  ábra - (a) Példa (x, f(x)) értékpárok és egy velük konzisztens lineáris hipotézis. (b) Egy – ugyanazokkal az adatokkal konzisztens – hetedfokú polinom. (c) Egy másik adathalmaz egy pontosan illeszkedő hatodfokú polinommal, illetve egy közelítő lineáris illesztéssel. (d) Egy egyszerű szinuszos illesztés ugyanazokra az adatokra. (a) Példa (x, f(x)) értékpárok és egy velük konzisztens lineáris hipotézis. (b) Egy – ugyanazokkal az adatokkal konzisztens – hetedfokú polinom. (c) Egy másik adathalmaz egy pontosan illeszkedő hatodfokú polinommal, illetve egy közelítő lineáris illesztéssel. (d) Egy egyszerű szinuszos illesztés ugyanazokra az adatokra. Fontos A  ábra ismert példát mutat be: egy adathalmazra egyváltozós függvényt illesztünk. A minták (x, f(x)) értékpárok, ahol mind x, mind f(x) valós értékek. Legyen a H hipotézistér (hypothesis space H) – azon hipotézisek halmaza, amelyeket meg fogunk vizsgálni – a legfeljebb k-ad fokú polinomok tere. (Például 3x^2 + 2, x^17 – 4x^3 stb.) A  (a) ábra valamilyen adatokat és egy az adatokra pontosan illeszkedő egyenes vonalat (elsőfokú polinom) mutat. Ez a vonal itt egy konzisztens (consistent) hipotézis, mivel valamennyi adatra illeszkedik. A  (b) ábra egy nagy fokszámú polinomot mutat, amely szintén konzisztens ugyanazokra az adatokra nézve. Ez a két példa illusztrálja az induktív tanulás első komoly megoldandó kérdését: hogyan válasszunk több konzisztens hipotézis közül? Egy lehetséges válasz az ún. Ockham borotvája^[182] (Ockham’s razor) elv: részesítsük előnyben a legegyszerűbb olyan hipotézist, amely konzisztens az adatokkal. Ez józan intuíciónak tűnik, mivel azok a hipotézisek, amelyek nem egyszerűbbek, mint maguk az adatok, nem nyernek ki semmilyen mintázatot az adatokból. Az egyszerűség definíciója nem könnyű, de értelmesnek tűnik azt mondani, hogy egy elsőfokú polinom egyszerűbb, mint egy 12-ed fokú. Fontos A  (c) ábra egy másik adathalmazt mutat. Ezekre az adatokra nem lehet konzisztens módon egyenest illeszteni, valójában egy hatodfokú polinomra (amely 7 szabad paraméterrel rendelkezik) van szükség a pontos illesztéshez. A halmazban csak 7 adat van, így a polinomnak épp annyi paramétere van, ahány adatpont. Ennek megfelelően úgy tűnik, hogy nem sikerül valamilyen mintázatot találnunk az adathalmazban, így nem várunk jó általánosítóképességet. Jobban járhatunk, ha inkább egy egyszerű egyenest illesztünk, amely ugyan nem konzisztens, de ésszerű becsléseket adhat. Ez azt valószínűsíti, hogy a keresett függvény nem determinisztikus (vagy ami ezzel közelítőleg ekvivalens: a valódi bemeneti minták nem figyelhetők meg teljes pontossággal). Nemdeterminisztikus függvények esetén elkerülhetetlen, hogy kompromisszumot kössünk a hipotézis komplexitása és az adatokhoz való illeszkedés pontossága között. A  fejezetben bemutatjuk, hogy ezt a kompromisszumot hogyan lehet a valószínűség-számítás elméletének felhasználásával kialakítani.
Az egyenlőségi következtetések mellett a tételbizonyítások tartalmaztak különféle speciális célú döntési folyamatokat. Nelson és Oppen (Nelson és Oppen, 1979) javasoltak egy nagy hatású sémát az ilyen eljárásoknak egy általános következtetési rendszerbe integrálására. Más módszerek is hasonló problémákkal foglalkoztak, beleértve Stickel (Stickel, 1985) „elméleti rezolúcióját”, valamint Manna és Waldinger (Manna és Waldinger, 1986) „speciális relációit”.
a korábban definiált H[r] hipotézis számára hamis negatív példa lesz. H[r]-ből, illetve a példa leírásából kikövetkeztethető mind a VárjunkE(X[13]) – ezt állítja a példa, mind a ¬VárjunkE(X[13]) – ezt állítja a hipotézis. Tehát a hipotézis és a példa logikailag ellentmondásban van. * A hipotézis szempontjából egy példa hamis pozitív (false positive), ha a valóságban negatív, de a hipotézis szerint pozitív.^[189]
A teljes együttes valószínűségeket korábban már meghatároztuk, így készen is vagyunk – kivéve, ha a számítási bonyolultság kérdésével is törődünk. 12 ismeretlen négyzetünk van; így az összegzés 2^12 = 4096 tagból fog állni. Általánosságban, az összegzés exponenciálisan nő a négyzetek számával.
Garry Kasparov 1996-ban játszott először a Deep Blue-val ( ábra). Akkor a győzelmet aratott, annak ellenére, hogy egy játszmát a gép nyert. Egy évvel rá azonban visszavágót hirdettek és a Deep Blue továbbfejlesztett változata (becenevén Deeper Blue), 3.5 - 2.5 arányban győzni tudott [3]. Az utolsó játszma, amely a döntő volt, kevesebb, mint egy órát tartott és mindössze 19 lépésből állt.
A ma irányításelméletnek (control theory) nevezett terület keletkezésében Norbert Wiener (1894–1964) játszott központi szerepet. Wiener zseniális matematikus volt, aki többek közt Bertrand Russell-lel is együttműködött, mielőtt az érdeklődését a biológiai és mechanikai szabályozó rendszereknek és ezek a kognitív mechanizmusokkal való kapcsolatának szentelte. Craikhoz hasonlóan (aki szintén használt szabályozási rendszereket mint pszichológiai modelleket), Wiener és kollégái, Arturo Rosenblueth és Julian Bigelow kihívást jelentettek a behaviorista ortodoxia számára (Rosenblueth és társai, 1943). Nézetük szerint a célorientált viselkedés a „hibát” – az aktuális állapot és a célállapot közötti különbséget – minimalizáló szabályozó mechanizmusból fejlődik ki. Az 1940-es évek végén Wiener, Warren McCulloch-val, Walter Pittsszel és Neumann Jánossal együtt egy sor konferenciát szervezett, ahol a kognitív folyamatok új matematikai és számítási modelljeivel foglalkoztak, és a behaviorista tudományok területén sok tudósra voltak hatással. Wiener Cybernetics c. könyve (Wiener, 1948) bestseller lett, és ráébresztette a széles publikumot az intelligens mesterséges gépek lehetőségére.
Az (y[j] – (θ[1]x[j] + θ[2])) mennyiség valójában az (x[j], y[j]) hibája (error) – azaz a tényleges y[j]érték és a becsült érték különbsége. Így E nem más, mint a jól ismert hibanégyzetek összege (sum of squared errors). Ezt a standard lineáris regresszió (linear regression) minimalizálja. Most megérthetjük, hogy miért: a hibanégyzetek összegének minimalizálása nem más, mint a maximum-likelihood lineáris (egyenessel ábrázolható) modell megadása, feltéve, hogy az adatokat rögzített varianciájú Gauss-zaj mellett generáltuk.
A következő (WA, SA) él konzisztens, hiszen WA egyetlen RED értéke konzisztens SA egyetlen lehetséges BLUE értékével. A (WA, SA) él tehát lekerül a listáról, és következik az (NT, SA) él. Szerencsére ez is konzisztens, hiszen NT egyetlen GREEN értéke is konzisztens SA egyetlen BLUE értékével. Ennek megfelelően az (NT, SA) él is lekerül az él-listáról, és a (Q, SA) élre kerül a sor.
Az MR vizsgálat elsősorban a hidrogénben gazdag víz és zsír, illetve az elmozdulások kimutatásában érzékeny- ezeket „látja”. Szöveti felbontóképessége azonban ennél lényegesen szélesebb körű: a módszer a különböző szövetek jellemzésére az ultrahanghoz hasonlóan több, egymástól eléggé eltérő mutatót tud felhasználni. Egyazon területről a rádiófrekvenciás impulzus-sorozat jellege, illetve a visszaérkező szignál mérési módja függvényében más és más kép nyerhető. Ugyanakkor a kapott értékek mindig viszonylagosak, a Hounsfield-értékektől eltérően nem számszerűsíthetők.
Ennek a webbel kapcsolatos változata a Webes Ontológia. Az ezt megvalósító nyelvet pedig OWL-nek hívjuk. Fontos Az OWL nyelv nagyobb szókészlettel rendelkezik, mint a fentebb taglalt nyelvek, és elemei egy formális szemantikát kínálnak.[11]
A kategóriákra vonatkozó Partíció reláció mintájára egy RészPartíció relációt definiálhatunk (lásd  feladat). Egy objektum a RészPartíció-jában felsorolt részeiből áll, és bizonyos tulajdonságaira ezekből a részekből lehet következtetni. Így például egy összetett objektum tömege a részeihez tartozó tömegek összege. Jegyezzük meg, hogy ez a kategóriákra nem vonatkozik: egy kategóriának nincs tömege annak ellenére, hogy az elemeinek lehet tömege.
A  ábrán sárgával jelölt mezőkön felismertem egy 1-1 mintát, amiből következően a  ábrán kérdőjellel jelölt mezőről megállapítottam, hogy biztonságos. Ekkor újabb 1-1 mintát találtam, amiből a  ábrán látható következtetést vonhattam le. Első pillantásra itt nem találtam semmit, ezért felfedtem a korábban biztonságosnak minősített mezőket. Ez látható a  ábrán. Ekkor vettem észre egy „rejtett” 1-2-2-1 mintát (a  ábrán zölddel jelölve): ha a zölddel jelölt mezők aknaszámából levonjuk a már megtalált aknaszomszédok számát, épp egy 1-2-2-1 mintát kapunk, így az  ábrán látható következtetéseket vontam le. Ezután felnyitottam a kérdőjeles mezőket, és kizárólag a két alapstratégia (fennmaradó helyek és telítettség vizsgálata) segítségével megoldottam a feladványt. ( ábra)
A logikai programozás közel áll egy olyan technológiához, amely megtestesíti azt a deklaratív ideált, amelyet a  fejezetben írtunk le, miszerint a rendszereket úgy kell létrehozni, hogy a tudást formális nyelven fejezzük ki, és a problémákat egy következtetési folyamat végigfuttatásával oldjuk meg. Ezt az ideális megközelítést Robert Kowalski egyenlete foglalja össze:
A feltételes következményeket a porszívóvilág Szív cselekvésére mutattuk be, ahol az, hogy melyik kocka lesz tiszta, attól függ, hogy mely kockán áll a robot. Tud mondani olyan ítéletlogikai változókat, melyek a porszívóvilág állapotait definiálják úgy, hogy a Szív cselekvésnek feltétel nélküli leírása legyen? Adja meg a Szív, a Balra és a Jobbra cselekvések leírását az állításaik felhasználásával, és mutassa meg, hogy ezek kielégítők a világ összes lehetséges állapotának leírásához.
Olyan porszívóvilágra, ahol tiszta négyzetek piszkossá válhatnak, jó megoldás lehet az eredeti megközelítés: az ágens ide-oda cikázik, és port szív, ha kell. Ez racionális viselkedés addig, amíg nincs semmiféle ismeretünk arról, hogy milyen gyakran válnak tiszta mezők piszkossá. Ha azonban tudjuk, hogy minden időpillanatban egy mező p valószínűséggel válik piszkossá, kiszámíthatjuk annak várható értékét, hogy mennyi idő elteltével válik jó eséllyel (a 'jó esélyt' általában 95%-nak szokás választani a statisztikában) újra piszkossá, és ekkor megvizsgálni. Így minimalizálhatjuk a büntetést, amit a miatt kapunk, hogy a mező piszkos vagy feleslegesen mozogtunk. Amennyiben az ágens eleinte nem tud semmit a 'bepiszkolódás' valószínűségéről, úgy érdemes megtanulnia azt úgy, hogy eleinte gyakran felkeres minden mezőt, majd ha már elég tapasztalatot szerzett, csak akkor mozog, ha úgy ítéli, már jó eséllyel szükséges takarítania.
Az ismétlődő állapotok tehát a megoldható problémákat megoldhatatlan problémákká alakítják, amennyiben az algoritmus nem képes ezeket az állapotokat detektálni. A detektálás általában azt jelenti, hogy az új kifejtendő csomópontot a már kifejtett csomópontokkal hasonlítjuk össze. Egyezés esetén az adott csomóponthoz az algoritmus két utat talált, és valamelyiket eldobhatja. Fontos A mélységi keresés csak a gyökeret és az aktuális csomóponttal összekötő úton fekvő állapotokat tárolja. E csomópontoknak az aktuális csomóponttal való összehasonlítása lehetővé teszi a hurkok felismerését, amiket ezek után azonnal el lehet dobni. Ez meg is véd attól, hogy a hurkok révén véges állapotterekből ne keletkezzenek végtelen keresési fák. Sajnos azonban nem tud megvédeni a nem hurkos utak exponenciális megsokszorozódásától az olyan problémákban, mint amilyenek a  ábrán látható. Ezt elkerülni csak úgy lehet, hogy több csomópontot tartunk a memóriában. Az idő és a tárigény között egy alapvető kompromisszum létezik. Az az algoritmus, amely elfelejti a történetét, kénytelen azt megismételni.
Az MRV heurisztika tehát két változót is, az NT-t és a V-t is potenciális folytatásként javasolja, így ezek közül kell választanunk. Tegyük fel, hogy az algoritmus az NT-t választja, és ennek adja az egyetlen lehetséges értéket, a RED-et (NT=RED).
Nunberg a metonímia (metonymy) formális modelljét vázolja (Nunberg, 1979). Lakoff és Johnson az angol nyelv gyakori metaforáit katalogizálja és magával ragadó módon elemzi (Lakoff és Johnson, 1980). Ortony metaforáról szóló cikkgyűjteményt mutat be (Ortony, 1979); Martin a metafora értelmezésének számítógépes megközelítését adja meg (Martin, 1990).
A hagyományos megközelítés az MI-ben az volt, hogy a számítható racionalitással kezdünk, majd az erőforráskorlátok kielégíthetősége érdekében kompromisszumokat hozunk. Ha ezek a korlátok csak kis problémákat okoznak, akkor a keletkező terv várhatóan a KO ágens tervéhez lesz hasonló. Ha azonban az erőforráskorlátok kritikussá válnak – például mert a környezet egyre összetettebbé válik –, akkor a két terv várhatóan el fog térni egymástól. A korlátozott optimalitás elmélete ezen korlátok kezelésére kínál elveket.
Az első megközelítés úgy jár el, hogy néhány változónak értéket ad, a maradékok pedig fát fognak alkotni. Vegyük ismét az  (a) ábrán látható kényszergráfot az Ausztrália-példához. Ha törölni tudnánk Dél-Ausztráliát, akkor a gráf fává válhatna (ahogy az ábra (b) részén látható). Szerencsére meg tudjuk tenni ezt (a gráfban, nem a kontinensen) azzal, hogy DA értékét rögzítjük, és a többi változó tartományából töröljük azokat az értéket, melyek inkonzisztensek a DA számára választottal.
olyan fát kell visszaadnia, amelynek gyökerében S áll, levelei a „the wumpus is dead” és belső csomópontjai az ℰ[0 ]nyelvtan nem záró szimbólumai. A  ábrán láthattunk egy ilyen fát. Folytonos szövegként a következőképpen írhatjuk:
A különféle képalkotó eljárásokkal nyert képeket nem lehet helyesen értelmezni olyankor, ha túl kevés vagy túl sok jelet fogtunk fel, esetleg a mintavétel nem abból a jeltartományból történt, amelyben az elváltozás egyáltalán megnyilvánul.
Több rejtett neuronnal különböző helyeken több, eltérő méretű dudort tudunk létrehozni. Valójában egyetlen, megfelelően nagy rejtett réteggel a bemenetek tetszőleges folytonos függvénye tetszőleges pontossággal reprezentálható, sőt két réteggel még nemfolytonos függvények is reprezentálhatók.^[206] Sajnos egy egyedi neurális struktúra esetén nemigen lehet megmondani, hogy milyen függvények reprezentálhatók, és milyenek nem.
Eddig a CHINOOK 1994. előtti felépítését ismertettem. Említésre került, hogy kísérletképpen a megnyitási adatbázist tovább nem alkalmazták és a csapat számára meglepő módon a CHINOOK jobb eredményeket ért el. Most ismertetném a program életében létrejött változásokat, vagyis a CHINOOK végleges felépítését.
Képzeljünk el egy leszámítolatlan MDF-et három állapottal, (1, 2, 3) és sorrendben a következő jutalmakkal –1, –2, 0. A 3-as állapot egy végállapot. Az 1-es és 2-es állapotokban két lehetséges cselekvés van: a és b. Az állapotátmenet-modell a következő: * Az 1-es állapotban az a cselekvés az ágenst a 2-es állapotba mozgatja 0,8 valószínűséggel, míg 0,2 valószínűséggel helyben hagyja. * Az 2-es állapotban az a cselekvés az ágenst az 1-es állapotba mozgatja 0,8 valószínűséggel, és 0,2 valószínűséggel helyben hagyja. * Mind az 1-es, mind a 2-es állapotban a b cselekvés az ágenst a 3-as állapotba mozgatja 0,1 valószínűséggel, és 0,9 valószínűséggel helyben hagyja.
A természetes nyelv generálását az ötvenes években, a gépi fordítás legelső napjaitól kezdve figyelembe vették, de a hetvenes évekig nem merült fel egynyelvű problémaként. Simmons és Slocum (Simmons és Slocum, 1972), valamint Goldman (Goldman, 1975) munkája reprezentatív. A Penman (Bateman és társai, 1989) volt az első teljes generáló rendszer, amely a Szisztematikus Nyelvtanra (Systematic Grammar) (Kasper, 1988) épült. A kilencvenes években két fontos szabadon elérhető generáló rendszer vált elérhetővé, a KPML (Bateman, 1997) és a FUF (Elhadad, 1993). A generálásról szóló fontos könyvek között szerepel (McKeown, 1985; Hovy, 1988; Patten, 1988; Reiter és Dale, 2000).
Az ellenfél csapatába tartozó ágensek célja szintén minél több energiát gyűjteni az ennivaló elfogyasztásával. A különböző csapatba tartozó ágensek nem tartózkodhatnak azonos cellákon, így ha egy ágens egy ennivalóra ráállt és azt éppen eszi, akkor az ellenfél ágensei az ennivalóhoz csak akkor férnek hozzá, ha az éppen fogyasztó ágenst egy támadással félrelökik. Csapattagok tartózkodhatnak azonos cellán, így előfordulhat, hogy egy csapatból egyszerre több ágens is párhuzamosan fogyasztja az ennivalót. Minden megtámadott ágens a támadást követően érzékeli a támadója azonosítóját illetve pozícióját még akkor is, ha a támadó végig a látóterén kívül helyezkedett el.
Az ágens első feladata, hogy releváns termékajánlatokat találjon (majd később meglátjuk, hogy a releváns ajánlatok közül hogyan kell kiválasztani a legjobbat). Legyen a lekérdezés a terméknek a felhasználó által begépelt leírása (például „laptop”), akkor egy weblap a lekérdezés szempontjából releváns ajánlat, ha a lap releváns és tényleg egy ajánlat. A lappal kapcsolatos URL-t is nyomon fogjuk követni:
A formális nyelvet (formal language) karakterfüzérek (strings) (lehetséges, hogy végtelen) halmazaként definiáljuk. Minden egyes füzér az úgynevezett záró (vagy terminális) szimbólumok (terminal symbols) – amelyeket néha szavaknak hívunk – öszszekapcsolt sorozata. Például az elsőrendű logikában a záró szimbólumok között van a ∧ és a P, és egy tipikus füzér a „P ∧ Q”. A „P Q ∧” füzér nem része a nyelvnek. A formális nyelvek, mint például az elsőrendű logika vagy a Java, szigorú matematikai definíciókkal rendelkeznek. Ezzel ellentétben a természetes nyelvek (natural languages), mint például a kínai, a dán és az angol, nem rendelkeznek szigorú definícióval, hanem beszélők egy közössége használja. Ebben a fejezetben a természetes nyelveket megpróbáljuk formális nyelveknek tekinteni, bár tisztában vagyunk azzal, hogy az illeszkedés nem lesz tökéletes.
A kényszerek bizonyos típusai gyakran fordulnak elő a valós problémákban és speciális célú algoritmusokkal jóval hatékonyabban kezelhetők, mint az eddig leírt általános célú módszerekkel. Például a MindKül kényszer azt állítja, hogy a benne szereplő öszszes változóknak mind különböző értéket kell felvennie (ahogy a betűrejtvényes példában láttuk). A MindKül kényszer inkonzisztencia-ellenőrzésére egy egyszerű módszer az, hogy ha m változó szerepel a kényszerben, és ha ezeknek együttesen n különböző értéke lehet, akkor m > n esetén a kényszert nem lehet kielégíteni.
Ezeken a feladatokon túl szükség van még módszerekre az állapotátmenet- és érzékelő modellek megfigyelésekből történő megtanulására. Csakúgy, mint a statikus Bayes-hálóknál, a dinamikus Bayes-hálós tanulás elvégezhető mint a következtetés mellékterméke. A következtetés egy becslést ad arra, hogy milyen átmenetek következtek be valójában, és milyen állapotok generálták az érzékelők értékeit, és ezek a becslések felhasználhatók a modell frissítésére. A frissített modellek jobb becsléseket adnak, és a folyamat a konvergálásig iterálódik. A teljes folyamat a várhatóérték-maximálás vagy EM algoritmus egy esete (lásd  alfejezet). Figyelemre méltó részlet, hogy a tanulás teljes simítós következtetést igényel szűrés helyett, mivel ez jobb becsléseket ad a folyamat állapotára. Lehet, hogy a szűréssel való tanulás nem konvergál helyesen; gondoljunk például a gyilkosságok felderítésének a megtanulására: a visszatekintés mindig szükséges annak kikövetkeztetéséhez a megfigyelhető bizonyítékok alapján, hogy mi történt a gyilkosság helyszínén.
Háttértudás ⊨ Hipotézis Fontos Az első kényszer a   képlettel megegyezik, azért a MAT-ot eredetileg a példákból való tanulás jobb módszerének tekintették. Azonban amiatt, hogy a háttértudásnak elegendőnek kell lennie a Hipotézis megmagyarázásához is, ami viszont a megfigyelést magyarázza meg, a megfigyelt esetből az ágens igazából semmi tényszerűen újat nem tanul. Az ágens a példát akár ki is következtethette volna az ismert tudásanyag alapján, bár elképzelhető, hogy ez elfogadhatatlan mennyiségű számítással járt volna együtt. Újabban a MAT-ot úgy tekintik, mint az egyik olyan módszert, amely elsődleges elméleteknek speciális célú tudásba való konvertálását végzi. A MAT algoritmussal a  alfejezetben foglalkozunk.
Az E[i] -vel jelölt pontok által határolt szakaszokon a 3 alapeset az alábbiak szerint váltakozik P[1]P[2] szakasz mentén: * P[1] és E[1] között a szakasz és pont közötti alapeset érvényesül: pontként P[4] szerepel; * E[1] és E[2] között a szakasz és szakasz közötti alapeset érvényesül: a második szakasz a P[3]P[4]; * E[2] és E[4] között a szakasz és pont közötti alapeset érvényesül: pontként P[3] szerepel; * E[4] és E[5] között a szakasz és szakasz közötti alapeset érvényesül: a második szakasz P[3]P[4]; * E[5] és P[2] között a pont és szakasz közötti alapeset érvényesül: pontként ismét P[4] szerepel.
Tehát ahhoz, hogy a H[2] hipotézis általánosítását létrehozzuk, egyszerűen egy olyan C[1] definíciót kell találnunk, amelyik logikailag következik C[2]-ből. Ezt könnyen megtehetjük. Ha például C[2](x) Alternatíva(x) ∧ Vendégek(x, Néhány), akkor egy általánosítási lehetőség a C[1](x) ≡ Vendégek(x, Néhány). Ezt feltételek törlésének (dropping conditions) nevezzük. Felfoghatjuk úgy, hogy egy gyengébb definíció jön létre, így a pozitív példák nagyobb halmazát teszi lehetővé. Számos egyéb általánosítási eljárás is létezik, azon nyelvtől függően, amelyen a műveleteket végezzük. Hasonlóképpen, egy hipotézist szűkíteni tudunk, ha további feltételeket adunk a hozzá tartozó definíciójelölthöz, illetve ha eltávolítunk tagokat egy diszjunktív definícióból. Lássuk, hogyan működik ez az éttermi feladatban,  ábra adatait használva: * Az első példa – X[1] – pozitív. Alternatíva(X[1]) igaz, tehát vegyük fel a következő kiinduló hipotézist:
Leírtunk többféle módszert arra, hogy ágensprogramok hogyan választják ki cselekvéseiket. Nem magyaráztuk meg azonban eddig, hogy hogyan jön létre az ágensprogram. Híres korai cikkében Turing (Turing, 1950) azzal az ötlettel foglalkozik, hogy intelligens gépeit ténylegesen kézzel programozza. Megbecsüli, mennyi munkát vehet ez igénybe, és megállapítja, hogy „valamilyen eredményesebb módszer kívánatosnak látszik”. Az általa javasolt módszer az, hogy építsünk tanuló gépeket, majd tanítsuk ezeket. Az MI sok területén ez ma a javasolt módszer korszerű rendszerek építésére. A tanulásnak van egy másik előnye is, ahogy megállapítottuk korábban: lehetővé teszi az ágens számára, hogy kezdetben ismeretlen környezetben működjön, és kompetensebbé váljon, mint ahogy azt kezdeti tudása lehetővé tette volna. Ebben a fejezetben röviden bemutatjuk a tanuló ágensek főbb elveit. A könyv majdnem minden fejezetében rámutatunk a tanulás lehetőségeire és módszereire az egyes ágensfajtákban. A VI. rész merül el mélyebben a különféle tanulási algoritmusokban.  ábra - Egy modellalapú, hasznosságalapú ágens. A világ modellje mellett egy hasznosságfüggvényt is alkalmaz, amely a világ állapotaihoz rendelt preferenciáit méri. Ezek után olyan cselekvést választ, amely a legjobb várható hasznossághoz vezet, amit az összes lehetséges végállapot előfordulási valószínűségével súlyozott átlagolásával számít ki. Egy modellalapú, hasznosságalapú ágens. A világ modellje mellett egy hasznosságfüggvényt is alkalmaz, amely a világ állapotaihoz rendelt preferenciáit méri. Ezek után olyan cselekvést választ, amely a legjobb várható hasznossághoz vezet, amit az összes lehetséges végállapot előfordulási valószínűségével súlyozott átlagolásával számít ki.
Most megmutatjuk, hogy az MCMC-Kérdez mintavételi lépésében megadott q(x⟶x′) átmenet-valószínűség eleget tesz a teljes egyensúly egyenletének, ahol a stacionárius eloszlás a P(x∣e), a rejtett változók valódi a posteriori eloszlása. Ezt két lépésben teszszük meg. Először, egy Markov-láncot definiálunk, amelyben minden változót az öszszes többi változóval feltételesen mintavételezünk, és megmutatjuk, hogy ez eleget tesz a teljes egyensúly egyenletének. Majd egyszerűen megállapítjuk, hogy Bayes-hálóknál ez ekvivalens azzal a feltételes mintavételezéssel, hogy a feltétel a változó Markov-takarója (lásd 14. szakasz - Feltételes függetlenségi relációk Bayes-hálókban részben).
Egy 1991-ben megjelent cikk szerint Schaeffer magát a megnyitási adatbázis megtervezését tartotta a CHINOOK nagy gyengepontjának, ugyanis az adatbázis tapasztalati alapon épül fel, nincs rá bizonyíték, hogy valóban az ott felírt lépések lennének a legoptimálisabbak [3].
[2] Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple plays-Part I: I.I.D. Rewards, Venkatachalam Anantharam, Pravin Varaiya, Jean Warland, IEEE Transactions on Automatic control, Vol. AC-32, NO. 11, 1987 November
Eszközei: * Kitkontrollok: A készülékhez alkalmazott reagenskészletben található, a kit gyártója által szolgáltatott, a készülékre optimalizált kontrollanyag. A gyártó köteles a leírásban feltüntetni a kontroll célértékét, mérési bizonytalanságát és a célérték visszavezethetőségét. * Független kontrollanyagok: Nem egy gyártó rendszerére optimalizált, a kalibrátor és a reagens gyártási tételeitől nem függő, torzításmentes, a betegmintával azonos mátrixú, objektív információt nyújtó precíziókontrollok. * Valódiságkontrollok: A mérés valódiságát (torzítatlanságát), vagyis a rendszeres hibát vizsgálhatjuk valódiságkontroll – anyagokkal. Ilyen kontrollanyagokat a külső minőség – ellenőrzést végző referencialaboratóriumok készítenek. A referencialaboratórium az adott mintakomponensre megadja a valódiság – kontroll anyag valódi értékét (x[v]) é szórását. A laboratórium 2-3 mérés középértékével meghatározza a várható értéket (x). A mérések valódisága (torzítatlansága) megfelelő, ha a valódi érték és a mért érték különbsége kisebb a háromszoros metodikai szórásnál, abs (x[v ]– X)<3s.
A megoldást az jelentené, ha ennek megfelelően fordított irányban (is) modelleznénk a problémát, mivel értékeket nem kezelhetünk egyszerre értékként is, és változóként is, amely változót kap értékül (amihez változó kerül hozzárendelésre), ahogy a változókat sem kezelhetjük ezek szerint egyszerre változóként, és értékként is. Pontosabban a KKP-megoldó algoritmusok ilyenfajta modellekkel nem tudnának mit kezdeni. Tehát próbáljuk meg „kifésülni” ezt az egészet. Próbáljuk meg fordított irányban is modellezni a problémát úgy, hogy amit a primér reprezentációban értéknek tekintünk, az legyen (annak feleljen meg) változó, és amit a primér reprezentációban változónak kezelünk, az érték legyen. Ezt az újabb reprezentációt pedig stílszerűen nevezzük duál reprezentációnak.
Az eddig tárgyalt pályatervezési algoritmusok közül egyik sem foglalkozott a robotika egyik központi problémájával: a bizonytalansággal. A robotikában a bizonytalanság a környezet részleges megfigyelhetőségéből és a robot mozgásának sztochasztikus (vagy nem modellezett) összetevőiből ered. Hibát okozhat az is, ha valamilyen közelítéses módszert, például részecskeszűrést használunk. Ilyen esetekben a robotnak nincs pontos információja a helyzetéről, még akkor sem, ha a környezet sztochasztikus vonásait tökéletesen modelleztük.
Első látásra úgy tűnik, hogy a természetes nyelvek (mint az angol vagy a magyar) valóban nagyon kifejezők. Képesek voltunk csaknem a teljes könyvet természetes nyelven megírni, és csak alkalmanként kellett áttérnünk más nyelvekre (beleértve a logikát, a matematikát és a diagramok nyelvét). Nagy múltú hagyomány a nyelvészetben és a nyelvek filozófiájában, hogy a természetes nyelvet elsősorban deklaratív tudásreprezentációs nyelvnek tekintjük, és megpróbáljuk rögzíteni ennek formális szemantikáját. Ha egy ilyen kutatás sikeres lenne, nagy előrelépést jelenthetne a mesterséges intelligencia számára, mert egy természetes nyelvet (vagy annak valamilyen származékát) használhatnánk a tudásreprezentációhoz vagy a következtető rendszerekben.
Felhasználva ezt a definíciót, egy állapot valódi hasznossága az U^π (s), amit U(s)-sel jelölünk. Ez a a leszámítolt jutalmak várható értéke akkor, amikor az ágens egy optimális eljárásmódot hajt végre. Vegyük észre, hogy az U(s) és az R(s) igen eltérő mennyiségek: az R(s) a „rövid távú” jutalom az s-ben tartózkodásért, míg az U(s) a „hosszú távú” összjutalom s-től kezdve. A  ábrán láthatók a hasznosságok a 4 × 3-as világban. Látható, hogy a hasznosságok nagyobbak a +1-es kijárathoz közeli állapotoknál, mivel kevesebb lépés szükséges a kijárat eléréséhez.
Ha valami tudományterület visszanyúlik az ókori görögökhöz, akkor az azt szokta jelenteni, hogy az egy fontos tudományterület. Hisz, ha 2-3 ezer évvel is foglalkoztak vele… Na a logikai programozás nem ilyen. Nem nyúlik vissza az ókori görögökhöz, ugyanakkor mégis egy fontos, és az idő előre haladtával egyre fontosabbá váló tudományterület.
Melyik mondat szavait tudta sorrendbe tenni? Milyen tudás alapján volt erre képes? Tanítson egy bigram modellt egy tanító korpusz alapján, és használja arra, hogy meghatározza egy tesztkorpusz mondatainak legnagyobb valószínűségű permutációit. Mutassa meg a modell pontosságát.
 Megjegyzés Az  ábra a különböző algoritmusokat az n-királynő problémán teszteli. Próbálja meg ugyanezt egy véletlenszerűen generált térképszínezési problémával is: osszon el az egységsíkon véletlenszerűen n pontot, válasszon ki véletlenszerűen egy X pontot, kösse X-et a legközelebbi olyan Y ponthoz, amelyikkel X még nincs összekötve, és a vonal semelyik más vonalat nem metsz; ismételje a fenti lépést mindaddig, amíg újabb összeköttetés már nem lehetséges. Számítsa ki a teljesítménytáblázatot a legnagyobb n-re, amire csak tudja (mind d = 3-at mind, d = 4-et használva). Fűzzön magyarázatokat a kapott eredményhez.
Figyeljünk oda arra, hogy azon a parton, ahol a csónak van, lehet több kannibál, mint hittérítő (ez a 'felesleg' a csónakban ül), a másikon azonban nem! (Ezért is nagyon fontos jelölni, hogy hol van a csónak.)
Eltérően az ítéletlogikai literáloktól, az elsőrendű literálok tartalmazhatnak változókat, amely esetben a változókat univerzális kvantorral ellátottnak tételezzük fel. (A határozott klózok írásánál általában elhagyjuk az univerzális kvantorokat.) A határozott klóz egy megfelelő normál forma, hogy az általánosított Modus Ponensszel alkalmazhassuk. Nem minden tudásbázist lehet átalakítani határozott klózok halmazává, az egyetlen pozitív literális korlátja miatt, de sokat igen. Gondoljuk át a következő problémát:
